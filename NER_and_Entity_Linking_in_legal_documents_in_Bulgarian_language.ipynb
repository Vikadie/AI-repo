{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER and Entity Linking in legal documents in Bulgarian language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "18QqUo4aVcjoZfepAgU-I6HNL1JRqm6Eu",
      "authorship_tag": "ABX9TyPEaASAhxae4vYLSjTnJfou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikadie/AI-repo/blob/master/NER_and_Entity_Linking_in_legal_documents_in_Bulgarian_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlsqWrBmYiEz"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnhWNKNacJMU",
        "outputId": "6c8b451d-4169-4adc-8cb9-be4499b1a6e4"
      },
      "source": [
        "import os\r\n",
        "from random import randint, seed\r\n",
        "from time import time\r\n",
        "import json\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n",
        "!pip install nose\r\n",
        "\r\n",
        "from nose.tools import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 28.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 8.7MB/s \n",
            "\u001b[?25hInstalling collected packages: nose\n",
            "Successfully installed nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLfOEgGz-N7w",
        "outputId": "5f875adc-5a8e-44ee-d9ea-baceccec58e9"
      },
      "source": [
        "!pip install -q -U tf-models-official\r\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 83kB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 53.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 58.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 17.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 56.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 52.0MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 8.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsaasJH4cY9r"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "\r\n",
        "from official.nlp import bert\r\n",
        "from official.nlp.bert.tokenization import FullTokenizer\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7rhmWFXKizg"
      },
      "source": [
        "# import sys\r\n",
        "\r\n",
        "# !test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\r\n",
        "# if not 'bert_repo' in sys.path:\r\n",
        "#   sys.path += ['bert_repo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeYRuoq_KrTp"
      },
      "source": [
        "# # import python modules defined by BERT\r\n",
        "# import modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-NtMro9h4Se"
      },
      "source": [
        "from official.nlp.optimization import AdamWeightDecay, WarmUp\r\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twPwH61fYk9j"
      },
      "source": [
        "# NER and Entity Linking in unstructured legal documents in Bulgarian language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otloh6UrcelW"
      },
      "source": [
        "##### Final exam report\r\n",
        "\r\n",
        "*Viktor Belchev - student*\r\n",
        "\r\n",
        "*Deep Learning - Software University*\r\n",
        "\r\n",
        "*February 2021*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ZAILvGhvbs"
      },
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQ76Jofh7sb"
      },
      "source": [
        "Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rR9jyxPiPpb"
      },
      "source": [
        "In all legal documents there is usage of citation of different laws or other juridical terms often hidden behind some abbreviations. While this aims to make the text shorter and clearer it is mostly causing troubles in understanding and translation to simple language not only to regular persons, but sometimes even lawyers and people with juridical background feel lost. Therefore, it often requires an additional research in the legal litterature. At this stage another problem might occur - the correct decoding of abbreviated terms can become obstacle on top of the the overall understanding of the information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SDwDTdFmS0a"
      },
      "source": [
        "In this paper, I will try to use the modern approach of Deep Learning to create a helpful tool that overcomes these problems. Using the state-of-the-art available models in the field of Natual Language Processing like BERT and an abbreviation list available at this stage, I will try to achieve an acceptable accuracy in this task for Bulgarian language that is known as combination of two different tasks: Named Entity Recognition and Entity Linking. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC38oPP9iDuY"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HysH52mHqc3n"
      },
      "source": [
        "Generally, in Natural Language Processing (further, NLP) the process of disambiguation of terms is known as Entity Linknig (further, EL), which goes hand in hand with another operation called Named Entity Recognition (further, NER). As explained by Iva Marinova in her **\"Reconstructing NER Corpora: a Case Study on Bulgarian\"** while in the field of Deep Learning \r\n",
        "these two related tasks are considered to be well covered in\r\n",
        "NLP for Germanic, Romance and other language groups,\r\n",
        "they are still under-resourced for the Slavic languages, especially from a multilingual perspective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tppf-Tke1evN"
      },
      "source": [
        "Usually, the order of application of both tasks is by starting with NER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoh_BX7uu4CJ"
      },
      "source": [
        "The purpose of NER is to tag words in a sentences based on some predefined tags, in order to extract some important information of the sentence, like for instanse names, geographical locations, dates, currency etc.\r\n",
        "In NER, each token in the sentence will get tagged with a label, the label will tell the specific meaning of the token. In that way, through NER, we can analyze the sentence with more details and extract the important information.\r\n",
        "\r\n",
        "There are two popular approaches for NER:\r\n",
        "- multi-class classification based where NER is treated as a multi-class classification process, and we can use some text classification method to label the token.\r\n",
        "- Conditional Random Field(CRF) based method labels the token taking context into account, then predicts sequences of labels for sequences of sentence token then get the most reasonable one. It is a probabilistic graphical model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy19WENNgfHb"
      },
      "source": [
        "The identification of named entity mentions in texts is often implemented using a sequence tagger, where each token is labeled with an BIO tag, indicating whether the token begins a named entity — (B-), whether it is inside of a named entity (I-), or outside of a named entity (O-). This type of annotation has been proposed for the first time at CoNLL-2003 dataset created for NER (Tjong Kim Sang and De Meulder, 2003). There are other tag notation types. For instance, each token can be predicted with a tag indicated by B-(begin), I-(inside), E-(end), S-(singleton) of a named entity with its type, or O-(outside) of named entities. But, I will stick to BIO format of representation for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6VoY4TeHPNa"
      },
      "source": [
        "Entity linking can be applied rigth after the NER task is performed althought in some papers on this topic there is proposal to do it in parallel (jointly) for each token, so that each subtask benefits from the partial output of the other subtask, and thus alleviate error propagations that are unavoidable in pipeline settings. \r\n",
        "Generally, EL is the task of mapping words from text (e.g. names of persons, locations and organizations) to entities from the target knowledge base. For this pupose I use a document containing most of the existing abbreviations used in legal documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNl8XkxiQ5b"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Nzf5h0KOdV"
      },
      "source": [
        "Usually, no matter the specific task, Deep Learning models creation is based on big data for training, validation and test. For Bulgarian language generally such data could be available if we start scraping web pages, which is huge amount of work. But this is only one side of the hidden obstacles - the effectiveness of the model created for the task is a real challenge on its own. \r\n",
        "\r\n",
        "Luckily, after the publication of the famous paper called \"Attention is all you need\" by Vaswani and the \"appearance\" of *Transformer*, there is a huge advancement in the model creation compared to previous usage of recurrence (RNN), Bidirectional Lont-Short Term Memory units (BiLSTM), convolutions (CNN) and CRF. Transformer utilizes stacked self-attention and pointwise, fully connected layers to build basic blocks for encoder and decoder.  Experiments on various tasks show Transformers to be superior in quality while requiring significantly less time to train.\r\n",
        "\r\n",
        "Based mostly on transformer, it already exists pre-trained models that provide results pretty close to humans on some general tasks. Some of the most used methods are ELMo(Embeddings from Language Models), OpenAI GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers)... \r\n",
        "\r\n",
        "It is important to underline that these state-of-the-art models use specific representation of the text, called embeddings, usually so called *hybrid representation* of text in low dimensional real-valued dense vectors. It is called *hybrid* as it uses *Word-level* and *Character-level* representation along with some additional features, where each dimensions represents a latent feature. This way it also captures the semantic and syntactic properties of words, but also the context for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlIsgqLVaxN"
      },
      "source": [
        "In recent years, the advancements of NLP in general and NER in particular has been greatly influenced by deep transfer learning methods capable of creating contextual representations of words, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created. Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context – sentences around the sentence of interest – to improve sequence tagging performance.\r\n",
        "\r\n",
        "Precisely for the fact of using this cross-sentence context, but also with the advantage to be pre-trained on Bulgarian texts, in this notebook, I focus on the recent BERT deep transfer learning models based on self-attention and the transformer architecture. BERT uses a fixed-size window that limits the amount of text that can be input to the model at one time. The model maximum window size, or maximum sequence length, is fixed during pre-training, with 512 wordpieces a common choice. This window fits dozens of typical sentences of input at a time, allowing the inclusion of extensive sentence context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejQVdKrgiZT"
      },
      "source": [
        "There are many advantages that pushed me towards usage of BERT. To enumerate some, I would say that it provides:\r\n",
        "1. quicker development\r\n",
        "2. overcome the problem of missing data for training, which is generally the case for Bulgarian\r\n",
        "3. state-of-the-art better results - BERT is built on top of a number of clever ideas considered top in NLP community in latest years – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).\r\n",
        "\r\n",
        "BERT is also one of the preferred model giving the best results used by Ilias Chalkidis et al. when dealing with  Large-Scale Multi-Label Text Classification (LMTC) in the legal domain (EU legislation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaxZDnSrGtZ0"
      },
      "source": [
        "On the other hand, there are some disadvantages, like:\r\n",
        "1. it is very large. The LARGE version of BERT would provide better results, but unfortunately that would require bigger computational ability and time.\r\n",
        "2. Even when using the BASE version, it remains slow for fine-tuning.\r\n",
        "3. The multilingual version that I need to use cannot be disitilled - the vocabulary used for fine-tuning of BERT must remain the original one.\r\n",
        "4. It uses a specific and a bit complicated jargon (domain-specific language), meaning that the tokenization with BERT should be done with BERT Tokenizer.\r\n",
        "\r\n",
        "The last two disadvantages represent in fact the specifity and maybe the strength of BERT. Its vocabulary is indeed fixed, but it has the capability to break down the unknown word into subwords and makes a token out of each subword (if subword exists in the vocabulary). In case the subword do not exist in the vocabulary it can continue spliting it into subwords down to a character level. To recognize the subword it prepends it with \"##\" flag, exceot for the first subword.\r\n",
        "\r\n",
        "On its turn the subword split would create a problem with labeling. Generally, in the test and train part each word is tagged. If an unknown word is splitted to subwords, a specific tag should be used for it, that would indicate that the tag valid for this word (the initial whole word) would be the one given to the first subword (original word) and a specific tag would be assigned to subwords after the first one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Z6ezlroorO"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wzBDQNrAdGS"
      },
      "source": [
        "Before getting to the problem of tags given to subwords, we need a dataset, big enough, that can be used to fine-tune our BERT model. This dataset should implement the following requirements:\r\n",
        "   - it must be created for a NER task;\r\n",
        "   - it must be in Bulgarian;\r\n",
        "   - it must contain special annotations (tags) for recognition of legal phrases;\r\n",
        "   - it must be big enough to train deep network model;\r\n",
        "   - ideally it should have a train, validation and test datasets.\r\n",
        "\r\n",
        "Well, the first four requirements need to be mandatory fulfilled. After all if the dataset is big enough there are ways and methods to make a consise split for train, validation and test datasets. \r\n",
        "\r\n",
        "But it is hard task to implement all four requirements. In fact, I was not able to find such dataset on Internet. Luckily, there is one dataset recently created for NER task, which was in Bulgarian - the dataset done by Iva Marinova et al. pesented in May 2020. The dataset is available at https://github.com/usmiva/bg-ner. With it, I could cover half of the requirements for my task. Unfortunately, as it was not created for utilization on legal texts, there were not a specific tag for legal phrases inside. Still, it was the best one I could find. Therefore, I decided to use it as a base, a starting point, and add to it the required information covering the legal part gathered by me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqEl2ZSDprp"
      },
      "source": [
        "But before start adding information, let me reveal what and how is implemented inside, in order to decide at what level it will suit me and how to add the missing information.\r\n",
        "\r\n",
        "The original Bulgarian corpus consists of 916 text files extracted from various news websites. The training dataset contains information on two topics – Brexit and the trial of the Pakistani Christian Asia Bibi, accused of blasphemy, while the main subjects for the test data are the Nord Stream 2 project and the recent developments in RyanAir’s business history.\r\n",
        "\r\n",
        "The type of annotation used inside followed the format used for the first time at CoNLL-2003 but used only the first and the last column (ommitting the part-of-speech tag and synctatic chunk tag) - meaning that the input files were segmented into sentences and tokens per line (first column), and each token was combined with its corresponding Named Entity tag (the second column). The NE tags were of type person (PER), organization (ORG), location (LOC), product (PRO), and event (EVT) and each of them had a prefix using the BIO format. Like in most NER tasks, NEs are considered to be non-recursive, non- overlapping, and whenever one NE is embedded in another NE, only the top-most entity is annotated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLkW9vyiDgi8"
      },
      "source": [
        "The 2 files available for download were 2 text files (.txt) - one train file with 220 700 lines and one test file with ~65 000 lines.\r\n",
        "\r\n",
        "Well, armed with this information, it was obvious that the missing part was for legal phrases, thus missing tag for tham. I decided that I could simply add a NE tag LAW. After a quick review there were only few word that could match this new tag in the existing dataset.\r\n",
        "\r\n",
        "Therefore, I added to the training file 117 documents taken from the \"Decision Register\" of [The Administrative Court of Sofia City (ASCS)](http://www.admincourtsofia.bg/Default.aspx?alias=www.admincourtsofia.bg/en) representing  the first 5 working days of year 2021 (from 4.01 to 8.01.2021). Each of this document was transformed to text, the sentences containing legal mentions were extracted and transformed in a file following the format of the original dataset using a simple Python script. The tag were than manually reviewed and annotated as correctly as possible not forgeting the initially available tags for person, organization etc. with the BIO prefixes.\r\n",
        "\r\n",
        "In that way the train document grew up to 347 642 lines. \r\n",
        "\r\n",
        "The original test documant were split in two - one for validation and one for test datasets. 40 documents  from the same source (court decisions published from 11.01.2021) were annotated and splitted the same way it was done for the train part. With that operation the validation file consisted of 56 880 lines and the test file of 56 908 lines. \r\n",
        "\r\n",
        "In that way the ration train vs. validation was assured to be at the reasonable 84% / 14% level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWe3li-eK799"
      },
      "source": [
        "### Selecting the model and loading it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYUn2cXa7AN5"
      },
      "source": [
        "First, I need to upload the BERT model. The choosen model by is:\r\n",
        "- the BERT Multilanguage version (in order to have a pre-trained model that has already seen Bulgarian language and Bulgarian words are part of it vocabulary);\r\n",
        "- the Cased version, meaning that whether the words contains capital letter or not matter to the model;\r\n",
        "- BASE version, as using the LARGE model would take too many ressources for training without such significal improvement in the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIbmx1xC8ZtW"
      },
      "source": [
        "When using BERT one must be aware that the tokenization with BERT should be done with BERT Tokenizer. There are also other mandatory requirement for the fine-tunning of BERT - used vocabulary should be exactly the original of the pre-trained model.\r\n",
        "\r\n",
        "There are two ways to upload the model.\r\n",
        "\r\n",
        "The first one is by loading the model from TensoFlow Hub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JEjEGAu0ES",
        "outputId": "bdec049b-5f1d-46f8-97a2-8303f8f87c0d"
      },
      "source": [
        "bert_model_name = 'bert_multi_cased_L-12_H-768_A-12' \r\n",
        "map_name_to_handle = {'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'}\r\n",
        "\r\n",
        "map_model_to_preprocess = {'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',}\r\n",
        "\r\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\r\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\r\n",
        "\r\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\r\n",
        "print(f'Preprocessing model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2tr-98HIFgn"
      },
      "source": [
        "Using TensoFlow Hub has the advantage of having its incorporated \"pre-processing\" procedure based on which it can automatically load the required preprocessing model. This model can be loaded into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose a fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3YV-TZVv12B",
        "outputId": "e4cf3bc0-7fc7-466c-e47d-332ab0f7c9ae"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\r\n",
        "tok = bert_preprocess_model(['Hello TensorFlow!'])\r\n",
        "print(f'Keys       : {list(tok.keys())}')\r\n",
        "print(f'Shape      : {tok[\"input_word_ids\"].shape}')\r\n",
        "print(f'Word Ids   : {tok[\"input_word_ids\"][0, :12]}')\r\n",
        "print(f'Input Mask : {tok[\"input_mask\"][0, :12]}')\r\n",
        "print(f'Type Ids   : {tok[\"input_type_ids\"][0, :12]}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [  101 31178 16411 28919 11565 27863   106   102     0     0     0     0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76rmrViBJQ-4"
      },
      "source": [
        "After loading it and conducting a trial on a brief sentence, it is visible that it outputs a dictionary containing all the three mandatory inputs: *'input_word_ids', 'input_mask'* and *'input_type_ids'*.\r\n",
        "It is important to note also that the input is truncated to 128 tokens. Luckily, the number of tokens can be customized. Also, to the `input_word_ids` we can see automatical append of other required tags at the beginning and at the end of the sentence. It is interesting to note as well that the `input_type_ids` only have zeros (0) because this is a single sentence input, which is the case of the NER task as well. For other tasks where there are multiple sentence input, it would have one number for each input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdgt-TIN9eDV"
      },
      "source": [
        "This \"preprocessing\" model simplifies a lot the work required by the model, but unfortunately, along with the 'attribute' part, in NER task I have to make the appropriate transformation on the 'label' part as well.\r\n",
        "\r\n",
        "This is what led me to the second possibility, which consists in downloading the required version to a directory from where it can be directly loaded. It can be done with 2 line of code:\r\n",
        "\r\n",
        "`model_name = \"multi_cased_L-12_H-768_A-12\"`\r\n",
        "\r\n",
        "`model_dir = bert.fetch_google_bert_model(model_name, gs_folder_bert)`\r\n",
        "\r\n",
        "I did that in my Google Disk drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EjrcRxhryqy",
        "outputId": "d95ade3b-697a-4066-856d-73a839b0eab6"
      },
      "source": [
        "gs_folder_bert = '/content/drive/MyDrive/Colab Notebooks/bert_model/'\r\n",
        "\r\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.ipynb_checkpoints',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_config.json',\n",
              " 'bert_model.ckpt.index',\n",
              " 'bert_model.ckpt.meta',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IGZmswUrlNG",
        "outputId": "4f5e550a-68b8-4b24-b289-a61b0c9d774e"
      },
      "source": [
        "# Set up tokenizer to generate Tensorflow dataset\r\n",
        "tokenizer = FullTokenizer(\r\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\r\n",
        "     do_lower_case=False)\r\n",
        "\r\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 119547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1m8OeaD-uCx"
      },
      "source": [
        "We can use the same example as above on a simple sentence that is not transformed to a list to verify that the upper case matters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOIr3_1n-tm_",
        "outputId": "79eed1b1-575f-4495-d81a-5e8ef8855017"
      },
      "source": [
        "tokens = tokenizer.tokenize(\"Hello TensorFlow!\")\r\n",
        "print(tokens)\r\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "print(ids)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'Ten', '##sor', '##F', '##low', '!']\n",
            "[31178, 16411, 28919, 11565, 27863, 106]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQYKlNarASK5"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeOafYlWM1LY"
      },
      "source": [
        "I will position the \"*Constants*\" section here as some of the constants will be used in the next section. As thing progress I will add below all *constant values* that I will use with some explanation below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wez32z_oIU4-"
      },
      "source": [
        "EPOCHS = 3  # Total number of training epochs to perform\r\n",
        "MAX_SEQ_LENGTH = 256  # the length of the biggest sentence\r\n",
        "BATCH_SIZE = 16  # Total batch size for training.\r\n",
        "LEARNING_RATE = 5e-5  # The initial learning rate for Adam.\r\n",
        "WARM_UP_PROPORTION = 0.1  # Proportion of training to perform linear learning rate warmup for. e.g., 0.1 = 10% of training.\r\n",
        "WEIGHT_DECAY = 0.01  # Weight decay if we apply some\r\n",
        "ADAM_EPSILON = 0.01  # Epsilon for Adam optimizer\r\n",
        "SEED = seed(42)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_iment_7NcZ"
      },
      "source": [
        "As \"prescribed\" on the official [GitHub page of BERT](https://github.com/google-research/bert) in order to avoid any out-of-memory issues when using BERT model `MAX_SEQ_LENGTH` should be up to 512. Having in mind the numerous splits of the specific `tokenizer` of BERT and the maximum tokens length found during the creation of the dataset, I fix the maximum sequence length to `256`. It might not be enough as we have additional split of words to subwords due to BERT specifity, but makeing it bigger will require to much memory and will decrease teh speed of the process. In the case of maximum sequence set on '256', the benchmark of the `BATCH_SIZE` found in the same place is `16`.\r\n",
        "\r\n",
        "For the initial learning rate (`LEARNING_RATE`), in line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5). I'll use `5e-5`. During the BERT pre-training, the learning rate is a linear warm-up phase over the first `10% of training steps`, meaning the the `WARM-UP PROPOTION` should be set the same. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYRJWwDf_XSD"
      },
      "source": [
        "### Reading the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTlgRkwlLEog"
      },
      "source": [
        "After loading the model, it is time to read the documents in order to prepare the train, validation and test datasets. For that I need to create a function that will read the data sentence by sentence, that will be transformed afterwords to features with another function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41QFFFxmQYAC"
      },
      "source": [
        "The visualizing test done after each function will be conducted on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIiGhmobLjeD"
      },
      "source": [
        "def read_data(filename):\r\n",
        "  \"\"\"\"reading the file and returning for each sentence a tuple of \r\n",
        "  a list of attributes and a list of corresponding labels\r\n",
        "  \r\n",
        "  side-effects: printing the full path to the file\r\n",
        "                printing the number of sentences inside the set\r\n",
        "                printing the size of longest sentence of the set\r\n",
        "  \"\"\"\r\n",
        "  data, sentence, label = [], [], []\r\n",
        "  num_sentense, max_sentence_length = 0, 0\r\n",
        "  sen = ''\r\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\r\n",
        "    for line in f:\r\n",
        "      if len(line) == 0 or line[0] == '\\n':\r\n",
        "        if len(sentence) > 0:\r\n",
        "          data.append((sentence, label))\r\n",
        "          if len(sentence) > max_sentence_length:\r\n",
        "            max_sentence_length = len(sentence)\r\n",
        "            sen = sentence\r\n",
        "          sentence, label = [], []\r\n",
        "          num_sentense += 1\r\n",
        "        continue\r\n",
        "      word, lab = line.rstrip('\\n').split('\\t')\r\n",
        "      sentence.append(word)\r\n",
        "      label.append(lab)\r\n",
        "\r\n",
        "    if len(sentence) > 0:\r\n",
        "      data.append((sentence, label))\r\n",
        "      num_sentense += 1\r\n",
        "      if len(sentence) > max_sentence_length:\r\n",
        "            max_sentence_length = len(sentence)\r\n",
        "\r\n",
        "    print(\"Full path to the filename:\", filename)\r\n",
        "    print(\"Number of sentences:\", num_sentense)\r\n",
        "    print(\"Maximum token lenght of a sentence:\", max_sentence_length)\r\n",
        "  return data  # [tuple(attributes, labels)]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QzJIUSeQL4o",
        "outputId": "828e1fe6-c947-4604-8450-619ab0655146"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/data/'\r\n",
        "\r\n",
        "val_file = os.path.join(path, 'val_NER_BG.txt')\r\n",
        "\r\n",
        "val_readed_data = read_data(val_file)\r\n",
        "\r\n",
        "# printing of the first 5 values\r\n",
        "print()\r\n",
        "print(val_readed_data[:5])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/val_NER_BG.txt\n",
            "Number of sentences: 1695\n",
            "Maximum token lenght of a sentence: 240\n",
            "\n",
            "[(['Газопроводът', 'Северен', 'поток', '2', ',', 'който', 'по', 'план', 'ще', 'пренася', 'ежегодно', '55', 'милиарда', 'кубични', 'метра', 'природен', 'газ', 'от', 'Русия', 'към', 'ЕС', 'през', 'Балтийско', 'море', ',', 'вече', 'бе', 'одобрен', 'от', 'Германия', 'и', 'Финландия', '.'], ['O', 'B-PRO', 'I-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O']), (['САЩ', ',', 'в', 'отговор', 'заявиха', ',', 'че', 'тръбопроводът', 'ще', 'повиши', 'зависимостта', 'на', 'Европа', 'от', 'руския', 'газ', '.'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']), (['Списание', '\"', 'Foreign', 'policy', '\"', 'цитира', 'три', 'източника', 'близки', 'до', 'въпроса', ',', 'които', 'твърдят', 'че', 'администрацията', 'на', 'САЩ', 'е', 'близо', 'до', 'налагането', 'на', 'санкции', 'върху', 'енергийни', 'компании', 'от', 'Германия', 'и', 'други', 'държави', 'от', 'ЕС', ',', 'които', 'са', 'замесени', 'в', 'изграждането', 'на', 'проекта', 'за', 'руски', 'газов', 'тръбопровод', 'Северен', 'поток', '2', '.'], ['O', 'O', 'B-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'I-PRO', 'O']), (['Ключови', 'фигури', 'в', 'администрацията', 'на', 'президента', 'Доналд', 'Тръмп', ',', 'които', 'виждат', 'санкциите', 'като', '\"', 'много', 'вероятна', 'опция', '\"', ',', 'няма', 'да', 'се', 'спрат', 'пред', 'нищо', ',', 'за_да', 'блокират', 'Северен', 'поток', ',', 'заявява', 'един', 'от', 'източниците', 'на', 'списанието', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Източник', 'от', 'Държавния', 'департамент', 'е', 'заявил', ',', 'че', '\"', 'бяхме', 'ясни', ',', 'че', 'компании', 'работещи', 'в', 'руския', 'енергиен', 'сектор', 'и', 'сектора', 'за', 'износ', 'на', 'тръбопроводи', 'навлизат', 'в', 'бизнес', 'посока', ',', 'която', 'носи', 'опасност', 'от', 'санкции', '\"', '.'], ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQYwbtNIpAZp"
      },
      "source": [
        "All these sentences need to be transformed in features that BERT understands using the BERT tokenization.\r\n",
        "\r\n",
        "By tokenizing a sentence we in fact encode the sentence. There is one special requirement for that tokenization - 2 additional labels should be added to the list of words representing each sentence: `['CLS']` and `['SEP']`. These 2 additional labels are required by BERT. `['CLS']` indicates that we will talk about \"classification problem\", so `['CLS']` token will be put at the beginning of each phrase, and each sentence and its corresponding label list should end with a `['SEP']` - \"separator\" token. Their ids a respectively `[101]` and `[102]`\r\n",
        ".\r\n",
        "\r\n",
        "The feature required by BERT, apart from the ids of the tokenized sentence and its respective labels using the same index mapping everywhere defined in advance), is the \"input masks\" which allows the model to cleanly differentiate between content and padding. Similar masking is needed for the labels.\r\n",
        "\r\n",
        "The most specific part that I will be using here is the creation and consequently usage of the variable `valid_ids`. This variable will be a list which will be responsible for the replication of the logic of subwords to the `label_id` list, by marking only the first subword's label as valid, so that the label becomes \"responsible\" for the whole word.\r\n",
        "\r\n",
        "There are different ways to provide all features to the model. Most examples on this topic use dictionnaries, but the this a simple class will be more easy to deal with when using is further, in order to stuck them together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ5Yflqymacj"
      },
      "source": [
        "class InputFeatures:\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, ntokens, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\r\n",
        "      self.ntokens = ntokens  # only for representatinal purpose\r\n",
        "      self.input_ids = input_ids  # encoded words\r\n",
        "      self.input_mask = input_mask  # mask indicating padding or not\r\n",
        "      self.segment_ids = segment_ids  # we talk about NER task, so the segment_ids will be just '0's\r\n",
        "      self.label_id = label_id  # encoded label\r\n",
        "      self.valid_ids = valid_ids  # when the word is split to subwords, it indicates only the first id as valid '1', next subwords as '0'\r\n",
        "      self.label_mask = label_mask  # mask indicating padding or not\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvpzX9nmRlD"
      },
      "source": [
        "def convert_examples_to_features(examples, label_map, max_seq_length, tokenizer):\r\n",
        "    \"\"\"Loads a data file into a list of `InputFeatures`s.\"\"\"\r\n",
        "\r\n",
        "    features = []  # the list of `InputFeatures` to be returned\r\n",
        "    for (ex_index, example) in enumerate(examples):\r\n",
        "        textlist = example[0]\r\n",
        "        labellist = example[1]\r\n",
        "        tokens = []\r\n",
        "        labels = []\r\n",
        "        valid_ids = []\r\n",
        "        label_mask = []\r\n",
        "        for i, word in enumerate(textlist):\r\n",
        "            token = tokenizer.tokenize(word)\r\n",
        "            tokens.extend(token)\r\n",
        "            label_1 = labellist[i]\r\n",
        "            for m in range(len(token)):\r\n",
        "                if m == 0:\r\n",
        "                    labels.append(label_1)\r\n",
        "                    valid_ids.append(1)\r\n",
        "                    label_mask.append(True)\r\n",
        "                else:\r\n",
        "                    valid_ids.append(0)\r\n",
        "        \r\n",
        "        # checking if a sentence is longer than max_seq_length, if yes -> cut it\r\n",
        "        if len(tokens) >= max_seq_length - 1:\r\n",
        "            tokens = tokens[0:(max_seq_length - 2)]\r\n",
        "            labels = labels[0:(max_seq_length - 2)]\r\n",
        "            valid_ids = valid_ids[0:(max_seq_length - 2)]\r\n",
        "            label_mask = label_mask[0:(max_seq_length - 2)]\r\n",
        "\r\n",
        "        # init\r\n",
        "        ntokens = []\r\n",
        "        segment_ids = []\r\n",
        "        label_ids = []\r\n",
        "\r\n",
        "        # adding the mandatory ['CLS'] at the beginning\r\n",
        "        ntokens.append(\"[CLS]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        valid_ids.insert(0, 1)\r\n",
        "        label_mask.insert(0, True)\r\n",
        "        label_ids.append(label_map[\"[CLS]\"])\r\n",
        "        for i, token in enumerate(tokens):\r\n",
        "            ntokens.append(token)\r\n",
        "            segment_ids.append(0)\r\n",
        "            if len(labels) > i:\r\n",
        "                label_ids.append(label_map[labels[i]])\r\n",
        "\r\n",
        "        # adding the mandatory ['SEP'] at the end of each sentence\r\n",
        "        ntokens.append(\"[SEP]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        valid_ids.append(1)\r\n",
        "        label_mask.append(True)\r\n",
        "        label_ids.append(label_map[\"[SEP]\"])\r\n",
        "\r\n",
        "        # transforming `ntokens` to BERT's tokenizer ids\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "        label_mask = [True] * len(label_ids)\r\n",
        "\r\n",
        "        # padding\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "            label_ids.append(0)\r\n",
        "            valid_ids.append(1)\r\n",
        "            label_mask.append(False)\r\n",
        "        while len(label_ids) < max_seq_length:\r\n",
        "            label_ids.append(0)\r\n",
        "            label_mask.append(False)\r\n",
        "\r\n",
        "        # last check - all lengths should correspond to max_seq_length\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "        assert len(label_ids) == max_seq_length\r\n",
        "        assert len(valid_ids) == max_seq_length\r\n",
        "        assert len(label_mask) == max_seq_length\r\n",
        "\r\n",
        "        # adding to the list\r\n",
        "        features.append(\r\n",
        "            InputFeatures(ntokens=ntokens,\r\n",
        "                          input_ids=input_ids,\r\n",
        "                          input_mask=input_mask,\r\n",
        "                          segment_ids=segment_ids,\r\n",
        "                          label_id=label_ids,\r\n",
        "                          valid_ids=valid_ids,\r\n",
        "                          label_mask=label_mask))\r\n",
        "    return features"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVqo7l9lrNAA"
      },
      "source": [
        "In order to use this function, I will need to create the list of the available labels including the required by BERT `['CLS']` and `['SEP']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1vPizjArq0k"
      },
      "source": [
        "label_list = [\"O\", \"B-LAW\", \"I-LAW\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-PRO\", \"I-PRO\", \"B-EVT\", \"I-EVT\", \"[CLS]\", \"[SEP]\"]\r\n",
        "\r\n",
        "label_map = {label: i for i, label in enumerate(label_list, 1)}"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeX1MoH5sT2_"
      },
      "source": [
        "val_features = convert_examples_to_features(examples=val_readed_data, label_map=label_map, max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MkxTDgvGMqw"
      },
      "source": [
        "Here is an example of the outcome of this function on the first sentence from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVullaray4sI",
        "outputId": "3e9ba6ac-6692-4c26-f212-22ea77876c9c"
      },
      "source": [
        "print(val_features[0])\r\n",
        "print(\"ntokens =\", val_features[0].ntokens)\r\n",
        "print(\"input_ids =\", val_features[0].input_ids)\r\n",
        "print(\"input_mask =\", val_features[0].input_mask)\r\n",
        "print(\"segment_ids =\", val_features[0].segment_ids)\r\n",
        "print(\"label_id =\", val_features[0].label_id)\r\n",
        "print(\"valid_ids =\", val_features[0].valid_ids)\r\n",
        "print(\"label_mask =\", val_features[0].label_mask)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.InputFeatures object at 0x7fa56bb90780>\n",
            "ntokens = ['[CLS]', 'Г', '##аз', '##оп', '##рово', '##д', '##ът', 'Се', '##вер', '##ен', 'по', '##ток', '2', ',', 'който', 'по', 'план', 'ще', 'пре', '##нася', 'ежегодно', '55', 'ми', '##ли', '##арда', 'к', '##уб', '##ични', 'метра', 'природе', '##н', 'газ', 'от', 'Русия', 'към', 'ЕС', 'през', 'Ба', '##лт', '##ий', '##ско', 'море', ',', 'вече', 'б', '##е', 'од', '##об', '##рен', 'от', 'Германия', 'и', 'Ф', '##ин', '##ландия', '.', '[SEP]']\n",
            "input_ids = [101, 512, 26313, 58056, 55048, 10746, 13368, 52203, 32418, 10928, 10297, 20422, 123, 117, 16362, 10297, 35718, 16892, 38494, 87280, 84167, 11358, 37140, 10783, 72123, 551, 40124, 53928, 41921, 93710, 10267, 44352, 10332, 13014, 15977, 109795, 12112, 101086, 33262, 11550, 13566, 27165, 117, 45721, 542, 10205, 10430, 33276, 27332, 10332, 20823, 549, 529, 12029, 68103, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "input_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segment_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "label_id = [14, 1, 10, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 6, 1, 8, 9, 1, 1, 1, 1, 1, 8, 1, 8, 1, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "valid_ids = [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_mask = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOtyK-8Oo6d"
      },
      "source": [
        "As for each sentence and its respective labels there are the required ids and masks, in order to be proceeded by BERT, it should be transformed to acceptable tensors. This will allow also the backed by accelerator memory (like GPU, TPU).\r\n",
        "\r\n",
        "For that I will use the function `tf.data.Dataset.from_tensor_slices()`. This will also allow to apply transformations using `shuffle` and `batch` functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XvCYjc3RJ4x"
      },
      "source": [
        "def transform_to_dataset(features, examples, seed, batch_size, training_dataset=False):\r\n",
        "\r\n",
        "  all_input_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.input_ids for f in features]))\r\n",
        "  all_input_mask = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.input_mask for f in features]))\r\n",
        "  all_segment_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.segment_ids for f in features]))\r\n",
        "  all_valid_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.valid_ids for f in features]))\r\n",
        "  all_label_mask = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.label_mask for f in features]))\r\n",
        "  all_label_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.label_id for f in features]))\r\n",
        "\r\n",
        "  # Dataset using tf.data\r\n",
        "  data = tf.data.Dataset.zip(\r\n",
        "      (all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids, all_label_mask))\r\n",
        "  \r\n",
        "  number_features = len(features)\r\n",
        "\r\n",
        "  if training_dataset:\r\n",
        "      shuffled_data = data.shuffle(buffer_size=int(len(features) * 0.1),\r\n",
        "                                  seed=seed, reshuffle_each_iteration=True)\r\n",
        "      batched_data = shuffled_data.batch(batch_size)\r\n",
        "  else:\r\n",
        "      batched_data = data.batch(batch_size)\r\n",
        "\r\n",
        "  return data, batched_data, number_features"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuziyKhTiRa"
      },
      "source": [
        "validation, batched_val_data, _ = transform_to_dataset(features=val_features, examples=val_readed_data, seed=SEED, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMdQ3JB3XPWa",
        "outputId": "f0db386c-89db-41ac-a859-d33befcb601c"
      },
      "source": [
        "print(validation)\r\n",
        "print(batched_val_data)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ZipDataset shapes: ((256,), (256,), (256,), (256,), (256,), (256,)), types: (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.bool)>\n",
            "<BatchDataset shapes: ((None, 256), (None, 256), (None, 256), (None, 256), (None, 256), (None, 256)), types: (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.bool)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJdvTsQHSeM"
      },
      "source": [
        "I used the `validation` dataset preparation as example, but I'll need to do the same for the `train` and `test` datasets. For additional simplification I will create a single function that includes all these required transformative functions inside:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYVEfkCqI6rL"
      },
      "source": [
        "def data_preprocess(dataset_type, path, label_map, max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer, batch_size=BATCH_SIZE, seed=SEED):\r\n",
        "  \"\"\"\r\n",
        "  function englobelling the customized bert pre-process of data for a NER task\r\n",
        "  params: dataset_type: string with possible dataset_types: 'train', 'val', 'test'\r\n",
        "          path: the directory where the datasets are situated\r\n",
        "          label_map: desired mapping of all the available labels + including '[SEP]' and '[CLS]'\r\n",
        "  \r\n",
        "  returns: tuple of zipped dataset and batched dataset\r\n",
        "  \"\"\"\r\n",
        "  filename = f\"{dataset_type}_NER_BG.txt\" \r\n",
        "  \r\n",
        "  file = os.path.join(path, filename)\r\n",
        "\r\n",
        "  readed_data = read_data(file)\r\n",
        "\r\n",
        "  features = convert_examples_to_features(examples=readed_data, label_map=label_map, max_seq_length=max_seq_length, tokenizer=tokenizer)\r\n",
        "\r\n",
        "  training_dataset = True if dataset_type == 'train' else False\r\n",
        "\r\n",
        "  return transform_to_dataset(features=features, examples=readed_data, seed=seed, batch_size=batch_size, training_dataset=training_dataset)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KTHtA2uHf9w",
        "outputId": "17838f4a-4247-47fa-a029-834bfb95a371"
      },
      "source": [
        "# training dataset\r\n",
        "train_data, batched_train_data, train_size = data_preprocess(dataset_type='train', path=path, label_map=label_map)\r\n",
        "\r\n",
        "# validation dataset\r\n",
        "val_data, batched_val_data, _ = data_preprocess(dataset_type='val', path=path, label_map=label_map)\r\n",
        "\r\n",
        "# test dataset\r\n",
        "test_data, batched_test_data, _ = data_preprocess(dataset_type='test', path=path, label_map=label_map)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/train_NER_BG.txt\n",
            "Number of sentences: 8785\n",
            "Maximum token lenght of a sentence: 245\n",
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/val_NER_BG.txt\n",
            "Number of sentences: 1695\n",
            "Maximum token lenght of a sentence: 240\n",
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/test_NER_BG.txt\n",
            "Number of sentences: 1461\n",
            "Maximum token lenght of a sentence: 285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE2D8ffU7ugV",
        "outputId": "46bc4dc3-3b8b-4ba0-d32c-32603b2eab5d"
      },
      "source": [
        "batched_train_data.element_spec"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.bool, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJkRQQu9_npF"
      },
      "source": [
        "### Building the BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x87d-g7M87nr"
      },
      "source": [
        "Unfortunately, it is impossible to use the newly created module `bert-for-tf2` made to implement many of the GLUE tasks. Using it for GLUE task is really very user-firnedly. But as it doesn't accept all the usual \"inputs\", skipping the `input_mask`, it is not suitable at this moment for NER task.\r\n",
        "\r\n",
        "Using the BERT model as a layer with this module is really easy task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aveAC5n98inF"
      },
      "source": [
        "# !pip import bert-for-tf2\r\n",
        "# import bert\r\n",
        "\r\n",
        "# bert_params = bert.params_from_pretrained_ckpt(gs_folder_bert)\r\n",
        "# print(\"bert_params:\")\r\n",
        "# pprint(bert_params)\r\n",
        "# print(bert_params == bert_params_1)\r\n",
        "# print(l_bert)\r\n",
        "# print(bert_m)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3bVmsK1KS33"
      },
      "source": [
        "# creation of BERT as a layer\r\n",
        "# l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\r\n",
        "# model_ckpt = os.path.join(gs_folder_bert, \"bert_model.ckpt\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUGvVLJC_tPq"
      },
      "source": [
        "Unfortunately, I unserstood the problem the hard way and after losing quite lot of time in debugging I understood that I had to find different way for using BERT as a layer in my model. The original [BERT model](https://github.com/google-research/bert) created by Jacob Devlin unfortunately uses lots of old and already modified modules, so it is also unconvinient for my case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iGSOfS5_yZD"
      },
      "source": [
        "After the model is already selected, the configuration of the pretrained model should be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtbZSUnIAECB",
        "outputId": "f0fb0c8a-5add-4673-aa11-26ada6f6df37"
      },
      "source": [
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\r\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\r\n",
        "\r\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\r\n",
        "\r\n",
        "print(\"config_dict:\")\r\n",
        "pprint(config_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config_dict:\n",
            "{'attention_probs_dropout_prob': 0.1,\n",
            " 'directionality': 'bidi',\n",
            " 'hidden_act': 'gelu',\n",
            " 'hidden_dropout_prob': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_size': 3072,\n",
            " 'max_position_embeddings': 512,\n",
            " 'num_attention_heads': 12,\n",
            " 'num_hidden_layers': 12,\n",
            " 'pooler_fc_size': 768,\n",
            " 'pooler_num_attention_heads': 12,\n",
            " 'pooler_num_fc_layers': 3,\n",
            " 'pooler_size_per_head': 128,\n",
            " 'pooler_type': 'first_token_transform',\n",
            " 'type_vocab_size': 2,\n",
            " 'vocab_size': 119547}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuwguV-9xsvQ"
      },
      "source": [
        "Before putting BERT into my model, I'll demonstrate its outputs and how the input can be simulate using my customized preprocess. It will be done based on the TensorFlow Hub model chosen in teh section *\"Selecting the model and loading it\"*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2X-0x9vVYEc"
      },
      "source": [
        "bert_enc_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-FUYZ5GzA-h"
      },
      "source": [
        "The test will be done again on the `validation` dataset, resulted from the visualization used before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9MTQ74obTlh"
      },
      "source": [
        "# choosing the first sentence\r\n",
        "input_word_i, input_m, input_type_i, *others = next(iter(validation))"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_4uT_t9nv5O"
      },
      "source": [
        "input_word_ids = [\r\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.int64, name=ft)\r\n",
        "      for ft in input_word_i.numpy()]\r\n",
        "input_mask = [\r\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.int64, name=ft)\r\n",
        "      for ft in input_m.numpy()]\r\n",
        "input_type_id = [\r\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.int64, name=ft)\r\n",
        "      for ft in input_type_i.numpy()]"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN2gq28CsHBs",
        "outputId": "462c5828-d5d8-4213-bc41-1dc29a04742a"
      },
      "source": [
        "print(input_type_id)\r\n",
        "type(input_type_id[0])"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5926')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5927')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5928')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5929')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5930')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5931')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5932')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5933')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5934')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5935')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5936')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5937')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5938')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5939')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5940')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5941')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5942')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5943')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5944')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5945')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5946')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5947')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5948')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5949')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5950')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5951')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5952')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5953')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5954')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5955')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5956')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5957')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5958')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5959')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5960')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5961')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5962')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5963')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5964')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5965')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5966')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5967')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5968')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5969')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5970')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5971')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5972')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5973')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5974')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5975')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5976')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5977')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5978')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5979')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5980')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5981')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5982')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5983')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5984')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5985')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5986')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5987')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5988')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5989')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5990')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5991')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5992')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5993')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5994')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5995')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5996')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5997')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5998')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_5999')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6000')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6001')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6002')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6003')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6004')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6005')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6006')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6007')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6008')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6009')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6010')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6011')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6012')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6013')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6014')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6015')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6016')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6017')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6018')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6019')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6020')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6021')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6022')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6023')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6024')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6025')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6026')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6027')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6028')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6029')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6030')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6031')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6032')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6033')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6034')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6035')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6036')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6037')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6038')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6039')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6040')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6041')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6042')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6043')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6044')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6045')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6046')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6047')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6048')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6049')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6050')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6051')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6052')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6053')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6054')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6055')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6056')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6057')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6058')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6059')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6060')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6061')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6062')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6063')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6064')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6065')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6066')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6067')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6068')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6069')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6070')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6071')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6072')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6073')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6074')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6075')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6076')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6077')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6078')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6079')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6080')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6081')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6082')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6083')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6084')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6085')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6086')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6087')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6088')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6089')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6090')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6091')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6092')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6093')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6094')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6095')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6096')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6097')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6098')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6099')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6100')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6101')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6102')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6103')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6104')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6105')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6106')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6107')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6108')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6109')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6110')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6111')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6112')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6113')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6114')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6115')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6116')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6117')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6118')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6119')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6120')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6121')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6122')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6123')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6124')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6125')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6126')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6127')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6128')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6129')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6130')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6131')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6132')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6133')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6134')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6135')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6136')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6137')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6138')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6139')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6140')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6141')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6142')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6143')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6144')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6145')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6146')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6147')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6148')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6149')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6150')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6151')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6152')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6153')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6154')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6155')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6156')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6157')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6158')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6159')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6160')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6161')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6162')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6163')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6164')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6165')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6166')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6167')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6168')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6169')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6170')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6171')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6172')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6173')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6174')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6175')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6176')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6177')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6178')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6179')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6180')>, <KerasTensor: shape=(None,) dtype=int64 (created by layer 'input_6181')>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.engine.keras_tensor.KerasTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8LUT38ueHBY"
      },
      "source": [
        "inputs = dict(\r\n",
        "      input_word_ids=input_word_ids, input_mask=input_mask, input_type_ids=input_type_id,\r\n",
        "  )"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf_FLMOSVWfG"
      },
      "source": [
        "bert_results = bert_enc_model(inputs)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsCGcEKU5yuW"
      },
      "source": [
        "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDfHLe3B508Q"
      },
      "source": [
        "inp = dict(\r\n",
        "    input_word_ids = [tf.keras.Input(shape=(), dtype=tf.int64, name='input_word_ids')],\r\n",
        "    input_mask = [tf.keras.Input(shape=(), dtype=tf.int64, name='input_mask')],\r\n",
        "    input_type_ids = [tf.keras.Input(shape=(), dtype=tf.int64, name='input_type_ids')],\r\n",
        ")\r\n",
        "\r\n",
        "output = bert_enc_model(inp)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALaMpdTH9MJO",
        "outputId": "baa98564-f504-41eb-f58a-dc108fc368da"
      },
      "source": [
        "print(output['encoder_outputs'][-1])"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(1, None, 768), dtype=tf.float32, name=None), name='keras_layer_9/StatefulPartitionedCall:12', description=\"created by layer 'keras_layer_9'\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjPWToaCs8CR",
        "outputId": "d9af78a1-6d63-4772-944f-e8a0a5de0527"
      },
      "source": [
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\r\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\r\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\r\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\r\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\r\n",
        "print(f'Encoder Outputs Shape:{bert_results[\"encoder_outputs\"][0].shape}')\r\n",
        "print(f'Encoder Outputs Values:{bert_results[\"encoder_outputs\"][0][0, :12]}')"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Pooled Outputs Shape:(256, 768)\n",
            "Pooled Outputs Values:KerasTensor(type_spec=TensorSpec(shape=(12,), dtype=tf.float32, name=None), name='tf.__operators__.getitem_13/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_13'\")\n",
            "Sequence Outputs Shape:(256, None, 768)\n",
            "Sequence Outputs Values:KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf.__operators__.getitem_14/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_14'\")\n",
            "Encoder Outputs Shape:(256, None, 768)\n",
            "Encoder Outputs Values:KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='tf.__operators__.getitem_15/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_15'\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Po4mcBB10BR"
      },
      "source": [
        "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\r\n",
        "\r\n",
        "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`.\r\n",
        "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of it as a contextual embedding for every token in a sentence.\r\n",
        "- `encoder_outputs` are the intermediate activations of the **L** Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the `i`-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLIEi5PhVpSK"
      },
      "source": [
        "OUT_UNITS = len(label_list)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMggil-bCyi8"
      },
      "source": [
        "class BertNer(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self, bert_model,float_type, num_labels, max_seq_length, final_layer_initializer=None):\r\n",
        "        '''\r\n",
        "        bert_model : bert_model\r\n",
        "        float_type : tf.float32\r\n",
        "        num_labels : num of tags in NER task\r\n",
        "        max_seq_length : max_seq_length of tokens\r\n",
        "        final_layer_initializer : default:  tf.keras.initializers.TruncatedNormal\r\n",
        "        '''\r\n",
        "        super(BertNer, self).__init__()\r\n",
        "        \r\n",
        "        # defining the Input of the BERT layer\r\n",
        "        input_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int64, name='input_word_ids')\r\n",
        "        input_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int64, name='input_mask')\r\n",
        "        input_type_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int64, name='input_type_ids')\r\n",
        "\r\n",
        "        # defining the Outputs of the BERT model\r\n",
        "        sequence_output = bert_model(input_word_ids, input_mask,input_type_ids)\r\n",
        "\r\n",
        "        # BERT model as a layer\r\n",
        "        self.bert = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids],outputs=[sequence_output])\r\n",
        "        # self.bert = bert_model\r\n",
        "\r\n",
        "        if final_layer_initializer is not None:\r\n",
        "            initializer = final_layer_initializer\r\n",
        "        else:\r\n",
        "            initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_model.params['initializer_range'])\r\n",
        "\r\n",
        "        # adding default DROPOUT Layer\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=bert_model.params['hidden_dropout'])\r\n",
        "        \r\n",
        "        # defining the output Layer\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            num_labels, kernel_initializer=initializer, activation='softmax',name='output', dtype=float_type)\r\n",
        "    \r\n",
        "\r\n",
        "    def call(self, input_word_ids,input_mask=None,input_type_ids=None,valid_mask=None, **kwargs):\r\n",
        "        sequence_output = self.bert([input_word_ids, input_mask, input_type_ids])  # ,**kwargs)\r\n",
        "        valid_output = []\r\n",
        "        for i in range(sequence_output.shape[0]):  # shape[0] is batch_size\r\n",
        "            r = 0\r\n",
        "            temp = []\r\n",
        "            for j in range(sequence_output.shape[1]):  # shape[1] is max_seq_len\r\n",
        "                if valid_mask[i][j] == 1:\r\n",
        "                    temp = temp + [sequence_output[i][j]]\r\n",
        "                else:\r\n",
        "                    r += 1\r\n",
        "            temp = temp + r * [tf.zeros_like(sequence_output[i][j])]\r\n",
        "            valid_output = valid_output + temp\r\n",
        "        valid_output = tf.reshape(tf.stack(valid_output),sequence_output.shape)\r\n",
        "        sequence_output = self.dropout(\r\n",
        "            valid_output, training=kwargs.get('training', False))\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZhzjzz8F_9T"
      },
      "source": [
        "ner = BertNer(l_bert, tf.float32, OUT_UNITS, MAX_SEQ_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EkmS14YjyD",
        "outputId": "91cd9224-d6dc-4022-adc6-1f6c2b44cf0a"
      },
      "source": [
        "ner.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.engine.functional.Functional at 0x7f51298f7630>,\n",
              " <tensorflow.python.keras.layers.core.Dropout at 0x7f5129a15898>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f5129a15048>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc1HqRph7HFV",
        "outputId": "ccc0d334-6159-4ff2-dc8c-baa113b32296"
      },
      "source": [
        "len(ner.variables)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPB2Pf-d7Jg4"
      },
      "source": [
        "# ner.build(input_shape=(None, MAX_SEQ_LENGTH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR1PiJ_dkgdw"
      },
      "source": [
        "#############################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlov907AoLFa",
        "outputId": "9d611516-ea69-4024-aeef-78c8f80cb5ae"
      },
      "source": [
        "bert_ckpt_file_1   = os.path.join(gs_folder_bert, \"bert_model.ckpt\")\r\n",
        "bert_config_file_1 = os.path.join(gs_folder_bert, \"bert_config.json\")\r\n",
        "\r\n",
        "print(bert_ckpt_file_1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/bert_model/bert_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOjOvISpoZ3u",
        "outputId": "fa1a904a-5b6d-4be1-a46a-fb763ef03506"
      },
      "source": [
        "print(bert_config_file_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/bert_model/bert_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bliHISrEop-v",
        "outputId": "3d1c0e74-5540-4f67-9d77-83732e12e45c"
      },
      "source": [
        "with tf.io.gfile.GFile(bert_config_file_1, \"r\") as reader:\r\n",
        "      bc = bert.StockBertConfig.from_json_string(reader.read())\r\n",
        "      bert_params = bert.loader.map_stock_config_to_params(bc)\r\n",
        "      bert_m = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\r\n",
        "        \r\n",
        "input_ids      = tf.keras.layers.Input(shape=(256,), dtype='int32', name=\"input_ids\")\r\n",
        "token_type_ids = tf.keras.layers.Input(shape=(256,), dtype='int32', name=\"token_type_ids\")\r\n",
        "segment_ids    = tf.keras.layers.Input(shape=(256,), dtype='int32', name=\"segment_ids\")\r\n",
        "output         = bert_m([input_ids, token_type_ids])\r\n",
        "\r\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0KHLdauULQY"
      },
      "source": [
        "# model = tf.keras.models.Sequential([\r\n",
        "#   tf.keras.layers.InputLayer(input_shape=(None, 3)),  # [input_word_ids, input_mask, input_type_ids] \r\n",
        "#   l_bert,\r\n",
        "#   tf.keras.layers.Dense(units=OUT_UNITS, activation=\"softmax\")\r\n",
        "# ])\r\n",
        "# model.build(input_shape=(None, MAX_SEQ_LENGTH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "qQETWafw815B",
        "outputId": "66d012f7-7470-497d-a073-ef849b3e5952"
      },
      "source": [
        "tf.keras.utils.plot_model(ner, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE0AAAAoCAIAAAA0UuAOAAAABmJLR0QA/wD/AP+gvaeTAAADA0lEQVRoge2ZP0gyYRzHn+elM4WzvCj0Ll0k17yxKQjEUZcagqKtcHBpkRqCiIaWhhqjJRyClo6G/kxNJZQ+WwVCRhEIJRWadHTn7x2OpLcrfXxLfd/zPtMP/D3f5/d9fH764w4DAGoBfjW7gAbRKj7bytHa2lo+n29iKfVgdHSU53n0/vucn59vXj11YWNj4+rqSovb3n8wPT3dhHLqxvHxcTlulf40fRoL06exMH0aC9PnG319fQMDAw0ohZKXlxf8BsuylKuq+9zc3Ky1lJmZmVqX1MTCwgIAAEChUKBcQnVvGYahL6JYLB4eHtLnNwYqn4QQjuMcDkckEtEOMhqNOhwOn8+XSCQQQuPj4zzPx2Ixv98/MjKSSCQwxvf393qpqampnp6eiYkJu90eCARUVa2s9mk9S0tLDMMIgjA3N0drFN7o7e2Fzzg5Oenv78/n85lMxuv1bm9v7+7uBoPBp6cnSZKGhoYA4OHhgeO429vblZWV8/Nzv9//qZSW6XQ6AeDu7q6jo+Py8rKyml6hVCpdX1/LskwI4Xl+f3//q72Gh4ePjo60uK36SSBks9lYlmVZNhQKEUIsFsvBwUFnZydCyO12azkWi0UQhGg0enFxQaPZ3d3t8XhkWU6lUhXU9Asxxh6PByEkimI4HD49PQ0Gg1W3q+1/RZZlq9Wq3TTtnG5ubmpS0PMdNUVRrFYrTSaVT1VVFUVJp9OSJA0ODoqiKEnS2dlZqVR6fHz8kIwxzuVyz8/PiqLQiFdW07O3t7e1tfX6+koI2dnZCQQCNLtU789MJiOKYnt7u8vlWlxcBABVVSORCMdxgiDE43EAGBsbQwiFw2EAKBaLPp+vq6srmUzq1SYnJxFCs7Ozq6urGONQKFRZTQ8hxO12Mwzj9XrX19e/as4P/Vnd5//Le5/1mvuy2SzWkc1mGy+iQfV7+xe4XC749hPwHxHRMOd4Y2H6NBamT2Nh+jQWreLzj3loeXm5WXXUg3Q6XY5xebAy9nte/FMD5D9Oq/Rnq/j8DbM2GvIZ1gvgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipk897WZwBY_"
      },
      "source": [
        "def flatten_layers(root_layer):\r\n",
        "    if isinstance(root_layer, tf.keras.layers.Layer):\r\n",
        "        yield root_layer\r\n",
        "    for layer in root_layer._layers:\r\n",
        "        for sub_layer in flatten_layers(layer):\r\n",
        "            yield sub_layer\r\n",
        "\r\n",
        "def freeze_bert_layers(l_bert):\r\n",
        "    \"\"\"\r\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\r\n",
        "    \"\"\"\r\n",
        "    for layer in flatten_layers(l_bert):\r\n",
        "      print(layer)\r\n",
        "      print(layer.name)\r\n",
        "      print(layer._layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3uEBei7w0eR",
        "outputId": "22c85975-efcb-420c-cc79-a4f1c76208a7"
      },
      "source": [
        "freeze_bert_layers(ner)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.BertNer object at 0x7f51299f5a90>\n",
            "bert_ner_5\n",
            "[<tensorflow.python.keras.engine.functional.Functional object at 0x7f51298f7630>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129a15898>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129a15048>]\n",
            "<tensorflow.python.keras.engine.functional.Functional object at 0x7f51298f7630>\n",
            "model_1\n",
            "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f51299f5668>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f51299f5f60>, <tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f51299f5d68>, <bert.model.BertModelLayer object at 0x7f5129a76828>]\n",
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f51299f5668>\n",
            "input_word_ids\n",
            "[]\n",
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f51299f5f60>\n",
            "input_mask\n",
            "[]\n",
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f51299f5d68>\n",
            "input_type_ids\n",
            "[]\n",
            "<bert.model.BertModelLayer object at 0x7f5129a76828>\n",
            "bert\n",
            "[<bert.embeddings.BertEmbeddingsLayer object at 0x7f51a89f5fd0>, <bert.transformer.TransformerEncoderLayer object at 0x7f512b07a7b8>]\n",
            "<bert.embeddings.BertEmbeddingsLayer object at 0x7f51a89f5fd0>\n",
            "embeddings\n",
            "[<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f5126020fd0>, <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f5126020860>, <bert.embeddings.PositionEmbeddingLayer object at 0x7f51260207f0>, <params_flow.normalization.LayerNormalization object at 0x7f5126019b70>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5126019240>]\n",
            "<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f5126020fd0>\n",
            "word_embeddings\n",
            "[]\n",
            "<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f5126020860>\n",
            "token_type_embeddings\n",
            "[]\n",
            "<bert.embeddings.PositionEmbeddingLayer object at 0x7f51260207f0>\n",
            "position_embeddings\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5126019b70>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5126019240>\n",
            "dropout\n",
            "[]\n",
            "<bert.transformer.TransformerEncoderLayer object at 0x7f512b07a7b8>\n",
            "encoder\n",
            "[ListWrapper([<bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603c630>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603cc50>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603c8d0>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603c128>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e438>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e940>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e7b8>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1ee80>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e908>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1ed30>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f51284458d0>, <bert.transformer.SingleTransformerEncoderLayer object at 0x7f51284459b0>])]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603c630>\n",
            "layer_0\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f5128459208>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5128459080>, <bert.transformer.ProjectionLayer object at 0x7f51284590f0>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f5128459208>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f51292a3ba8>, <bert.transformer.ProjectionLayer object at 0x7f5129209828>]\n",
            "<bert.attention.AttentionLayer object at 0x7f51292a3ba8>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f512921aa20>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512921a400>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5128459cf8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5128459d30>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512921aa20>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512921a400>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5128459cf8>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5128459d30>\n",
            "dropout_1\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129209828>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129283a20>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f512944e9e8>, <params_flow.normalization.LayerNormalization object at 0x7f512944e2e8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129283a20>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f512944e9e8>\n",
            "dropout_2\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f512944e2e8>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5128459080>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51284590f0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f512843feb8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5127d36080>, <params_flow.normalization.LayerNormalization object at 0x7f5127d36550>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512843feb8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5127d36080>\n",
            "dropout_3\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5127d36550>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603cc50>\n",
            "layer_1\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512843fa58>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51297e2ef0>, <bert.transformer.ProjectionLayer object at 0x7f5129237080>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512843fa58>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f51297d6c50>, <bert.transformer.ProjectionLayer object at 0x7f51297d6ef0>]\n",
            "<bert.attention.AttentionLayer object at 0x7f51297d6c50>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f512938aa90>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512938a6d8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51297f6cc0>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51297f6780>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512938aa90>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512938a6d8>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51297f6cc0>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51297f6780>\n",
            "dropout_4\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51297d6ef0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51293b2eb8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51293bca58>, <params_flow.normalization.LayerNormalization object at 0x7f51293bc080>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51293b2eb8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51293bca58>\n",
            "dropout_5\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51293bc080>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51297e2ef0>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129237080>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129280358>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51292805f8>, <params_flow.normalization.LayerNormalization object at 0x7f51292807b8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129280358>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51292805f8>\n",
            "dropout_6\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51292807b8>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603c8d0>\n",
            "layer_2\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51297e3d30>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51292bbbe0>, <bert.transformer.ProjectionLayer object at 0x7f51292a1710>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51297e3d30>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f51292a1ef0>, <bert.transformer.ProjectionLayer object at 0x7f51292af0f0>]\n",
            "<bert.attention.AttentionLayer object at 0x7f51292a1ef0>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51292af5f8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51292af908>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51292afc18>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51292afeb8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51292af5f8>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51292af908>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51292afc18>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51292afeb8>\n",
            "dropout_7\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51292af0f0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129a489b0>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129a465c0>, <params_flow.normalization.LayerNormalization object at 0x7f5129a46e48>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129a489b0>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129a465c0>\n",
            "dropout_8\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5129a46e48>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51292bbbe0>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51292a1710>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51296b3ac8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51296b3d68>, <params_flow.normalization.LayerNormalization object at 0x7f51296b3f28>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51296b3ac8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51296b3d68>\n",
            "dropout_9\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51296b3f28>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f512603c128>\n",
            "layer_3\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51296b34e0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129698940>, <bert.transformer.ProjectionLayer object at 0x7f5127003240>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51296b34e0>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f51270056a0>, <bert.transformer.ProjectionLayer object at 0x7f5127005860>]\n",
            "<bert.attention.AttentionLayer object at 0x7f51270056a0>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5127005d68>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5127017208>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51270173c8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5127017668>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5127005d68>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5127017208>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51270173c8>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5127017668>\n",
            "dropout_10\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5127005860>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129922588>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f512992b550>, <params_flow.normalization.LayerNormalization object at 0x7f512992b748>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129922588>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f512992b550>\n",
            "dropout_11\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f512992b748>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129698940>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5127003240>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129595278>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129595518>, <params_flow.normalization.LayerNormalization object at 0x7f51295956d8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129595278>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129595518>\n",
            "dropout_12\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51295956d8>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e438>\n",
            "layer_4\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512958add8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129ac0b00>, <bert.transformer.ProjectionLayer object at 0x7f5129ac4f98>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512958add8>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f5129acee10>, <bert.transformer.ProjectionLayer object at 0x7f5129acefd0>]\n",
            "<bert.attention.AttentionLayer object at 0x7f5129acee10>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129ad4518>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129ad4828>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129ad4b38>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129ad4dd8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129ad4518>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129ad4828>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129ad4b38>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129ad4dd8>\n",
            "dropout_13\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129acefd0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129aa2e10>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129aa9cc0>, <params_flow.normalization.LayerNormalization object at 0x7f5129aa9eb8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129aa2e10>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129aa9cc0>\n",
            "dropout_14\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5129aa9eb8>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129ac0b00>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129ac4f98>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51298a19e8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51298a1c88>, <params_flow.normalization.LayerNormalization object at 0x7f51298a1e48>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51298a19e8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51298a1c88>\n",
            "dropout_15\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51298a1e48>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e940>\n",
            "layer_5\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51298a1400>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129601860>, <bert.transformer.ProjectionLayer object at 0x7f5129605160>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51298a1400>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f512960e5c0>, <bert.transformer.ProjectionLayer object at 0x7f512960e780>]\n",
            "<bert.attention.AttentionLayer object at 0x7f512960e5c0>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f512960ec88>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512960ef98>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129617278>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51296175c0>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512960ec88>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512960ef98>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129617278>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51296175c0>\n",
            "dropout_16\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f512960e780>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51297a24a8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51297aa2b0>, <params_flow.normalization.LayerNormalization object at 0x7f51297aa550>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51297a24a8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51297aa2b0>\n",
            "dropout_17\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51297aa550>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129601860>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129605160>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129100198>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129100438>, <params_flow.normalization.LayerNormalization object at 0x7f51291005f8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129100198>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129100438>\n",
            "dropout_18\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51291005f8>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e7b8>\n",
            "layer_6\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512913bb70>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512911dfd0>, <bert.transformer.ProjectionLayer object at 0x7f5129505e80>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512913bb70>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f5129508d30>, <bert.transformer.ProjectionLayer object at 0x7f5129508eb8>]\n",
            "<bert.attention.AttentionLayer object at 0x7f5129508d30>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129511438>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129511748>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129511a58>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129511cf8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129511438>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129511748>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129511a58>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129511cf8>\n",
            "dropout_19\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129508eb8>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129b657f0>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129b68be0>, <params_flow.normalization.LayerNormalization object at 0x7f5129b68dd8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129b657f0>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129b68be0>\n",
            "dropout_20\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5129b68dd8>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512911dfd0>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129505e80>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5126058908>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5126058ba8>, <params_flow.normalization.LayerNormalization object at 0x7f5126058d68>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5126058908>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5126058ba8>\n",
            "dropout_21\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5126058d68>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1ee80>\n",
            "layer_7\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f5126062748>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5126079780>, <bert.transformer.ProjectionLayer object at 0x7f5129940080>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f5126062748>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f51299444e0>, <bert.transformer.ProjectionLayer object at 0x7f51299446a0>]\n",
            "<bert.attention.AttentionLayer object at 0x7f51299444e0>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129944ba8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129944eb8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512994b208>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f512994b4a8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129944ba8>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129944eb8>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512994b208>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f512994b4a8>\n",
            "dropout_22\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51299446a0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129a15f60>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129a22390>, <params_flow.normalization.LayerNormalization object at 0x7f5129a22588>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129a15f60>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129a22390>\n",
            "dropout_23\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5129a22588>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5126079780>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129940080>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51299d90b8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51299d9358>, <params_flow.normalization.LayerNormalization object at 0x7f51299d9518>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51299d90b8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51299d9358>\n",
            "dropout_24\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51299d9518>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1e908>\n",
            "layer_8\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51299d3a90>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51299f5ef0>, <bert.transformer.ProjectionLayer object at 0x7f51299fcda0>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f51299d3a90>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f512997fc50>, <bert.transformer.ProjectionLayer object at 0x7f512997fe10>]\n",
            "<bert.attention.AttentionLayer object at 0x7f512997fc50>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129986358>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129986668>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129986978>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129986c18>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129986358>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129986668>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129986978>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129986c18>\n",
            "dropout_25\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f512997fe10>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51298d4710>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51298dcb00>, <params_flow.normalization.LayerNormalization object at 0x7f51298dccf8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51298d4710>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51298dcb00>\n",
            "dropout_26\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51298dccf8>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51299f5ef0>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51299fcda0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129854828>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129854ac8>, <params_flow.normalization.LayerNormalization object at 0x7f5129854c88>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129854828>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129854ac8>\n",
            "dropout_27\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5129854c88>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f5127d1ed30>\n",
            "layer_9\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f5129854240>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51298716a0>, <bert.transformer.ProjectionLayer object at 0x7f51298784a8>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f5129854240>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f5129801400>, <bert.transformer.ProjectionLayer object at 0x7f51298015c0>]\n",
            "<bert.attention.AttentionLayer object at 0x7f5129801400>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129801ac8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129801dd8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f5129808128>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51298083c8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129801ac8>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129801dd8>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129808128>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51298083c8>\n",
            "dropout_28\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51298015c0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129749e80>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51297572b0>, <params_flow.normalization.LayerNormalization object at 0x7f51297574a8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129749e80>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51297572b0>\n",
            "dropout_29\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51297574a8>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51298716a0>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51298784a8>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f512964ef98>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5129652278>, <params_flow.normalization.LayerNormalization object at 0x7f5129652438>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512964ef98>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129652278>\n",
            "dropout_30\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5129652438>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f51284458d0>\n",
            "layer_10\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512964e9b0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512966ee10>, <bert.transformer.ProjectionLayer object at 0x7f5129673cc0>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512964e9b0>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f512967ab70>, <bert.transformer.ProjectionLayer object at 0x7f512967ad30>]\n",
            "<bert.attention.AttentionLayer object at 0x7f512967ab70>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51295c3278>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51295c3588>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51295c3898>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51295c3b38>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51295c3278>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51295c3588>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51295c3898>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51295c3b38>\n",
            "dropout_31\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f512967ad30>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5128f0c630>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f5128f169e8>, <params_flow.normalization.LayerNormalization object at 0x7f5128f16c18>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5128f0c630>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5128f169e8>\n",
            "dropout_32\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f5128f16c18>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512966ee10>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f5129673cc0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f512858e748>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f512858e9e8>, <params_flow.normalization.LayerNormalization object at 0x7f512858eba8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512858e748>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f512858e9e8>\n",
            "dropout_33\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f512858eba8>\n",
            "LayerNorm\n",
            "[]\n",
            "<bert.transformer.SingleTransformerEncoderLayer object at 0x7f51284459b0>\n",
            "layer_11\n",
            "[<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512858e160>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51285af470>, <bert.transformer.ProjectionLayer object at 0x7f51285b73c8>]\n",
            "<bert.transformer.TransformerSelfAttentionLayer object at 0x7f512858e160>\n",
            "attention\n",
            "[<bert.attention.AttentionLayer object at 0x7f51285bc320>, <bert.transformer.ProjectionLayer object at 0x7f51285bc4e0>]\n",
            "<bert.attention.AttentionLayer object at 0x7f51285bc320>\n",
            "self\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51285bc9e8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f51285bccf8>, <tensorflow.python.keras.layers.core.Dense object at 0x7f512953f048>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f512953f2e8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51285bc9e8>\n",
            "query\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51285bccf8>\n",
            "key\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f512953f048>\n",
            "value\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f512953f2e8>\n",
            "dropout_34\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51285bc4e0>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f5129304da0>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51293131d0>, <params_flow.normalization.LayerNormalization object at 0x7f51293133c8>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129304da0>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51293131d0>\n",
            "dropout_35\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51293133c8>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51285af470>\n",
            "intermediate\n",
            "[]\n",
            "<bert.transformer.ProjectionLayer object at 0x7f51285b73c8>\n",
            "output\n",
            "[<tensorflow.python.keras.layers.core.Dense object at 0x7f51293c5eb8>, <tensorflow.python.keras.layers.core.Dropout object at 0x7f51293cb198>, <params_flow.normalization.LayerNormalization object at 0x7f51293cb358>]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f51293c5eb8>\n",
            "dense\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f51293cb198>\n",
            "dropout_36\n",
            "[]\n",
            "<params_flow.normalization.LayerNormalization object at 0x7f51293cb358>\n",
            "LayerNorm\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dropout object at 0x7f5129a15898>\n",
            "dropout_5\n",
            "[]\n",
            "<tensorflow.python.keras.layers.core.Dense object at 0x7f5129a15048>\n",
            "output\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6IxgfR7INZo"
      },
      "source": [
        "#####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3xorHTuUx5l"
      },
      "source": [
        "### Set up the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdXW6hI4lNP"
      },
      "source": [
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). As described in the prodecure of [Fine-Tuning Bert in Tensorflow Tutorials](https://www.tensorflow.org/official_models/fine_tuning_bert#set_up_the_optimizer): \"BERT adopts the Adam optimizer with weight decay (aka \"AdamW\"). It also employs a learning rate schedule that firstly warms up from 0 and then decays to 0.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhTFM4hEVXd9",
        "outputId": "bb86c5b5-4abd-465f-c8d5-b3458b0ae473"
      },
      "source": [
        "# set_up steps\r\n",
        "\r\n",
        "steps_per_epoch = int(train_size / BATCH_SIZE)\r\n",
        "\r\n",
        "num_train_steps = steps_per_epoch * EPOCHS\r\n",
        "\r\n",
        "warmup_steps = int(WARM_UP_PROPORTION * num_train_steps)\r\n",
        "\r\n",
        "print(warmup_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1Cryrra5au"
      },
      "source": [
        "It is commonly observed that a monotonically decreasing learning rate, whose degree of change is carefully chosen, results in a better performing model (source [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay)). The PolynomialDecay schedule applies a polynomial decay function to an optimizer step, given a provided initial_learning_rate, to reach an end_learning_rate in the given decay_steps. \r\n",
        "\r\n",
        "The base learning rate schedule used here is a linear decay to zero over the training run, visible also on the graph below :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "C2vbNw8PaFlM",
        "outputId": "ba0dc2a1-cd1c-4102-8c77-e693180c8440"
      },
      "source": [
        "decay_schedule  = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=LEARNING_RATE,\r\n",
        "                                                                 decay_steps=num_train_steps,\r\n",
        "                                                                 end_learning_rate=0.0)\r\n",
        "\r\n",
        "plt.plot([decay_schedule(n) for n in range(num_train_steps)])\r\n",
        "plt.xlabel('number of training steps')\r\n",
        "plt.ylabel('learning rate decay schedule')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDAQNTfdpbj3"
      },
      "source": [
        "Using `nlp.optimization` `WarmUp` class, the definition of the warm-up schedule is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "bKLedlUkIN0X",
        "outputId": "813c0c17-fb14-4392-edfe-692f6836e0fc"
      },
      "source": [
        "warmup_schedule = WarmUp(initial_learning_rate=decay_schedule(warmup_steps),\r\n",
        "                         decay_schedule_fn=decay_schedule,\r\n",
        "                         warmup_steps=warmup_steps)\r\n",
        "  \r\n",
        "plt.plot([warmup_schedule(n) for n in range(num_train_steps)])\r\n",
        "plt.xlabel('number of training steps')\r\n",
        "plt.ylabel('learning rate with warm-up and decay schedule')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxrmrY_pw8a"
      },
      "source": [
        "It warms up to the `initial_learning_rate` following the learning rate level at the moment the decay schedule calculates it depending on the number of training steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaUPJZjLnClB"
      },
      "source": [
        "optimizer = AdamWeightDecay(\r\n",
        "            learning_rate=warmup_schedule,\r\n",
        "            weight_decay_rate=WEIGHT_DECAY,\r\n",
        "            epsilon=ADAM_EPSILON,\r\n",
        "            exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\r\n",
        "# default values: beta_1=0.9, beta_2=0.999,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J3vnfOBnGt3",
        "outputId": "e5849ea4-1582-42d0-b431-df4a0f430d54"
      },
      "source": [
        "type(optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "official.nlp.optimization.AdamWeightDecay"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dbwhrdzl7bR"
      },
      "source": [
        "### Select loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwePPs-RmBwD"
      },
      "source": [
        "Select metrics to measure the loss and the accuracy of the model it the next step. These metrics accumulate the values over epochs and then print the overall result. Generally, no matter the dataset, the performance of the NER system is commonly measured by the F1-score.\r\n",
        "\r\n",
        "F1 score is the harmonic mean of precision and recall. Output range is `[0, 1]`.\r\n",
        "$$F_1 = 2 . \\frac{precision . recall}{recision + recall}$$\r\n",
        "\r\n",
        "In the `tensorflow_addons.metrics` module `F1Score` exists as ready-to-use option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Ww8ekymFzT"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\r\n",
        "\r\n",
        "f1_metric = F1Score(OUT_UNITS, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUAqR5XpJo_u"
      },
      "source": [
        "def dice_loss(y_true, y_predicted):\r\n",
        "\r\n",
        "    y_true_f = tf.layers.flatten(y_true)\r\n",
        "    y_pred_f = tf.layers.flatten(y_predicted)\r\n",
        "\r\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\r\n",
        "\r\n",
        "    if loss_type == 'jaccard':\r\n",
        "        union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))\r\n",
        "\r\n",
        "    elif loss_type == 'sorensen':\r\n",
        "        union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\r\n",
        "    \r\n",
        "    return (2. * intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())\r\n",
        "\r\n",
        "    # num_sum = 2.0 * tf.reduce_sum(y_true * y_predicted) + tf.keras.backend.epsilon()\r\n",
        "    # den_sum = tf.reduce_sum(y_true) + tf.reduce_sum(y_predicted) + tf.keras.backend.epsilon()\r\n",
        "    # # den_sum = tf.reduce_sum(tf.square(y_predicted)) + tf.reduce_sum(tf.square(y_true)) + tf.keras.backend.epsilon()\r\n",
        "\r\n",
        "    # return np.ndarray([1 - num_sum/den_sum], dtype = 'float32')\r\n",
        "\r\n",
        "loss = dice_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-t8De6u_fZn"
      },
      "source": [
        "The function `train_step` will be responsible on each epoch to:\r\n",
        "1. iterate over each example in the training Dataset grabbing its features (x) and label (y).\r\n",
        "2. Using the example's features, make a prediction and compare it with the label. To be able to compare it correctly the label_mask shouls be applied.\r\n",
        "3. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\r\n",
        "4. Use the optimizer to update the model's variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaevRo3dXex9"
      },
      "source": [
        " def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask, loss_fct=loss):\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "      logits = ner(input_ids, input_mask, segment_ids, valid_ids, training=True)\r\n",
        "        # batchsize, max_seq_length, num_labels\r\n",
        "      label_ids_masked = tf.boolean_mask(label_ids, label_mask)\r\n",
        "      logits_masked = tf.boolean_mask(logits, label_mask)\r\n",
        "      scce_loss = loss_fct(label_ids_masked, logits_masked)\r\n",
        "\r\n",
        "  gradients = tape.gradient(scce_loss, ner.trainable_variables)\r\n",
        "  optimizer.apply_gradients(list(zip(gradients, ner.trainable_variables)))\r\n",
        "  return scce_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-x_i17q4DP1",
        "outputId": "f5e9a29b-ab14-427a-b193-7f1173d6dc37"
      },
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOwWJKAcFzme"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A0-7Q3tFx2c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "id": "rrGLX3FbF6at",
        "outputId": "aa901b45-2ccc-4cbc-b6ab-7ae5ad9e0ef2"
      },
      "source": [
        "for epoch in range(1):\r\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\r\n",
        "  \r\n",
        "  # Training loop - using batches\r\n",
        "  for (input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask) in batched_train_data:\r\n",
        "    max_seq_len = 128\r\n",
        "    l_input_ids      = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\n",
        "    l_token_type_ids = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\n",
        "\r\n",
        "    output = l_bert([l_input_ids, l_token_type_ids])          # [batch_size, max_seq_len, hidden_size]\r\n",
        "    model = tf.keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\r\n",
        "    model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\r\n",
        "    \r\n",
        "    # loss = train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask)\r\n",
        "\r\n",
        "    # epoch_loss_avg.update_state(loss)\r\n",
        "    # f1_metric.update_state()\r\n",
        "    print(f'loss : {epoch_loss_avg.result()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-3bf80f68319e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ml_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_token_type_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# [batch_size, max_seq_len, hidden_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_token_type_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/bert/model.py:79 call  *\n        embedding_output = self.embeddings_layer(inputs, mask=mask, training=training)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer embeddings expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'Placeholder:0' shape=(None, 128) dtype=int32>, <tf.Tensor 'Placeholder_1:0' shape=(None, 128) dtype=int32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90VJKgLKiXdq"
      },
      "source": [
        "## Entity Linking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m11CTxXgibRt"
      },
      "source": [
        "The Entity Linking (EL) process transforms amiguous textual mention to a unique identifier by looking at the context in which the mention occurs. Thus it can be looked as 2 step process after the NER:\r\n",
        "1. Creation of Entity Linker - list of candidates for each mention generation\r\n",
        "2. Reduce the list to the final ID that represents the correct name.\r\n",
        "\r\n",
        "This is generally the method used in `spacy` module.\r\n",
        "\r\n",
        "Another option used for this is used in `deeppavlov` module (http://docs.deeppavlov.ai/en/master/features/models/entity_linking.html)where:\r\n",
        "1. NER is fed to tf-idf Vectorizer and the resulting sparse vector is converted to dense vector.\r\n",
        "2. A library called Faiss (https://github.com/facebookresearch/faiss) is used to find the k-nearest neighbours for tf-idf vector in the matrix where each row is a tf-idf vectors of words in entity titles.\r\n",
        "3. entities are ranked by number of relations in Wikidata (number of outgoing edges of nodes in the knowledge graph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Rwzz77iHaP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6t76Ft8iN9F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev807Omviqes"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaJkcSbuir8H"
      },
      "source": [
        "## Conclusion and Future Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaQ_6W4aixJU"
      },
      "source": [
        "Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIMcQMsiv_2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI_01lUDizGb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8g5xj1iz6G"
      },
      "source": [
        "## Ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJsVMxR_i5Bl"
      },
      "source": [
        "**Reconstructing NER Corpora: a Case Study on Bulgarian** - Iva Marinova, Laska Laskova, Petya Osenova, Kiril Simov, Alexander Popov - Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 4647–4652, Marseille, 11–16 May 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcmS0ntOi40-"
      },
      "source": [
        "**Tuning Multilingual Transformers for Named Entity Recognition on\r\n",
        "Slavic Languages** - Mikhail Arkhipov, Maria Trofimova, Yuri Kuratov, Alexey Sorokin - Neural Networks and Deep Learning Laboratory, Moscow Institute of Physics and Technology, Faculty of Mathematics and Mechanics, Moscow State University - Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 89–93, Florence, Italy, 2 August 2019. - https://www.aclweb.org/anthology/W19-3712.pdf\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFStE4fNi4dj"
      },
      "source": [
        "**BERT: Pre-training of Deep Bidirectional Transformers for\r\n",
        "Language Understanding** - Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova - Google AI Language, 24 May 2019 - https://arxiv.org/pdf/1810.04805.pdf - https://github.com/google-research/bert\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWT-xT0ex9P1"
      },
      "source": [
        "**Attention Is All You Need** - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin, 6 Dec 2017 - https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB8R_2r-CnYX"
      },
      "source": [
        "**A Survey on Deep Learning for Named Entity Recognition** - Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li - IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 18 Mar 2020 - https://arxiv.org/pdf/1812.09449v3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxlANqfFcWY"
      },
      "source": [
        "**Zero-Resource Cross-Domain Named Entity Recognition** - Zihan Liu, Genta Indra Winata, Pascale Fung - Center for Artificial Intelligence Research (CAiRE), Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, 19 May 2020 - https://arxiv.org/pdf/2002.05923.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbLxoqMU-Eia"
      },
      "source": [
        "**Exploring Cross-sentence Contexts for Named Entity Recognition with BERT** - Jouni Luoma, Sampo Pyysalo - Turku NLP group, University of Turku, Finland, 2 Jun 2020 - https://arxiv.org/pdf/2006.01563v1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbaNql7k2j9y"
      },
      "source": [
        "**NER with BERT in Action** - Bill Huang - July 30, 2019- https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcNDPUIv4Fm3"
      },
      "source": [
        "**The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)** - Jay Alammar blog - http://jalammar.github.io/illustrated-bert/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlvqgC_rUXm5"
      },
      "source": [
        "**Deep contextualized word representations** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer - Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science & Engineering, University of Washington, 22 Mar 2018 - https://arxiv.org/pdf/1802.05365.pdf\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPzo1mNoU0F-"
      },
      "source": [
        "**Improving Language Understanding by Generative Pre-Training** - Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever - Open AI - https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LYtPCX2usln"
      },
      "source": [
        "**Introduction to the conll-2003 shared task: Language independent named entity recognition.** - Tjong Kim Sang, E. F. and De Meulder, F. (2003) - https://arxiv.org/pdf/cs/0306050.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m28l7lsJxT-"
      },
      "source": [
        "**Large-Scale Multi-Label Text Classification on EU Legislation** - Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis,\r\n",
        "Ion Androutsopoulos - Department of Informatics, Athens University of Economics and Business, Greece (June 2019) - https://arxiv.org/pdf/1906.02192v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YotmtdHUvqxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}