{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER and Entity Linking in legal documents in Bulgarian language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "18QqUo4aVcjoZfepAgU-I6HNL1JRqm6Eu",
      "authorship_tag": "ABX9TyO6+GUrMOiYtEdSkbmSiB4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikadie/AI-repo/blob/master/NER_and_Entity_Linking_in_legal_documents_in_Bulgarian_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlsqWrBmYiEz"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnhWNKNacJMU",
        "outputId": "bdd2595d-052d-4f2a-b028-0d8d1fc06a41"
      },
      "source": [
        "import os\r\n",
        "from random import randint, seed\r\n",
        "from time import time\r\n",
        "import json\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import pickle\r\n",
        "\r\n",
        "!pip install nose\r\n",
        "\r\n",
        "from nose.tools import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 27.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 20.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81kB 18.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 17.1MB/s \n",
            "\u001b[?25hInstalling collected packages: nose\n",
            "Successfully installed nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLfOEgGz-N7w",
        "outputId": "12ffa6ff-20c4-4036-bfe6-90f6f0fa673e"
      },
      "source": [
        "!pip install -q -U tf-models-official\r\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 16.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 79kB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 47.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 57.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 58.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.4MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 16.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsaasJH4cY9r"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "\r\n",
        "from official.nlp import bert\r\n",
        "from official.nlp.bert.tokenization import FullTokenizer\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7rhmWFXKizg"
      },
      "source": [
        "# import sys\r\n",
        "\r\n",
        "# !test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\r\n",
        "# if not 'bert_repo' in sys.path:\r\n",
        "#   sys.path += ['bert_repo']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeYRuoq_KrTp"
      },
      "source": [
        "# # import python modules defined by BERT\r\n",
        "# import modeling"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-NtMro9h4Se"
      },
      "source": [
        "from official.nlp.optimization import AdamWeightDecay, WarmUp\r\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twPwH61fYk9j"
      },
      "source": [
        "# NER and Entity Linking in unstructured legal documents in Bulgarian language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otloh6UrcelW"
      },
      "source": [
        "##### Final exam report\r\n",
        "\r\n",
        "*Viktor Belchev - student*\r\n",
        "\r\n",
        "*Deep Learning - Software University*\r\n",
        "\r\n",
        "*February 2021*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ZAILvGhvbs"
      },
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQ76Jofh7sb"
      },
      "source": [
        "Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rR9jyxPiPpb"
      },
      "source": [
        "In all legal documents there is usage of citation of different laws or other juridical terms often hidden behind some abbreviations. While this aims to make the text shorter and clearer it is mostly causing troubles in understanding and translation to simple language not only to regular persons, but sometimes even lawyers and people with juridical background feel lost. Therefore, it often requires an additional research in the legal litterature. At this stage another problem might occur - the correct decoding of abbreviated terms can become obstacle on top of the the overall understanding of the information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SDwDTdFmS0a"
      },
      "source": [
        "In this paper, I will try to use the modern approach of Deep Learning to create a helpful tool that overcomes these problems. Using the state-of-the-art available models in the field of Natual Language Processing like BERT and an abbreviation list available at this stage, I will try to achieve an acceptable accuracy in this task for Bulgarian language that is known as combination of two different tasks: Named Entity Recognition and Entity Linking. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC38oPP9iDuY"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HysH52mHqc3n"
      },
      "source": [
        "Generally, in Natural Language Processing (further, NLP) the process of disambiguation of terms is known as Entity Linknig (further, EL), which goes hand in hand with another operation called Named Entity Recognition (further, NER). As explained by Iva Marinova in her **\"Reconstructing NER Corpora: a Case Study on Bulgarian\"** while in the field of Deep Learning \r\n",
        "these two related tasks are considered to be well covered in\r\n",
        "NLP for Germanic, Romance and other language groups,\r\n",
        "they are still under-resourced for the Slavic languages, especially from a multilingual perspective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tppf-Tke1evN"
      },
      "source": [
        "Usually, the order of application of both tasks is by starting with NER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoh_BX7uu4CJ"
      },
      "source": [
        "The purpose of NER is to tag words in a sentences based on some predefined tags, in order to extract some important information of the sentence, like for instanse names, geographical locations, dates, currency etc.\r\n",
        "In NER, each token in the sentence will get tagged with a label, the label will tell the specific meaning of the token. In that way, through NER, we can analyze the sentence with more details and extract the important information.\r\n",
        "\r\n",
        "There are two popular approaches for NER:\r\n",
        "- multi-class classification based where NER is treated as a multi-class classification process, and we can use some text classification method to label the token.\r\n",
        "- Conditional Random Field(CRF) based method labels the token taking context into account, then predicts sequences of labels for sequences of sentence token then get the most reasonable one. It is a probabilistic graphical model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy19WENNgfHb"
      },
      "source": [
        "The identification of named entity mentions in texts is often implemented using a sequence tagger, where each token is labeled with an BIO tag, indicating whether the token begins a named entity — (B-), whether it is inside of a named entity (I-), or outside of a named entity (O-). This type of annotation has been proposed for the first time at CoNLL-2003 dataset created for NER (Tjong Kim Sang and De Meulder, 2003). There are other tag notation types. For instance, each token can be predicted with a tag indicated by B-(begin), I-(inside), E-(end), S-(singleton) of a named entity with its type, or O-(outside) of named entities. But, I will stick to BIO format of representation for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6VoY4TeHPNa"
      },
      "source": [
        "Entity linking can be applied rigth after the NER task is performed althought in some papers on this topic there is proposal to do it in parallel (jointly) for each token, so that each subtask benefits from the partial output of the other subtask, and thus alleviate error propagations that are unavoidable in pipeline settings. \r\n",
        "Generally, EL is the task of mapping words from text (e.g. names of persons, locations and organizations) to entities from the target knowledge base. For this pupose I use a document containing most of the existing abbreviations used in legal documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNl8XkxiQ5b"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Nzf5h0KOdV"
      },
      "source": [
        "Usually, no matter the specific task, Deep Learning models creation is based on big data for training, validation and test. For Bulgarian language generally such data could be available if we start scraping web pages, which is huge amount of work. But this is only one side of the hidden obstacles - the effectiveness of the model created for the task is a real challenge on its own. \r\n",
        "\r\n",
        "Luckily, after the publication of the famous paper called \"Attention is all you need\" by Vaswani and the \"appearance\" of *Transformer*, there is a huge advancement in the model creation compared to previous usage of recurrence (RNN), Bidirectional Lont-Short Term Memory units (BiLSTM), convolutions (CNN) and CRF. Transformer utilizes stacked self-attention and pointwise, fully connected layers to build basic blocks for encoder and decoder.  Experiments on various tasks show Transformers to be superior in quality while requiring significantly less time to train.\r\n",
        "\r\n",
        "Based mostly on transformer, it already exists pre-trained models that provide results pretty close to humans on some general tasks. Some of the most used methods are ELMo(Embeddings from Language Models), OpenAI GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers)... \r\n",
        "\r\n",
        "It is important to underline that these state-of-the-art models use specific representation of the text, called embeddings, usually so called *hybrid representation* of text in low dimensional real-valued dense vectors. It is called *hybrid* as it uses *Word-level* and *Character-level* representation along with some additional features, where each dimensions represents a latent feature. This way it also captures the semantic and syntactic properties of words, but also the context for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlIsgqLVaxN"
      },
      "source": [
        "In recent years, the advancements of NLP in general and NER in particular has been greatly influenced by deep transfer learning methods capable of creating contextual representations of words, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created. Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context – sentences around the sentence of interest – to improve sequence tagging performance.\r\n",
        "\r\n",
        "Precisely for the fact of using this cross-sentence context, but also with the advantage to be pre-trained on Bulgarian texts, in this notebook, I focus on the recent BERT deep transfer learning models based on self-attention and the transformer architecture. BERT uses a fixed-size window that limits the amount of text that can be input to the model at one time. The model maximum window size, or maximum sequence length, is fixed during pre-training, with 512 wordpieces a common choice. This window fits dozens of typical sentences of input at a time, allowing the inclusion of extensive sentence context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejQVdKrgiZT"
      },
      "source": [
        "There are many advantages that pushed me towards usage of BERT. To enumerate some, I would say that it provides:\r\n",
        "1. quicker development\r\n",
        "2. overcome the problem of missing data for training, which is generally the case for Bulgarian\r\n",
        "3. state-of-the-art better results - BERT is built on top of a number of clever ideas considered top in NLP community in latest years – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).\r\n",
        "\r\n",
        "BERT is also one of the preferred model giving the best results used by Ilias Chalkidis et al. when dealing with  Large-Scale Multi-Label Text Classification (LMTC) in the legal domain (EU legislation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaxZDnSrGtZ0"
      },
      "source": [
        "On the other hand, there are some disadvantages, like:\r\n",
        "1. it is very large. The LARGE version of BERT would provide better results, but unfortunately that would require bigger computational ability and time.\r\n",
        "2. Even when using the BASE version, it remains slow for fine-tuning.\r\n",
        "3. The multilingual version that I need to use cannot be disitilled - the vocabulary used for fine-tuning of BERT must remain the original one.\r\n",
        "4. It uses a specific and a bit complicated jargon (domain-specific language), meaning that the tokenization with BERT should be done with BERT Tokenizer.\r\n",
        "\r\n",
        "The last two disadvantages represent in fact the specifity and maybe the strength of BERT. Its vocabulary is indeed fixed, but it has the capability to break down the unknown word into subwords and makes a token out of each subword (if subword exists in the vocabulary). In case the subword do not exist in the vocabulary it can continue spliting it into subwords down to a character level. To recognize the subword it prepends it with \"##\" flag, exceot for the first subword.\r\n",
        "\r\n",
        "On its turn the subword split would create a problem with labeling. Generally, in the test and train part each word is tagged. If an unknown word is splitted to subwords, a specific tag should be used for it, that would indicate that the tag valid for this word (the initial whole word) would be the one given to the first subword (original word) and a specific tag would be assigned to subwords after the first one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Z6ezlroorO"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wzBDQNrAdGS"
      },
      "source": [
        "Before getting to the problem of tags given to subwords, we need a dataset, big enough, that can be used to fine-tune our BERT model. This dataset should implement the following requirements:\r\n",
        "   - it must be created for a NER task;\r\n",
        "   - it must be in Bulgarian;\r\n",
        "   - it must contain special annotations (tags) for recognition of legal phrases;\r\n",
        "   - it must be big enough to train deep network model;\r\n",
        "   - ideally it should have a train, validation and test datasets.\r\n",
        "\r\n",
        "Well, the first four requirements need to be mandatory fulfilled. After all if the dataset is big enough there are ways and methods to make a consise split for train, validation and test datasets. \r\n",
        "\r\n",
        "But it is hard task to implement all four requirements. In fact, I was not able to find such dataset on Internet. Luckily, there is one dataset recently created for NER task, which was in Bulgarian - the dataset done by Iva Marinova et al. pesented in May 2020. The dataset is available at https://github.com/usmiva/bg-ner. With it, I could cover half of the requirements for my task. Unfortunately, as it was not created for utilization on legal texts, there were not a specific tag for legal phrases inside. Still, it was the best one I could find. Therefore, I decided to use it as a base, a starting point, and add to it the required information covering the legal part gathered by me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqEl2ZSDprp"
      },
      "source": [
        "But before start adding information, let me reveal what and how is implemented inside, in order to decide at what level it will suit me and how to add the missing information.\r\n",
        "\r\n",
        "The original Bulgarian corpus consists of 916 text files extracted from various news websites. The training dataset contains information on two topics – Brexit and the trial of the Pakistani Christian Asia Bibi, accused of blasphemy, while the main subjects for the test data are the Nord Stream 2 project and the recent developments in RyanAir’s business history.\r\n",
        "\r\n",
        "The type of annotation used inside followed the format used for the first time at CoNLL-2003 but used only the first and the last column (ommitting the part-of-speech tag and synctatic chunk tag) - meaning that the input files were segmented into sentences and tokens per line (first column), and each token was combined with its corresponding Named Entity tag (the second column). The NE tags were of type person (PER), organization (ORG), location (LOC), product (PRO), and event (EVT) and each of them had a prefix using the BIO format. Like in most NER tasks, NEs are considered to be non-recursive, non- overlapping, and whenever one NE is embedded in another NE, only the top-most entity is annotated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLkW9vyiDgi8"
      },
      "source": [
        "The 2 files available for download were 2 text files (.txt) - one train file with 220 700 lines and one test file with ~65 000 lines.\r\n",
        "\r\n",
        "Well, armed with this information, it was obvious that the missing part was for legal phrases, thus missing tag for tham. I decided that I could simply add a NE tag LAW. After a quick review there were only few word that could match this new tag in the existing dataset.\r\n",
        "\r\n",
        "Therefore, I added to the training file 117 documents taken from the \"Decision Register\" of [The Administrative Court of Sofia City (ASCS)](http://www.admincourtsofia.bg/Default.aspx?alias=www.admincourtsofia.bg/en) representing  the first 5 working days of year 2021 (from 4.01 to 8.01.2021). Each of this document was transformed to text, the sentences containing legal mentions were extracted and transformed in a file following the format of the original dataset using a simple Python script. The tag were than manually reviewed and annotated as correctly as possible not forgeting the initially available tags for person, organization etc. with the BIO prefixes.\r\n",
        "\r\n",
        "In that way the train document grew up to 347 642 lines. \r\n",
        "\r\n",
        "The original test documant were split in two - one for validation and one for test datasets. 40 documents  from the same source (court decisions published from 11.01.2021) were annotated and splitted the same way it was done for the train part. With that operation the validation file consisted of 56 880 lines and the test file of 56 908 lines. \r\n",
        "\r\n",
        "In that way the ration train vs. validation was assured to be at the reasonable 84% / 14% level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWe3li-eK799"
      },
      "source": [
        "### Selecting the model and loading it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYUn2cXa7AN5"
      },
      "source": [
        "First, I need to upload the BERT model. The choosen model by is:\r\n",
        "- the BERT Multilanguage version (in order to have a pre-trained model that has already seen Bulgarian language and Bulgarian words are part of it vocabulary);\r\n",
        "- the Cased version, meaning that whether the words contains capital letter or not matter to the model;\r\n",
        "- BASE version, as using the LARGE model would take too many ressources for training without such significal improvement in the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIbmx1xC8ZtW"
      },
      "source": [
        "When using BERT one must be aware that the tokenization with BERT should be done with BERT Tokenizer. There are also other mandatory requirement for the fine-tunning of BERT - used vocabulary should be exactly the original of the pre-trained model.\r\n",
        "\r\n",
        "There are two ways to upload the model.\r\n",
        "\r\n",
        "The first one is by loading the model from TensoFlow Hub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JEjEGAu0ES",
        "outputId": "6e0eab7d-b1c5-4938-d8b8-a99ee43fe894"
      },
      "source": [
        "bert_model_name = 'bert_multi_cased_L-12_H-768_A-12' \r\n",
        "map_name_to_handle = {'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'}\r\n",
        "\r\n",
        "map_model_to_preprocess = {'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',}\r\n",
        "\r\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\r\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\r\n",
        "\r\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\r\n",
        "print(f'Preprocessing model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2tr-98HIFgn"
      },
      "source": [
        "Using TensoFlow Hub has the advantage of having its incorporated \"pre-processing\" procedure based on which it can automatically load the required preprocessing model. This model can be loaded into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose a fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3YV-TZVv12B",
        "outputId": "054b940f-703b-4215-ee59-33ffa9785ba6"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\r\n",
        "tok = bert_preprocess_model(['Hello TensorFlow!'])\r\n",
        "print(f'Keys       : {list(tok.keys())}')\r\n",
        "print(f'Shape      : {tok[\"input_word_ids\"].shape}')\r\n",
        "print(f'Word Ids   : {tok[\"input_word_ids\"][0, :12]}')\r\n",
        "print(f'Input Mask : {tok[\"input_mask\"][0, :12]}')\r\n",
        "print(f'Type Ids   : {tok[\"input_type_ids\"][0, :12]}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [  101 31178 16411 28919 11565 27863   106   102     0     0     0     0]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 0 0 0 0]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76rmrViBJQ-4"
      },
      "source": [
        "After loading it and conducting a trial on a brief sentence, it is visible that it outputs a dictionary containing all the three mandatory inputs: *'input_word_ids', 'input_mask'* and *'input_type_ids'*.\r\n",
        "It is important to note also that the input is truncated to 128 tokens. Luckily, the number of tokens can be customized. Also, to the `input_word_ids` we can see automatical append of other required tags at the beginning and at the end of the sentence. It is interesting to note as well that the `input_type_ids` only have zeros (0) because this is a single sentence input, which is the case of the NER task as well. For other tasks where there are multiple sentence input, it would have one number for each input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdgt-TIN9eDV"
      },
      "source": [
        "This \"preprocessing\" model simplifies a lot the work required by the model, but unfortunately, along with the 'attribute' part, in NER task I have to make the appropriate transformation on the 'label' part as well.\r\n",
        "\r\n",
        "This is what led me to the second possibility, which consists in downloading the required version to a directory from where it can be directly loaded. It can be done with 2 line of code:\r\n",
        "\r\n",
        "`model_name = \"multi_cased_L-12_H-768_A-12\"`\r\n",
        "\r\n",
        "`model_dir = bert.fetch_google_bert_model(model_name, gs_folder_bert)`\r\n",
        "\r\n",
        "I did that in my Google Disk drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EjrcRxhryqy",
        "outputId": "7adb22b6-c66f-470d-a51f-2b6fc2d31c2d"
      },
      "source": [
        "gs_folder_bert = '/content/drive/MyDrive/Colab Notebooks/bert_model/'\r\n",
        "\r\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.ipynb_checkpoints',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_config.json',\n",
              " 'bert_model.ckpt.index',\n",
              " 'bert_model.ckpt.meta',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IGZmswUrlNG",
        "outputId": "de9903ab-5bb3-4676-ad58-c36f4da470cd"
      },
      "source": [
        "# Set up tokenizer to generate Tensorflow dataset\r\n",
        "tokenizer = FullTokenizer(\r\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\r\n",
        "     do_lower_case=False)\r\n",
        "\r\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 119547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1m8OeaD-uCx"
      },
      "source": [
        "We can use the same example as above on a simple sentence that is not transformed to a list to verify that the upper case matters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOIr3_1n-tm_",
        "outputId": "b7b49766-1e62-41bc-ab22-f001b4a05875"
      },
      "source": [
        "tokens = tokenizer.tokenize(\"Hello TensorFlow!\")\r\n",
        "print(tokens)\r\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "print(ids)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'Ten', '##sor', '##F', '##low', '!']\n",
            "[31178, 16411, 28919, 11565, 27863, 106]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQYKlNarASK5"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeOafYlWM1LY"
      },
      "source": [
        "I will position the \"*Constants*\" section here as some of the constants will be used in the next section. As thing progress I will add below all *constant values* that I will use with some explanation below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wez32z_oIU4-"
      },
      "source": [
        "EPOCHS = 3  # Total number of training epochs to perform\r\n",
        "MAX_SEQ_LENGTH = 256  # the length of the biggest sentence\r\n",
        "BATCH_SIZE = 16  # Total batch size for training.\r\n",
        "LEARNING_RATE = 5e-5  # The initial learning rate for Adam.\r\n",
        "WARM_UP_PROPORTION = 0.1  # Proportion of training to perform linear learning rate warmup for. e.g., 0.1 = 10% of training.\r\n",
        "WEIGHT_DECAY = 0.01  # Weight decay if we apply some\r\n",
        "ADAM_EPSILON = 0.01  # Epsilon for Adam optimizer\r\n",
        "SEED = seed(42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_iment_7NcZ"
      },
      "source": [
        "As \"prescribed\" on the official [GitHub page of BERT](https://github.com/google-research/bert) in order to avoid any out-of-memory issues when using BERT model `MAX_SEQ_LENGTH` should be up to 512. Having in mind the numerous splits of the specific `tokenizer` of BERT and the maximum tokens length found during the creation of the dataset, I fix the maximum sequence length to `256`. It might not be enough as we have additional split of words to subwords due to BERT specifity, but makeing it bigger will require to much memory and will decrease teh speed of the process. In the case of maximum sequence set on '256', the benchmark of the `BATCH_SIZE` found in the same place is `16`.\r\n",
        "\r\n",
        "For the initial learning rate (`LEARNING_RATE`), in line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5). I'll use `5e-5`. During the BERT pre-training, the learning rate is a linear warm-up phase over the first `10% of training steps`, meaning the the `WARM-UP PROPOTION` should be set the same. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYRJWwDf_XSD"
      },
      "source": [
        "### Reading the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTlgRkwlLEog"
      },
      "source": [
        "After loading the model, it is time to read the documents in order to prepare the train, validation and test datasets. For that I need to create a function that will read the data sentence by sentence, that will be transformed afterwords to features with another function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41QFFFxmQYAC"
      },
      "source": [
        "The visualizing test done after each function will be conducted on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIiGhmobLjeD"
      },
      "source": [
        "def read_data(filename):\r\n",
        "  \"\"\"\"reading the file and returning for each sentence a tuple of \r\n",
        "  a list of attributes and a list of corresponding labels\r\n",
        "  \r\n",
        "  side-effects: printing the full path to the file\r\n",
        "                printing the number of sentences inside the set\r\n",
        "                printing the size of longest sentence of the set\r\n",
        "  \"\"\"\r\n",
        "  data, sentence, label = [], [], []\r\n",
        "  num_sentense, max_sentence_length = 0, 0\r\n",
        "  sen = ''\r\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\r\n",
        "    for line in f:\r\n",
        "      if len(line) == 0 or line[0] == '\\n':\r\n",
        "        if len(sentence) > 0:\r\n",
        "          data.append((sentence, label))\r\n",
        "          if len(sentence) > max_sentence_length:\r\n",
        "            max_sentence_length = len(sentence)\r\n",
        "            sen = sentence\r\n",
        "          sentence, label = [], []\r\n",
        "          num_sentense += 1\r\n",
        "        continue\r\n",
        "      word, lab = line.rstrip('\\n').split('\\t')\r\n",
        "      sentence.append(word)\r\n",
        "      label.append(lab)\r\n",
        "\r\n",
        "    if len(sentence) > 0:\r\n",
        "      data.append((sentence, label))\r\n",
        "      num_sentense += 1\r\n",
        "      if len(sentence) > max_sentence_length:\r\n",
        "            max_sentence_length = len(sentence)\r\n",
        "\r\n",
        "    print(\"Full path to the filename:\", filename)\r\n",
        "    print(\"Number of sentences:\", num_sentense)\r\n",
        "    print(\"Maximum token lenght of a sentence:\", max_sentence_length)\r\n",
        "  return data  # [tuple(attributes, labels)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QzJIUSeQL4o",
        "outputId": "4acb559b-97d4-4301-b9dc-a17d8879f72d"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/data/'\r\n",
        "\r\n",
        "val_file = os.path.join(path, 'val_NER_BG.txt')\r\n",
        "\r\n",
        "val_readed_data = read_data(val_file)\r\n",
        "\r\n",
        "# printing of the first 5 values\r\n",
        "print()\r\n",
        "print(val_readed_data[:5])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/val_NER_BG.txt\n",
            "Number of sentences: 1695\n",
            "Maximum token lenght of a sentence: 240\n",
            "\n",
            "[(['Газопроводът', 'Северен', 'поток', '2', ',', 'който', 'по', 'план', 'ще', 'пренася', 'ежегодно', '55', 'милиарда', 'кубични', 'метра', 'природен', 'газ', 'от', 'Русия', 'към', 'ЕС', 'през', 'Балтийско', 'море', ',', 'вече', 'бе', 'одобрен', 'от', 'Германия', 'и', 'Финландия', '.'], ['O', 'B-PRO', 'I-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O']), (['САЩ', ',', 'в', 'отговор', 'заявиха', ',', 'че', 'тръбопроводът', 'ще', 'повиши', 'зависимостта', 'на', 'Европа', 'от', 'руския', 'газ', '.'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']), (['Списание', '\"', 'Foreign', 'policy', '\"', 'цитира', 'три', 'източника', 'близки', 'до', 'въпроса', ',', 'които', 'твърдят', 'че', 'администрацията', 'на', 'САЩ', 'е', 'близо', 'до', 'налагането', 'на', 'санкции', 'върху', 'енергийни', 'компании', 'от', 'Германия', 'и', 'други', 'държави', 'от', 'ЕС', ',', 'които', 'са', 'замесени', 'в', 'изграждането', 'на', 'проекта', 'за', 'руски', 'газов', 'тръбопровод', 'Северен', 'поток', '2', '.'], ['O', 'O', 'B-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'I-PRO', 'O']), (['Ключови', 'фигури', 'в', 'администрацията', 'на', 'президента', 'Доналд', 'Тръмп', ',', 'които', 'виждат', 'санкциите', 'като', '\"', 'много', 'вероятна', 'опция', '\"', ',', 'няма', 'да', 'се', 'спрат', 'пред', 'нищо', ',', 'за_да', 'блокират', 'Северен', 'поток', ',', 'заявява', 'един', 'от', 'източниците', 'на', 'списанието', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Източник', 'от', 'Държавния', 'департамент', 'е', 'заявил', ',', 'че', '\"', 'бяхме', 'ясни', ',', 'че', 'компании', 'работещи', 'в', 'руския', 'енергиен', 'сектор', 'и', 'сектора', 'за', 'износ', 'на', 'тръбопроводи', 'навлизат', 'в', 'бизнес', 'посока', ',', 'която', 'носи', 'опасност', 'от', 'санкции', '\"', '.'], ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQYwbtNIpAZp"
      },
      "source": [
        "All these sentences need to be transformed in features that BERT understands using the BERT tokenization.\r\n",
        "\r\n",
        "By tokenizing a sentence we in fact encode the sentence. There is one special requirement for that tokenization - 2 additional labels should be added to the list of words representing each sentence: `['CLS']` and `['SEP']`. These 2 additional labels are required by BERT. `['CLS']` indicates that we will talk about \"classification problem\", so `['CLS']` token will be put at the beginning of each phrase, and each sentence and its corresponding label list should end with a `['SEP']` - \"separator\" token. Their ids a respectively `[101]` and `[102]`\r\n",
        ".\r\n",
        "\r\n",
        "The feature required by BERT, apart from the ids of the tokenized sentence and its respective labels using the same index mapping everywhere defined in advance), is the \"input masks\" which allows the model to cleanly differentiate between content and padding. Similar masking is needed for the labels.\r\n",
        "\r\n",
        "The most specific part that I will be using here is the creation and consequently usage of the variable `valid_ids`. This variable will be a list which will be responsible for the replication of the logic of subwords to the `label_id` list, by marking only the first subword's label as valid, so that the label becomes \"responsible\" for the whole word.\r\n",
        "\r\n",
        "There are different ways to provide all features to the model. Most examples on this topic use dictionnaries, but the this a simple class will be more easy to deal with when using is further, in order to stuck them together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ5Yflqymacj"
      },
      "source": [
        "class InputFeatures:\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, ntokens, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\r\n",
        "      self.ntokens = ntokens  # only for representatinal purpose\r\n",
        "      self.input_ids = input_ids  # encoded words\r\n",
        "      self.input_mask = input_mask  # mask indicating padding or not\r\n",
        "      self.segment_ids = segment_ids  # we talk about NER task, so the segment_ids will be just '0's\r\n",
        "      self.label_id = label_id  # encoded label\r\n",
        "      self.valid_ids = valid_ids  # when the word is split to subwords, it indicates only the first id as valid '1', next subwords as '0'\r\n",
        "      self.label_mask = label_mask  # mask indicating padding or not\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvpzX9nmRlD"
      },
      "source": [
        "def convert_examples_to_features(examples, label_map, max_seq_length, tokenizer):\r\n",
        "    \"\"\"Loads a data file into a list of `InputFeatures`s.\"\"\"\r\n",
        "\r\n",
        "    features = []  # the list of `InputFeatures` to be returned\r\n",
        "    for (ex_index, example) in enumerate(examples):\r\n",
        "        textlist = example[0]\r\n",
        "        labellist = example[1]\r\n",
        "        tokens = []\r\n",
        "        labels = []\r\n",
        "        valid_ids = []\r\n",
        "        label_mask = []\r\n",
        "        for i, word in enumerate(textlist):\r\n",
        "            token = tokenizer.tokenize(word)\r\n",
        "            tokens.extend(token)\r\n",
        "            label_1 = labellist[i]\r\n",
        "            for m in range(len(token)):\r\n",
        "                if m == 0:\r\n",
        "                    labels.append(label_1)\r\n",
        "                    valid_ids.append(1)\r\n",
        "                    label_mask.append(True)\r\n",
        "                else:\r\n",
        "                    valid_ids.append(0)\r\n",
        "        \r\n",
        "        # checking if a sentence is longer than max_seq_length, if yes -> cut it\r\n",
        "        if len(tokens) >= max_seq_length - 1:\r\n",
        "            tokens = tokens[0:(max_seq_length - 2)]\r\n",
        "            labels = labels[0:(max_seq_length - 2)]\r\n",
        "            valid_ids = valid_ids[0:(max_seq_length - 2)]\r\n",
        "            label_mask = label_mask[0:(max_seq_length - 2)]\r\n",
        "\r\n",
        "        # init\r\n",
        "        ntokens = []\r\n",
        "        segment_ids = []\r\n",
        "        label_ids = []\r\n",
        "\r\n",
        "        # adding the mandatory ['CLS'] at the beginning\r\n",
        "        ntokens.append(\"[CLS]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        valid_ids.insert(0, 1)\r\n",
        "        label_mask.insert(0, True)\r\n",
        "        label_ids.append(label_map[\"[CLS]\"])\r\n",
        "        for i, token in enumerate(tokens):\r\n",
        "            ntokens.append(token)\r\n",
        "            segment_ids.append(0)\r\n",
        "            if len(labels) > i:\r\n",
        "                label_ids.append(label_map[labels[i]])\r\n",
        "\r\n",
        "        # adding the mandatory ['SEP'] at the end of each sentence\r\n",
        "        ntokens.append(\"[SEP]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        valid_ids.append(1)\r\n",
        "        label_mask.append(True)\r\n",
        "        label_ids.append(label_map[\"[SEP]\"])\r\n",
        "\r\n",
        "        # transforming `ntokens` to BERT's tokenizer ids\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "        label_mask = [True] * len(label_ids)\r\n",
        "\r\n",
        "        # padding\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "            label_ids.append(0)\r\n",
        "            valid_ids.append(1)\r\n",
        "            label_mask.append(False)\r\n",
        "        while len(label_ids) < max_seq_length:\r\n",
        "            label_ids.append(0)\r\n",
        "            label_mask.append(False)\r\n",
        "\r\n",
        "        # last check - all lengths should correspond to max_seq_length\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "        assert len(label_ids) == max_seq_length\r\n",
        "        assert len(valid_ids) == max_seq_length\r\n",
        "        assert len(label_mask) == max_seq_length\r\n",
        "\r\n",
        "        # adding to the list\r\n",
        "        features.append(\r\n",
        "            InputFeatures(ntokens=ntokens,\r\n",
        "                          input_ids=input_ids,\r\n",
        "                          input_mask=input_mask,\r\n",
        "                          segment_ids=segment_ids,\r\n",
        "                          label_id=label_ids,\r\n",
        "                          valid_ids=valid_ids,\r\n",
        "                          label_mask=label_mask))\r\n",
        "    return features"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVqo7l9lrNAA"
      },
      "source": [
        "In order to use this function, I will need to create the list of the available labels including the required by BERT `['CLS']` and `['SEP']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1vPizjArq0k"
      },
      "source": [
        "label_list = [\"O\", \"B-LAW\", \"I-LAW\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-PRO\", \"I-PRO\", \"B-EVT\", \"I-EVT\", \"[CLS]\", \"[SEP]\"]\r\n",
        "\r\n",
        "label_map = {label: i for i, label in enumerate(label_list, 1)}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeX1MoH5sT2_"
      },
      "source": [
        "val_features = convert_examples_to_features(examples=val_readed_data, label_map=label_map, max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MkxTDgvGMqw"
      },
      "source": [
        "Here is an example of the outcome of this function on the first sentence from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVullaray4sI",
        "outputId": "c3671c73-4406-4eb0-f3db-c7b71d2677f6"
      },
      "source": [
        "print(val_features[0])\r\n",
        "print(\"ntokens =\", val_features[0].ntokens)\r\n",
        "print(\"input_ids =\", val_features[0].input_ids)\r\n",
        "print(\"input_mask =\", val_features[0].input_mask)\r\n",
        "print(\"segment_ids =\", val_features[0].segment_ids)\r\n",
        "print(\"label_id =\", val_features[0].label_id)\r\n",
        "print(\"valid_ids =\", val_features[0].valid_ids)\r\n",
        "print(\"label_mask =\", val_features[0].label_mask)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.InputFeatures object at 0x7f3086155f60>\n",
            "ntokens = ['[CLS]', 'Г', '##аз', '##оп', '##рово', '##д', '##ът', 'Се', '##вер', '##ен', 'по', '##ток', '2', ',', 'който', 'по', 'план', 'ще', 'пре', '##нася', 'ежегодно', '55', 'ми', '##ли', '##арда', 'к', '##уб', '##ични', 'метра', 'природе', '##н', 'газ', 'от', 'Русия', 'към', 'ЕС', 'през', 'Ба', '##лт', '##ий', '##ско', 'море', ',', 'вече', 'б', '##е', 'од', '##об', '##рен', 'от', 'Германия', 'и', 'Ф', '##ин', '##ландия', '.', '[SEP]']\n",
            "input_ids = [101, 512, 26313, 58056, 55048, 10746, 13368, 52203, 32418, 10928, 10297, 20422, 123, 117, 16362, 10297, 35718, 16892, 38494, 87280, 84167, 11358, 37140, 10783, 72123, 551, 40124, 53928, 41921, 93710, 10267, 44352, 10332, 13014, 15977, 109795, 12112, 101086, 33262, 11550, 13566, 27165, 117, 45721, 542, 10205, 10430, 33276, 27332, 10332, 20823, 549, 529, 12029, 68103, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "input_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segment_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "label_id = [14, 1, 10, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 6, 1, 8, 9, 1, 1, 1, 1, 1, 8, 1, 8, 1, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "valid_ids = [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_mask = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOtyK-8Oo6d"
      },
      "source": [
        "As for each sentence and its respective labels there are the required ids and masks, in order to be proceeded by BERT, it should be transformed to acceptable tensors. This will allow also the backed by accelerator memory (like GPU, TPU).\r\n",
        "\r\n",
        "For that I will use the function `tf.data.Dataset.from_tensor_slices()`. This will also allow to apply transformations using `shuffle` and `batch` functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XvCYjc3RJ4x"
      },
      "source": [
        "def transform_to_dataset(features, examples, seed, batch_size, training_dataset=False):\r\n",
        "\r\n",
        "  all_input_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.input_ids for f in features]))\r\n",
        "  all_input_mask = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.input_mask for f in features]))\r\n",
        "  all_segment_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.segment_ids for f in features]))\r\n",
        "  all_valid_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.valid_ids for f in features]))\r\n",
        "  all_label_mask = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.label_mask for f in features]))\r\n",
        "  all_label_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.label_id for f in features]))\r\n",
        "\r\n",
        "  # Dataset using tf.data\r\n",
        "  data = tf.data.Dataset.zip(\r\n",
        "      (all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids, all_label_mask))\r\n",
        "  \r\n",
        "  number_features = len(features)\r\n",
        "\r\n",
        "  if training_dataset:\r\n",
        "      shuffled_data = data.shuffle(buffer_size=int(len(features) * 0.1),\r\n",
        "                                  seed=seed, reshuffle_each_iteration=True)\r\n",
        "      batched_data = shuffled_data.batch(batch_size)\r\n",
        "  else:\r\n",
        "      batched_data = data.batch(batch_size)\r\n",
        "\r\n",
        "  return data, batched_data, number_features"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuziyKhTiRa"
      },
      "source": [
        "validation, batched_val_data, _ = transform_to_dataset(features=val_features, examples=val_readed_data, seed=SEED, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMdQ3JB3XPWa",
        "outputId": "30194e62-3dcc-4338-a693-64a80b7dc9b0"
      },
      "source": [
        "print(validation)\r\n",
        "print(batched_val_data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ZipDataset shapes: ((256,), (256,), (256,), (256,), (256,), (256,)), types: (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.bool)>\n",
            "<BatchDataset shapes: ((None, 256), (None, 256), (None, 256), (None, 256), (None, 256), (None, 256)), types: (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.bool)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJdvTsQHSeM"
      },
      "source": [
        "I used the `validation` dataset preparation as example, but I'll need to do the same for the `train` and `test` datasets. For additional simplification I will create a single function that includes all these required transformative functions inside:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYVEfkCqI6rL"
      },
      "source": [
        "def data_preprocess(dataset_type, path, label_map, max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer, batch_size=BATCH_SIZE, seed=SEED):\r\n",
        "  \"\"\"\r\n",
        "  function englobelling the customized bert pre-process of data for a NER task\r\n",
        "  params: dataset_type: string with possible dataset_types: 'train', 'val', 'test'\r\n",
        "          path: the directory where the datasets are situated\r\n",
        "          label_map: desired mapping of all the available labels + including '[SEP]' and '[CLS]'\r\n",
        "  \r\n",
        "  returns: tuple of zipped dataset and batched dataset\r\n",
        "  \"\"\"\r\n",
        "  filename = f\"{dataset_type}_NER_BG.txt\" \r\n",
        "  \r\n",
        "  file = os.path.join(path, filename)\r\n",
        "\r\n",
        "  readed_data = read_data(file)\r\n",
        "\r\n",
        "  features = convert_examples_to_features(examples=readed_data, label_map=label_map, max_seq_length=max_seq_length, tokenizer=tokenizer)\r\n",
        "\r\n",
        "  training_dataset = True if dataset_type == 'train' else False\r\n",
        "\r\n",
        "  return transform_to_dataset(features=features, examples=readed_data, seed=seed, batch_size=batch_size, training_dataset=training_dataset)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KTHtA2uHf9w",
        "outputId": "12927437-87c7-4370-c36e-8714d7c4f5d6"
      },
      "source": [
        "# training dataset\r\n",
        "train_data, batched_train_data, train_size = data_preprocess(dataset_type='train', path=path, label_map=label_map)\r\n",
        "\r\n",
        "# validation dataset\r\n",
        "val_data, batched_val_data, _ = data_preprocess(dataset_type='val', path=path, label_map=label_map)\r\n",
        "\r\n",
        "# test dataset\r\n",
        "test_data, batched_test_data, _ = data_preprocess(dataset_type='test', path=path, label_map=label_map)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/train_NER_BG.txt\n",
            "Number of sentences: 8785\n",
            "Maximum token lenght of a sentence: 245\n",
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/val_NER_BG.txt\n",
            "Number of sentences: 1695\n",
            "Maximum token lenght of a sentence: 240\n",
            "Full path to the filename: /content/drive/MyDrive/Colab Notebooks/data/test_NER_BG.txt\n",
            "Number of sentences: 1461\n",
            "Maximum token lenght of a sentence: 285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE2D8ffU7ugV",
        "outputId": "af68b003-03a5-4dc8-aa04-94d021daa63d"
      },
      "source": [
        "batched_train_data.element_spec"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(None, 256), dtype=tf.bool, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJkRQQu9_npF"
      },
      "source": [
        "### Building the BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x87d-g7M87nr"
      },
      "source": [
        "Unfortunately, it is impossible to use the newly created module `bert-for-tf2` made to implement many of the GLUE tasks. Using it for GLUE task is really very user-firnedly. But as it doesn't accept all the usual \"inputs\", skipping the `input_mask`, it is not suitable at this moment for NER task.\r\n",
        "\r\n",
        "Using the BERT model as a layer with this module is really easy task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aveAC5n98inF"
      },
      "source": [
        "# !pip import bert-for-tf2\r\n",
        "# import bert\r\n",
        "\r\n",
        "# bert_params = bert.params_from_pretrained_ckpt(gs_folder_bert)\r\n",
        "# print(\"bert_params:\")\r\n",
        "# pprint(bert_params)\r\n",
        "# print(bert_params == bert_params_1)\r\n",
        "# print(l_bert)\r\n",
        "# print(bert_m)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3bVmsK1KS33"
      },
      "source": [
        "# creation of BERT as a layer\r\n",
        "# l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\r\n",
        "# model_ckpt = os.path.join(gs_folder_bert, \"bert_model.ckpt\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUGvVLJC_tPq"
      },
      "source": [
        "Unfortunately, I unserstood the problem the hard way and after losing quite lot of time in debugging I understood that I had to find different way for using BERT as a layer in my model. The original [BERT model](https://github.com/google-research/bert) created by Jacob Devlin unfortunately uses lots of old and already modified modules, so it is also unconvinient for my case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iGSOfS5_yZD"
      },
      "source": [
        "After the model is already selected, the configuration of the pretrained model should be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtbZSUnIAECB",
        "outputId": "46156b9c-0ac0-4be9-85f3-c5c88ad0f5e4"
      },
      "source": [
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\r\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\r\n",
        "\r\n",
        "# bert_config = bert.configs.BertConfig.from_dict(config_dict)\r\n",
        "\r\n",
        "print(\"config_dict:\")\r\n",
        "pprint(config_dict)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config_dict:\n",
            "{'attention_probs_dropout_prob': 0.1,\n",
            " 'directionality': 'bidi',\n",
            " 'hidden_act': 'gelu',\n",
            " 'hidden_dropout_prob': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_size': 3072,\n",
            " 'max_position_embeddings': 512,\n",
            " 'num_attention_heads': 12,\n",
            " 'num_hidden_layers': 12,\n",
            " 'pooler_fc_size': 768,\n",
            " 'pooler_num_attention_heads': 12,\n",
            " 'pooler_num_fc_layers': 3,\n",
            " 'pooler_size_per_head': 128,\n",
            " 'pooler_type': 'first_token_transform',\n",
            " 'type_vocab_size': 2,\n",
            " 'vocab_size': 119547}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuwguV-9xsvQ"
      },
      "source": [
        "Before putting BERT into my model, I'll demonstrate its outputs and how the input can be simulate using my customized preprocess. It will be done based on the TensorFlow Hub model chosen in teh section *\"Selecting the model and loading it\"*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2X-0x9vVYEc"
      },
      "source": [
        "bert_enc_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-FUYZ5GzA-h"
      },
      "source": [
        "The test will be done again on the `validation` dataset, resulted from the visualization used before:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Pc3Waxmmzn"
      },
      "source": [
        "I will prepare a function `model_input_preprocess` that I'll use to transform the prepared `input_word_ids`, `input_mask`, `input_type_ids` to the expected type by the model - in a form of dict (that was explored visually in the *\"Selecting the model and loading it\"* section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_4uT_t9nv5O"
      },
      "source": [
        "def model_input_preprocess(input_word_i, input_m, input_type_i):\r\n",
        "  \"\"\"function transforming the required three inputs by the bert model\r\n",
        "  to a single dictionary with the respective key names\"\"\"\r\n",
        "  input_word_ids=tf.dtypes.cast(input_word_i, 'int32') \r\n",
        "  input_mask=tf.dtypes.cast(input_m, 'int32')  \r\n",
        "  input_type_ids=tf.dtypes.cast(input_type_i, 'int32') \r\n",
        "\r\n",
        "  return dict(\r\n",
        "      input_word_ids=input_word_ids, \r\n",
        "      input_mask=input_mask, \r\n",
        "      input_type_ids=input_type_ids,\r\n",
        "  )"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9MTQ74obTlh",
        "outputId": "7f59b43c-f582-44d8-b252-5bce207b15b9"
      },
      "source": [
        "# choosing the first sentence from batched validation set\r\n",
        "for (input_id, input_m, segment_id, valid_id, label_id, label_mask) in batched_val_data:\r\n",
        "  inputs = model_input_preprocess(input_id, input_m, segment_id)\r\n",
        "  print('Keys           : ', list(inputs.keys()))\r\n",
        "  print('Shape Word Ids : ', inputs['input_word_ids'].shape)\r\n",
        "  print('Word Ids       : ', inputs['input_word_ids'][0, :16])\r\n",
        "  print('Shape Mask     : ', inputs['input_mask'].shape)\r\n",
        "  print('Input Mask     : ', inputs['input_mask'][0, :16])\r\n",
        "  print('Shape Type Ids : ', inputs['input_type_ids'].shape)\r\n",
        "  print('Type Ids       : ', inputs['input_type_ids'][0, :16])\r\n",
        "  print('#'*90)\r\n",
        "  bert_results = bert_enc_model(inputs)\r\n",
        "  print(f'Loaded BERT: {tfhub_handle_encoder}')\r\n",
        "  print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\r\n",
        "  print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\r\n",
        "  print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\r\n",
        "  print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')\r\n",
        "  print(f'Encoder Outputs Shape:{bert_results[\"encoder_outputs\"][0].shape}')\r\n",
        "  print(f'Encoder Outputs Values:{bert_results[\"encoder_outputs\"][0][0, :12]}')\r\n",
        "  print('#'*90)\r\n",
        "  print(valid_id)\r\n",
        "  break"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys           :  ['input_word_ids', 'input_mask', 'input_type_ids']\n",
            "Shape Word Ids :  (16, 256)\n",
            "Word Ids       :  tf.Tensor(\n",
            "[  101   512 26313 58056 55048 10746 13368 52203 32418 10928 10297 20422\n",
            "   123   117 16362 10297], shape=(16,), dtype=int32)\n",
            "Shape Mask     :  (16, 256)\n",
            "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(16,), dtype=int32)\n",
            "Shape Type Ids :  (16, 256)\n",
            "Type Ids       :  tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
            "##########################################################################################\n",
            "Loaded BERT: https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Pooled Outputs Shape:(16, 768)\n",
            "Pooled Outputs Values:[ 0.12184808 -0.0163886   0.203754   -0.18777439 -0.10683946  0.06027573\n",
            "  0.17212023  0.0979872  -0.22828776 -0.04825079 -0.01181884 -0.06624751]\n",
            "Sequence Outputs Shape:(16, 256, 768)\n",
            "Sequence Outputs Values:[[-0.12188576 -0.16414413 -0.37376097 ...  0.13160148  0.13240221\n",
            "  -0.2742458 ]\n",
            " [-0.01606957 -0.28604206  0.18511228 ...  0.6063422  -0.11062267\n",
            "  -0.7212991 ]\n",
            " [ 0.00533088 -0.38949755  0.5003082  ...  0.6275553  -0.49391478\n",
            "  -0.31647617]\n",
            " ...\n",
            " [-0.0694323   0.07602486  0.31742582 ...  0.38349953  0.39283466\n",
            "   0.04019697]\n",
            " [-0.12340717 -0.20836657 -0.56615543 ...  0.4071741  -0.23659927\n",
            "  -0.08806048]\n",
            " [-0.01703591 -0.7220667   0.34950417 ...  0.85474     0.33502135\n",
            "  -0.74006414]]\n",
            "Encoder Outputs Shape:(16, 256, 768)\n",
            "Encoder Outputs Values:[[-0.08864605 -0.03023656 -0.05646037 ...  0.06766584 -0.19885315\n",
            "   0.10505591]\n",
            " [-1.5488887   0.38121593  1.227031   ... -0.02830777 -0.16188714\n",
            "  -0.8764744 ]\n",
            " [ 0.03052079  0.64804906  0.779373   ...  0.38004103 -0.8068428\n",
            "  -0.2511574 ]\n",
            " ...\n",
            " [ 0.45039874  0.97274655 -0.234805   ...  0.40180868 -0.04346803\n",
            "  -0.27750117]\n",
            " [-0.13116044  0.67733335 -0.41029203 ...  1.1952648  -1.0869412\n",
            "  -0.826793  ]\n",
            " [ 1.4840587  -0.30291584 -1.6300993  ...  2.5867653   0.9226339\n",
            "  -0.423246  ]]\n",
            "##########################################################################################\n",
            "tf.Tensor(\n",
            "[[1 1 0 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]\n",
            " [1 1 0 ... 1 1 1]\n",
            " ...\n",
            " [1 1 0 ... 1 1 1]\n",
            " [1 1 0 ... 1 1 1]\n",
            " [1 1 1 ... 1 1 1]], shape=(16, 256), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Po4mcBB10BR"
      },
      "source": [
        "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\r\n",
        "\r\n",
        "- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`.\r\n",
        "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. It represents something as a contextual embedding for every token in a sentence.\r\n",
        "- `encoder_outputs` are the intermediate activations of the **L** Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the `i`-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMggil-bCyi8"
      },
      "source": [
        "class BertNer(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self, num_labels, max_seq_length, bert_model=tfhub_handle_encoder, final_layer_initializer=None):\r\n",
        "        '''\r\n",
        "        bert_model : tfhub_handle_encoder\r\n",
        "        num_labels : num of tags in NER task\r\n",
        "        max_seq_length : max_seq_length of tokens\r\n",
        "        final_layer_initializer : default:  tf.keras.initializers.TruncatedNormal\r\n",
        "        '''\r\n",
        "        super(BertNer, self).__init__()\r\n",
        "        \r\n",
        "        # defining the Input of the BERT layer\r\n",
        "        inp = dict(\r\n",
        "            input_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids'),\r\n",
        "            input_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask'),\r\n",
        "            input_type_ids = tf.keras.Input(shape=(max_seq_length, ), dtype=tf.int32, name='input_type_ids'),\r\n",
        "        )\r\n",
        "\r\n",
        "        # defining the BERT model encoder\r\n",
        "        encoder = hub.KerasLayer(tfhub_handle_encoder, name='BERT_encoder')\r\n",
        "\r\n",
        "        # BERT model as a layer\r\n",
        "        self.bert = encoder\r\n",
        "\r\n",
        "        if final_layer_initializer is not None:\r\n",
        "            initializer = final_layer_initializer\r\n",
        "        else:\r\n",
        "            initializer = tf.keras.initializers.TruncatedNormal(stddev=0.02)\r\n",
        "\r\n",
        "        # adding default DROPOUT Layer\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.1)\r\n",
        "        \r\n",
        "        # defining the output Layer\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            num_labels, kernel_initializer=initializer, activation='softmax',name='output', dtype=tf.float32)\r\n",
        "    \r\n",
        "\r\n",
        "    def call(self, input_word_ids, input_mask=None, input_type_ids=None, valid_mask=None, **kwargs):\r\n",
        "        TRAIN = kwargs.get('training', False)\r\n",
        "        if TRAIN:\r\n",
        "            self.bert.trainable = TRAIN\r\n",
        "        inputs = model_input_preprocess(input_word_ids, input_mask, input_type_ids)\r\n",
        "        output = self.bert(inputs)\r\n",
        "        sequence_output = output['sequence_output']\r\n",
        "        valid_output = []\r\n",
        "        for i in range(sequence_output.shape[0]):  # shape[0] is batch_size\r\n",
        "            r = 0\r\n",
        "            temp = []\r\n",
        "            for j in range(sequence_output.shape[1]):  # shape[1] is max_seq_len\r\n",
        "                if valid_mask[i][j] == 1:\r\n",
        "                    temp = temp + [sequence_output[i][j]]\r\n",
        "                else:\r\n",
        "                    r += 1\r\n",
        "            temp = temp + r * [tf.zeros_like(sequence_output[i][j])]\r\n",
        "            valid_output = valid_output + temp\r\n",
        "        valid_output = tf.reshape(tf.stack(valid_output), sequence_output.shape)\r\n",
        "        sequence_output = self.dropout(valid_output, training=TRAIN)\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "        return logits"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLIEi5PhVpSK"
      },
      "source": [
        "OUT_UNITS = len(label_list)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZhzjzz8F_9T"
      },
      "source": [
        "ner = BertNer(OUT_UNITS, MAX_SEQ_LENGTH)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EkmS14YjyD",
        "outputId": "f8c76e31-746b-4318-cc58-264c44f3dfbd"
      },
      "source": [
        "ner.layers"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow_hub.keras_layer.KerasLayer at 0x7f307cb5e1d0>,\n",
              " <tensorflow.python.keras.layers.core.Dropout at 0x7f2f5e8acfd0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7f2f5e8acc88>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc1HqRph7HFV",
        "outputId": "4a339dfa-136f-45cc-8b96-345083a7fa39"
      },
      "source": [
        "len(ner.variables)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPLT6PhatkWr"
      },
      "source": [
        "raise NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOjOvISpoZ3u"
      },
      "source": [
        "############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0KHLdauULQY"
      },
      "source": [
        "# model = tf.keras.models.Sequential([\r\n",
        "#   tf.keras.layers.InputLayer(input_shape=(None, 3)),  # [input_word_ids, input_mask, input_type_ids] \r\n",
        "#   l_bert,\r\n",
        "#   tf.keras.layers.Dense(units=OUT_UNITS, activation=\"softmax\")\r\n",
        "# ])\r\n",
        "# model.build(input_shape=(None, MAX_SEQ_LENGTH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-7ayqmF91Qk",
        "outputId": "8edfd4cc-21f1-49b0-bc8a-df9ef895ed32"
      },
      "source": [
        "inp = inputs\r\n",
        "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\r\n",
        "outputs = encoder(inputs)\r\n",
        "sequence_output = outputs['sequence_output']\r\n",
        "print(sequence_output.shape)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 256, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmDDPzjs8KkV"
      },
      "source": [
        "def build_classifier_model(num_classes, max_seq_length):\r\n",
        "  inp = dict(\r\n",
        "            input_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids'),\r\n",
        "            input_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask'),\r\n",
        "            input_type_ids = tf.keras.Input(shape=(max_seq_length, ), dtype=tf.int32, name='input_type_ids'),\r\n",
        "        )\r\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\r\n",
        "  outputs = encoder(inp)\r\n",
        "  sequence_output = outputs['sequence_output']\r\n",
        "  valid_output = []\r\n",
        "  for i in range(sequence_output.shape[0]):  # shape[0] is batch_size\r\n",
        "    r = 0\r\n",
        "    temp = []\r\n",
        "    for j in range(sequence_output.shape[1]):  # shape[1] is max_seq_len\r\n",
        "      if valid_mask[i][j] == 1:\r\n",
        "        temp = temp + [sequence_output[i][j]]\r\n",
        "      else:\r\n",
        "        r += 1\r\n",
        "    temp = temp + r * [tf.zeros_like(sequence_output[i][j])]\r\n",
        "    valid_output = valid_output + temp\r\n",
        "  valid_output = tf.reshape(tf.stack(valid_output), sequence_output.shape)\r\n",
        "  sequence_output = tf.keras.layers.Dropout(0.1)(valid_output)\r\n",
        "  sequence_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(sequence_output)\r\n",
        "  return tf.keras.Model(inp, sequence_output)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OYSQkwcT9L4Z",
        "outputId": "bf3ad7a6-0afd-434a-df7f-b7abbf46f2de"
      },
      "source": [
        "classifier_model = build_classifier_model(OUT_UNITS, MAX_SEQ_LENGTH)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-12ff042619b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_classifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_UNITS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-de8f093fd2a2>\u001b[0m in \u001b[0;36mbuild_classifier_model\u001b[0;34m(num_classes, max_seq_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mvalid_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# shape[0] is batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQETWafw815B"
      },
      "source": [
        "tf.keras.utils.plot_model(classifier_model, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipk897WZwBY_"
      },
      "source": [
        "def flatten_layers(root_layer):\r\n",
        "    if isinstance(root_layer, tf.keras.layers.Layer):\r\n",
        "        yield root_layer\r\n",
        "    for layer in root_layer._layers:\r\n",
        "        for sub_layer in flatten_layers(layer):\r\n",
        "            yield sub_layer\r\n",
        "\r\n",
        "def freeze_bert_layers(l_bert):\r\n",
        "    \"\"\"\r\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\r\n",
        "    \"\"\"\r\n",
        "    for layer in flatten_layers(l_bert):\r\n",
        "      #print(layer)\r\n",
        "      print(layer.name)\r\n",
        "      #print(layer._layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3uEBei7w0eR"
      },
      "source": [
        "freeze_bert_layers(ner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6IxgfR7INZo"
      },
      "source": [
        "#####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3xorHTuUx5l"
      },
      "source": [
        "### Set up the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdXW6hI4lNP"
      },
      "source": [
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). As described in the prodecure of [Fine-Tuning Bert in Tensorflow Tutorials](https://www.tensorflow.org/official_models/fine_tuning_bert#set_up_the_optimizer): \"BERT adopts the Adam optimizer with weight decay (aka \"AdamW\"). It also employs a learning rate schedule that firstly warms up from 0 and then decays to 0.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhTFM4hEVXd9"
      },
      "source": [
        "# set_up steps\r\n",
        "\r\n",
        "steps_per_epoch = int(train_size / BATCH_SIZE)\r\n",
        "\r\n",
        "num_train_steps = steps_per_epoch * EPOCHS\r\n",
        "\r\n",
        "warmup_steps = int(WARM_UP_PROPORTION * num_train_steps)\r\n",
        "\r\n",
        "print(warmup_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1Cryrra5au"
      },
      "source": [
        "It is commonly observed that a monotonically decreasing learning rate, whose degree of change is carefully chosen, results in a better performing model (source [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay)). The PolynomialDecay schedule applies a polynomial decay function to an optimizer step, given a provided initial_learning_rate, to reach an end_learning_rate in the given decay_steps. \r\n",
        "\r\n",
        "The base learning rate schedule used here is a linear decay to zero over the training run, visible also on the graph below :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2vbNw8PaFlM"
      },
      "source": [
        "decay_schedule  = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=LEARNING_RATE,\r\n",
        "                                                                 decay_steps=num_train_steps,\r\n",
        "                                                                 end_learning_rate=0.0)\r\n",
        "\r\n",
        "plt.plot([decay_schedule(n) for n in range(num_train_steps)])\r\n",
        "plt.xlabel('number of training steps')\r\n",
        "plt.ylabel('learning rate decay schedule')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDAQNTfdpbj3"
      },
      "source": [
        "Using `nlp.optimization` `WarmUp` class, the definition of the warm-up schedule is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKLedlUkIN0X"
      },
      "source": [
        "warmup_schedule = WarmUp(initial_learning_rate=decay_schedule(warmup_steps),\r\n",
        "                         decay_schedule_fn=decay_schedule,\r\n",
        "                         warmup_steps=warmup_steps)\r\n",
        "  \r\n",
        "plt.plot([warmup_schedule(n) for n in range(num_train_steps)])\r\n",
        "plt.xlabel('number of training steps')\r\n",
        "plt.ylabel('learning rate with warm-up and decay schedule')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxrmrY_pw8a"
      },
      "source": [
        "It warms up to the `initial_learning_rate` following the learning rate level at the moment the decay schedule calculates it depending on the number of training steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaUPJZjLnClB"
      },
      "source": [
        "optimizer = AdamWeightDecay(\r\n",
        "            learning_rate=warmup_schedule,\r\n",
        "            weight_decay_rate=WEIGHT_DECAY,\r\n",
        "            epsilon=ADAM_EPSILON,\r\n",
        "            exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\r\n",
        "# default values: beta_1=0.9, beta_2=0.999,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J3vnfOBnGt3"
      },
      "source": [
        "type(optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dbwhrdzl7bR"
      },
      "source": [
        "### Select loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwePPs-RmBwD"
      },
      "source": [
        "Select metrics to measure the loss and the accuracy of the model it the next step. These metrics accumulate the values over epochs and then print the overall result. Generally, no matter the dataset, the performance of the NER system is commonly measured by the F1-score.\r\n",
        "\r\n",
        "F1 score is the harmonic mean of precision and recall. Output range is `[0, 1]`.\r\n",
        "$$F_1 = 2 . \\frac{precision . recall}{recision + recall}$$\r\n",
        "\r\n",
        "In the `tensorflow_addons.metrics` module `F1Score` exists as ready-to-use option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Ww8ekymFzT"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\r\n",
        "\r\n",
        "f1_metric = F1Score(OUT_UNITS, average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUAqR5XpJo_u"
      },
      "source": [
        "def dice_loss(y_true, y_predicted):\r\n",
        "\r\n",
        "    y_true_f = tf.layers.flatten(y_true)\r\n",
        "    y_pred_f = tf.layers.flatten(y_predicted)\r\n",
        "\r\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\r\n",
        "\r\n",
        "    if loss_type == 'jaccard':\r\n",
        "        union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))\r\n",
        "\r\n",
        "    elif loss_type == 'sorensen':\r\n",
        "        union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\r\n",
        "    \r\n",
        "    return (2. * intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())\r\n",
        "\r\n",
        "    # num_sum = 2.0 * tf.reduce_sum(y_true * y_predicted) + tf.keras.backend.epsilon()\r\n",
        "    # den_sum = tf.reduce_sum(y_true) + tf.reduce_sum(y_predicted) + tf.keras.backend.epsilon()\r\n",
        "    # # den_sum = tf.reduce_sum(tf.square(y_predicted)) + tf.reduce_sum(tf.square(y_true)) + tf.keras.backend.epsilon()\r\n",
        "\r\n",
        "    # return np.ndarray([1 - num_sum/den_sum], dtype = 'float32')\r\n",
        "\r\n",
        "loss = dice_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-t8De6u_fZn"
      },
      "source": [
        "The function `train_step` will be responsible on each epoch to:\r\n",
        "1. iterate over each example in the training Dataset grabbing its features (x) and label (y).\r\n",
        "2. Using the example's features, make a prediction and compare it with the label. To be able to compare it correctly the label_mask shouls be applied.\r\n",
        "3. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\r\n",
        "4. Use the optimizer to update the model's variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaevRo3dXex9"
      },
      "source": [
        " def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask, loss_fct=loss):\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "      logits = ner(input_ids, input_mask, segment_ids, valid_ids, training=True)\r\n",
        "        # batchsize, max_seq_length, num_labels\r\n",
        "      label_ids_masked = tf.boolean_mask(label_ids, label_mask)\r\n",
        "      logits_masked = tf.boolean_mask(logits, label_mask)\r\n",
        "      scce_loss = loss_fct(label_ids_masked, logits_masked)\r\n",
        "\r\n",
        "  gradients = tape.gradient(scce_loss, ner.trainable_variables)\r\n",
        "  optimizer.apply_gradients(list(zip(gradients, ner.trainable_variables)))\r\n",
        "  return scce_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-x_i17q4DP1"
      },
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOwWJKAcFzme"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A0-7Q3tFx2c"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrGLX3FbF6at"
      },
      "source": [
        "for epoch in range(1):\r\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\r\n",
        "  \r\n",
        "  # Training loop - using batches\r\n",
        "  for (input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask) in batched_train_data:\r\n",
        "    max_seq_len = 128\r\n",
        "    l_input_ids      = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\n",
        "    l_token_type_ids = tf.keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\n",
        "\r\n",
        "    output = l_bert([l_input_ids, l_token_type_ids])          # [batch_size, max_seq_len, hidden_size]\r\n",
        "    model = tf.keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\r\n",
        "    model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\r\n",
        "    \r\n",
        "    # loss = train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask)\r\n",
        "\r\n",
        "    # epoch_loss_avg.update_state(loss)\r\n",
        "    # f1_metric.update_state()\r\n",
        "    print(f'loss : {epoch_loss_avg.result()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90VJKgLKiXdq"
      },
      "source": [
        "## Entity Linking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m11CTxXgibRt"
      },
      "source": [
        "The Entity Linking (EL) process transforms amiguous textual mention to a unique identifier by looking at the context in which the mention occurs. Thus it can be looked as 2 step process after the NER:\r\n",
        "1. Creation of Entity Linker - list of candidates for each mention generation\r\n",
        "2. Reduce the list to the final ID that represents the correct name.\r\n",
        "\r\n",
        "This is generally the method used in `spacy` module.\r\n",
        "\r\n",
        "Another option used for this is used in `deeppavlov` module (http://docs.deeppavlov.ai/en/master/features/models/entity_linking.html)where:\r\n",
        "1. NER is fed to tf-idf Vectorizer and the resulting sparse vector is converted to dense vector.\r\n",
        "2. A library called Faiss (https://github.com/facebookresearch/faiss) is used to find the k-nearest neighbours for tf-idf vector in the matrix where each row is a tf-idf vectors of words in entity titles.\r\n",
        "3. entities are ranked by number of relations in Wikidata (number of outgoing edges of nodes in the knowledge graph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Rwzz77iHaP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6t76Ft8iN9F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev807Omviqes"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaJkcSbuir8H"
      },
      "source": [
        "## Conclusion and Future Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaQ_6W4aixJU"
      },
      "source": [
        "Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIMcQMsiv_2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI_01lUDizGb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8g5xj1iz6G"
      },
      "source": [
        "## Ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJsVMxR_i5Bl"
      },
      "source": [
        "**Reconstructing NER Corpora: a Case Study on Bulgarian** - Iva Marinova, Laska Laskova, Petya Osenova, Kiril Simov, Alexander Popov - Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 4647–4652, Marseille, 11–16 May 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcmS0ntOi40-"
      },
      "source": [
        "**Tuning Multilingual Transformers for Named Entity Recognition on\r\n",
        "Slavic Languages** - Mikhail Arkhipov, Maria Trofimova, Yuri Kuratov, Alexey Sorokin - Neural Networks and Deep Learning Laboratory, Moscow Institute of Physics and Technology, Faculty of Mathematics and Mechanics, Moscow State University - Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 89–93, Florence, Italy, 2 August 2019. - https://www.aclweb.org/anthology/W19-3712.pdf\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFStE4fNi4dj"
      },
      "source": [
        "**BERT: Pre-training of Deep Bidirectional Transformers for\r\n",
        "Language Understanding** - Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova - Google AI Language, 24 May 2019 - https://arxiv.org/pdf/1810.04805.pdf - https://github.com/google-research/bert\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWT-xT0ex9P1"
      },
      "source": [
        "**Attention Is All You Need** - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin, 6 Dec 2017 - https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB8R_2r-CnYX"
      },
      "source": [
        "**A Survey on Deep Learning for Named Entity Recognition** - Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li - IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 18 Mar 2020 - https://arxiv.org/pdf/1812.09449v3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxlANqfFcWY"
      },
      "source": [
        "**Zero-Resource Cross-Domain Named Entity Recognition** - Zihan Liu, Genta Indra Winata, Pascale Fung - Center for Artificial Intelligence Research (CAiRE), Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, 19 May 2020 - https://arxiv.org/pdf/2002.05923.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbLxoqMU-Eia"
      },
      "source": [
        "**Exploring Cross-sentence Contexts for Named Entity Recognition with BERT** - Jouni Luoma, Sampo Pyysalo - Turku NLP group, University of Turku, Finland, 2 Jun 2020 - https://arxiv.org/pdf/2006.01563v1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbaNql7k2j9y"
      },
      "source": [
        "**NER with BERT in Action** - Bill Huang - July 30, 2019- https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcNDPUIv4Fm3"
      },
      "source": [
        "**The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)** - Jay Alammar blog - http://jalammar.github.io/illustrated-bert/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlvqgC_rUXm5"
      },
      "source": [
        "**Deep contextualized word representations** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer - Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science & Engineering, University of Washington, 22 Mar 2018 - https://arxiv.org/pdf/1802.05365.pdf\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPzo1mNoU0F-"
      },
      "source": [
        "**Improving Language Understanding by Generative Pre-Training** - Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever - Open AI - https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LYtPCX2usln"
      },
      "source": [
        "**Introduction to the conll-2003 shared task: Language independent named entity recognition.** - Tjong Kim Sang, E. F. and De Meulder, F. (2003) - https://arxiv.org/pdf/cs/0306050.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m28l7lsJxT-"
      },
      "source": [
        "**Large-Scale Multi-Label Text Classification on EU Legislation** - Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis,\r\n",
        "Ion Androutsopoulos - Department of Informatics, Athens University of Economics and Business, Greece (June 2019) - https://arxiv.org/pdf/1906.02192v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YotmtdHUvqxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}