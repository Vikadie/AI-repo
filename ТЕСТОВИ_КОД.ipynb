{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ТЕСТОВИ КОД.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIDs8RTXcSVKqBJogt4RAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikadie/AI-repo/blob/master/%D0%A2%D0%95%D0%A1%D0%A2%D0%9E%D0%92%D0%98_%D0%9A%D0%9E%D0%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dssOW5BCZOnY"
      },
      "source": [
        "Привет!\r\n",
        "\r\n",
        "Първото, което трябва да се направи е да се последва бутона горе \"Open in Colab\". Разчитам, че имаш някакъв акаунт в Google, за да можеш да ползваш услугите.\r\n",
        "\r\n",
        "Счетох, че това е най-подходящото място за тестване на програмата, тъй като изисква инсталирането на няколко зависимости, които отнемат около 1,5 минути като време, но може да се направи само веднъж. За да тръгне е необходимо да се направи следното:\r\n",
        "1. Отвори от падащите менюта горе \"Среда на изпълнението\" и избери \"Промяна на типа на изпълнение\". Като \"Хардуерен ускорител\" избери GPU + \"ЗАПАЗВАНЕ\".\r\n",
        "2. При първото отваряне и \"Свързване с хостваната среда за изпълнение\" трябва да се появи зеленото тикче в десния горен ъгъл. В случай, че не е, просто натисни върху \"Повторно свързване\" и то ще се свърже.\r\n",
        "\r\n",
        "ВНИМАНИЕ! При престой над известно време без да правиш нищо, Google ще те изключи. Ако престиш повече от 12 часа, ще те изключи за поне 12 часа. Google може и да са режат и по- често по тяхно усмотрение.\r\n",
        "\r\n",
        "3. Секцията `INSTALLING DEPENDENCIES` и `МАЛКО КОД` не е необходимо да се променят - те ще се изпълняват само веднъж. Ще трябва да последваш линка и да сложиш кода си. Тази първа част ще отнеме най-много време, но е само първоначално.\r\n",
        "4. След тази секция следва частта с `INPUT = \"\"\"`. От следващия ред надолу с Copy/Paste може да сложиш твоя текст. НЕ ТРИЙ И ПОСЛЕДНИТЕ КАВИЧКИ `\"\"\"` на най-долния ред. Важни са!\r\n",
        "5. Следва стартиране на всички клетки, които можеш да направиш с \"CTRL + F9\" (когато стартираш за първи път, за да мине през т.3) или ако вече си изпълнявал целия код веднъж с \"Shift + Enter\" на клетката с `INPUT = \"\"\"` и пак същото на клетката `OUPUT`, за да извади отговора."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnSrHE92glCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07fc55e-bdad-4943-cabd-161a23644ea7"
      },
      "source": [
        "#@title INSTALLING DEPENDENCIES { display-mode: \"form\" }\n",
        "import os\n",
        "import re\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install -q -U tf-models-official\n",
        "\n",
        "from official.nlp import bert\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "!pip install kora\n",
        "from kora import drive\n",
        "drive.download_folder('10_qbB6osqPSesCnzcH2Dw1VFFadf5F0U')\n",
        "\n",
        "# Set up tokenizer to generate Tensorflow dataset\n",
        "tokenizer = FullTokenizer(\n",
        "    vocab_file='/content/trainedv2_bert_multi_cased_L-12_H-768_A-12/assets/vocab.txt',\n",
        "     do_lower_case=False)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))\n",
        "\n",
        "print(\"Model loading...\", end=\"\")\n",
        "new_model = tf.saved_model.load('/content/trainedv2_bert_multi_cased_L-12_H-768_A-12')\n",
        "print(\"DONE!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 47.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 55.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 83kB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 47.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 58.1MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kora\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/0d/3d9ab9ee747f0925b038e8350ce137276a7a4730a96a3516485dc1b87ba3/kora-0.9.19-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hCollecting fastcore\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/98/60404e2817cff113a6ae4023bc1772e23179408fdf7857fa410551758dfe/fastcore-1.3.19-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from kora) (5.5.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastcore->kora) (19.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastcore->kora) (20.9)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (53.0.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastcore->kora) (2.4.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kora) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.19 kora-0.9.19\n",
            "10tmWUyTcuEwozKoGUkwLnGfZo24nHavQ assets application/vnd.google-apps.folder (1/3)\n",
            "113-n2GuMEVCE99oWgbkP3gIxl6htXgAB vocab.txt text/plain (1/1)\n",
            "Download 100%.\n",
            "10nOqZWx9GJ5k9pQiS93rw7IaO-bPPrLG saved_model.pb application/octet-stream (2/3)\n",
            "Download 100%.\n",
            "10hgXkcx-WJKxURRr5ZtwcvHkxhgDv2PV variables application/vnd.google-apps.folder (3/3)\n",
            "10izF53tsbeD9H6WUnXNN0IDhiy7g-NDW variables.data-00000-of-00001 application/octet-stream (1/2)\n",
            "Download 100%.\n",
            "10mvEkkqJZZ0J_z4qtO8q_9oY3rmuu-wL variables.index application/octet-stream (2/2)\n",
            "Download 100%.\n",
            "Vocab size: 119547\n",
            "Model loading...DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6xopKPIh0z1"
      },
      "source": [
        "#@title МАЛКО КОД { display-mode: \"form\" }\n",
        "def preparing_parameters(input):\n",
        "    \"\"\"\n",
        "    function transforming a juridical text file in word-tokens\n",
        "    Params:\n",
        "            input: str, the input text\n",
        "            outcome: str, reorganized text\n",
        "    Returns: outcome.\n",
        "             Prints a message when it is done.\n",
        "    \"\"\"\n",
        "    splitted_sentences = []\n",
        "    sentence = \"\"\n",
        "    pattern = r'(?<=.[.?!:]) +(?=[А-ЯA-Z][А-Яа-яA-Za-z]+[^\\.])'\n",
        "    p = r'(?<=[^А-Я].[.?!:]) +(?=[А-ЯA-Z][А-Яа-яA-Za-z]+[^\\.])'\n",
        "\n",
        "    for line in input.split('\\n'):\n",
        "        b = re.split(pattern, line.rstrip())\n",
        "\n",
        "        for words in b:\n",
        "            if not sentence:\n",
        "                sentence += words\n",
        "                continue\n",
        "            try:\n",
        "                if words[-1] in ['.', '!', '?', ':', '...']:\n",
        "                    sentence += ' ' + words\n",
        "                    if words.split()[-1] not in ['гр.', 'с.']:\n",
        "                        splitted_sentences.append(sentence)\n",
        "                        splitted_sentences.append('\\n\\n')\n",
        "                        sentence = \"\"\n",
        "                else:\n",
        "                    sentence += ' ' + words\n",
        "            except:\n",
        "                splitted_sentences.append(sentence)\n",
        "                splitted_sentences.append('\\n\\n')\n",
        "                sentence = \"\"\n",
        "\n",
        "    output = \"\"\n",
        "    for row in splitted_sentences:\n",
        "        lst = row.split()\n",
        "        if lst:\n",
        "            for word in lst:\n",
        "                w = re.split(r'([()„“\\';―\"/\\[\\.\\],:])', word)\n",
        "                for c in w:\n",
        "                    if c:\n",
        "                        output += str(c.rstrip('\\n')) + '\\n'\n",
        "        else:\n",
        "            output += ('\\n')\n",
        "\n",
        "    print(\"DONE! With the text re-organization!!!\")\n",
        "\n",
        "    return output\n",
        "\n",
        "########################################################\n",
        "# CONSTANTS\n",
        "MAX_SEQ_LENGTH = 256\n",
        "label_list = [\"O\", \"X\", \"B-LAW\", \"I-LAW\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \n",
        "              \"B-LOC\", \"I-LOC\", \"B-PRO\", \"I-PRO\", \"B-EVT\", \"I-EVT\", \"[CLS]\", \"[SEP]\"]\n",
        "\n",
        "label_map = {label: i for i, label in enumerate(label_list, 1)}\n",
        "\n",
        "decode_label_map = {i: label for label, i in label_map.items()}\n",
        "\n",
        "#######################################################\n",
        "# READING DATA\n",
        "def read_data(text):\n",
        "  \"\"\"\"reading the file and returning for each sentence list of its words-tokens\n",
        "  \n",
        "  side-effects: printing the number of sentences inside the set\n",
        "  \"\"\"\n",
        "  data, sentence= [], []\n",
        "  num_sentense = 0\n",
        "  f = text.split('\\n')\n",
        "  for line in f:\n",
        "    if len(line) == 0 or line[0] == '\\n':\n",
        "      if len(sentence) > 0:\n",
        "        data.append(sentence)\n",
        "        sentence = []\n",
        "        num_sentense += 1\n",
        "      continue\n",
        "    word = line.rstrip('\\n')\n",
        "    sentence.append(word)\n",
        "\n",
        "  if len(sentence) > 0:\n",
        "    data.append(sentence)\n",
        "    num_sentense += 1\n",
        "\n",
        "  print(\"Number of sentences:\", num_sentense)\n",
        "  return data\n",
        "\n",
        "class InputFeatures:\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, ntokens, input_ids, input_mask, segment_ids, valid_ids=None, label_mask=None):\n",
        "      self.ntokens = ntokens  # only for representatinal purpose\n",
        "      self.input_ids = input_ids  # encoded words\n",
        "      self.input_mask = input_mask  # mask indicating padding or not\n",
        "      self.segment_ids = segment_ids  # we talk about NER task, so the segment_ids will be just '0's\n",
        "      self.valid_ids = valid_ids  # when the word is split to subwords, it indicates only the first id as valid '1', next subwords as '0'\n",
        "\n",
        "def model_input_preprocess(input_word_i, input_m, input_type_i):\n",
        "  \"\"\"function transforming the required three inputs by the bert model\n",
        "  to a single dictionary with the respective key names\"\"\"\n",
        "\n",
        "  return dict(\n",
        "      input_word_ids=input_word_i, \n",
        "      input_mask=input_m, \n",
        "      input_type_ids=input_type_i,\n",
        "  )\n",
        "\n",
        "def convert_to_features(read_data, tokenizer, max_seq_length=MAX_SEQ_LENGTH):\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of `InputFeatures`s.\n",
        "    Reflects subwords split in `valid_ids` variable.\n",
        "    Applies padding to all features. Stack the features all together.\n",
        "    \"\"\"\n",
        "\n",
        "    features = []  # the list of `InputFeatures` to be returned\n",
        "    for line in read_data:\n",
        "        ntokens = []\n",
        "        valid_ids = []\n",
        "        for word in line:\n",
        "            token = tokenizer.tokenize(word)\n",
        "            ntokens.extend(token)\n",
        "            for m in range(len(token)):\n",
        "                if m == 0:\n",
        "                    valid_ids.append(1)\n",
        "                else:\n",
        "                    valid_ids.append(0)\n",
        "        \n",
        "        # checking if a sentence is longer than max_seq_length, if yes -> cut it\n",
        "        if len(ntokens) >= max_seq_length - 1:\n",
        "            ntokens = ntokens[0:(max_seq_length - 2)]\n",
        "            valid_ids = valid_ids[:(max_seq_length - 2)]\n",
        "\n",
        "        # adding the mandatory ['CLS'] at the beginning\n",
        "        ntokens.insert(0, \"[CLS]\")\n",
        "        valid_ids.insert(0, 0)\n",
        "\n",
        "        # adding the mandatory ['SEP'] at the end of each sentence\n",
        "        ntokens.append(\"[SEP]\")\n",
        "        valid_ids.append(0)\n",
        "\n",
        "        # transforming `ntokens` to BERT's tokenizer ids\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
        "        input_mask = [1] * len(input_ids) # creation of input_mask of size input_ids\n",
        "\n",
        "        # padding\n",
        "        input_ids = pad_sequences([input_ids], padding='post', maxlen=max_seq_length)\n",
        "        input_mask = pad_sequences([input_mask], padding='post', maxlen=max_seq_length)\n",
        "        segment_ids = np.zeros_like(input_mask, dtype='int32')\n",
        "        valid_ids = pad_sequences([valid_ids], padding='post', maxlen=max_seq_length)\n",
        "\n",
        "        # last check - all lengths should correspond to max_seq_length,\n",
        "        # dtype should be 'int32' and of type 'numpy'\n",
        "        assert input_ids.shape[1] == max_seq_length\n",
        "        assert input_mask.dtype == 'int32'\n",
        "        assert segment_ids.dtype == 'int32'\n",
        "        assert type(input_mask) == type(segment_ids)\n",
        "\n",
        "        # adding to the list\n",
        "        features.append(\n",
        "            InputFeatures(ntokens=ntokens,\n",
        "                          input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          segment_ids=segment_ids,\n",
        "                          valid_ids=valid_ids))\n",
        "    return features\n",
        "\n",
        "def transform_to_dataset(features):\n",
        "  \"\"\"function converting InputFeatures to a Dataset, imitating the pre-process output:\n",
        "  1. prefetched, cached, batched dictionaries consisting of:\n",
        "    - dictionary with keys ['input_word_ids', 'input_mask', 'input_type_ids'] as inputs\n",
        "    all above tensors the required dtype = 'int32' and size [BATCH_SIZE, MAX_SEQ_LENGTH]\n",
        "  2. 'valid_ids' parameter as output - this is additional to the official pre-process output\n",
        "  3. with number of features (sentences)\"\"\"\n",
        "\n",
        "  all_input_ids = tf.data.Dataset.from_tensor_slices(\n",
        "      ([f.input_ids[0] for f in features]))\n",
        "  all_input_mask = tf.data.Dataset.from_tensor_slices(\n",
        "      ([f.input_mask[0] for f in features]))\n",
        "  all_segment_ids = tf.data.Dataset.from_tensor_slices(\n",
        "      ([f.segment_ids[0] for f in features]))\n",
        "  all_valid_ids = tf.convert_to_tensor(\n",
        "      [f.valid_ids[0] for f in features])\n",
        "\n",
        "  # Dataset using tf.data\n",
        "  data = tf.data.Dataset.zip(\n",
        "      (all_input_ids, all_input_mask, all_segment_ids))\n",
        "  \n",
        "  number_features = len(features)\n",
        "  batched_data = data.batch(16)\n",
        "\n",
        "  dataset = batched_data.map(lambda input_ids, input_mask, type_ids: \n",
        "                             (model_input_preprocess(input_ids, input_mask, type_ids))\n",
        "                             )\n",
        "  dataset = dataset.cache().prefetch(buffer_size = tf.data.AUTOTUNE)\n",
        "\n",
        "  return dataset, all_valid_ids, number_features\n",
        "\n",
        "def data_preprocess(text, label_map, tokenizer=tokenizer):\n",
        "  \"\"\"\n",
        "  function englobelling the customized bert pre-process of data for a NER task\n",
        "  params: dataset_type: string with possible dataset_types: 'train', 'val', 'test'\n",
        "          path: the directory where the datasets are situated\n",
        "          label_map: desired mapping of all the available labels + including '[SEP]' and '[CLS]'\n",
        "          training: defaults to True. If False, the function returns the output \n",
        "                    of 'readed_data' function as well\n",
        "  returns: tuple of zipped dataset and batched dataset\n",
        "  \"\"\"\n",
        "  read_d = read_data(text)\n",
        "\n",
        "  features = convert_to_features(read_data=read_d, tokenizer=tokenizer)\n",
        "\n",
        "  to_return = transform_to_dataset(features=features)\n",
        " \n",
        "  return (read_d, to_return)\n",
        "\n",
        "#################################################\n",
        "# DECODING PART\n",
        "def transf_x_to_oth(predictions, tag='O'):\n",
        "  \"\"\"\n",
        "  manual transformation of 'X' to 'O' or other previous tag in labels\n",
        "  params: \n",
        "        predictions: list with predicted tags\n",
        "        to_tag: default 'O', but could be some other tag\n",
        "                if 'previous' than assigns ths tag that preceeds the tag\n",
        "                changing the prefix to 'I-' if needed\n",
        "  \"\"\"\n",
        "  for y in predictions:\n",
        "    while 'X' in y:\n",
        "      idx = y.index('X')\n",
        "      if tag == 'previous' and idx > 0:\n",
        "          to_tag = y[idx - 1]\n",
        "          if to_tag.startswith('B'):\n",
        "              to_tag = 'I' + to_tag[1:]\n",
        "      else:\n",
        "          to_tag = 'O'\n",
        "      y[idx] = to_tag\n",
        "  return predictions\n",
        "\n",
        "def decode_tags(nd_array, valid_ids, decode_label_map=decode_label_map, tag='O'):\n",
        "  \"\"\"\n",
        "  decoding function using the dictionary translating numbers to valid labels\n",
        "  by removing all subwords and padding based on the inserted valid_ids\n",
        "  if still padding met -> automatic translation as 'O'\n",
        "  params:\n",
        "          nd_array: array in format ndarray with labels/prediction\n",
        "          valid_ids: tensor, containing the valid_ids\n",
        "          decode_label_map: dictionary with correct mapping of labels vs index\n",
        "          to_tag: ['O', 'previous'] Replace errors in predictions: 'X', 0 (padding)\n",
        "                  '[SEP]', '[CLS]' with 'O' or previous tag. Default to previous\n",
        "  \"\"\"\n",
        "  if tag not in ['O', 'previous']:\n",
        "    raise ValueError(f'\"to_tag\" value of {to_tag} not supported. \\\n",
        "                        You have to choose between \"O\" and \"previous\"')\n",
        "  if tag == 'previous':\n",
        "    temp = [\n",
        "            [decode_label_map[i] if i not in [0, 15, 16] else 'X' \n",
        "              for i in tf.boolean_mask(tag_id, valid_id).numpy()] \n",
        "              for tag_id, valid_id in zip(nd_array, valid_ids)\n",
        "            ]\n",
        "    return transf_x_to_oth(temp, tag='previous')\n",
        "  return [\n",
        "          [decode_label_map[i] if i not in [0, 2, 15, 16] else 'O' \n",
        "           for i in tf.boolean_mask(tag_id, valid_id).numpy()] \n",
        "           for tag_id, valid_id in zip(nd_array, valid_ids)\n",
        "          ]\n",
        "\n",
        "def predictions_postprocess(model, dataset, valid_ids):\n",
        "  \"\"\"\n",
        "  preparing the labels and predictions, based on the inserted model,\n",
        "  in same shape as valid_ids\n",
        "  Returns: labels and predictions as ndarrays when 'return_decoded' is False\n",
        "           or decoded labels and prediction when 'return decoded' is True\n",
        "  \"\"\"\n",
        "  labs , preds = None, None\n",
        "  for batch, test_inp in enumerate(dataset):\n",
        "    predictions = model(test_inp)\n",
        "    pred = tf.argmax(predictions, axis=2)\n",
        "    if batch == 0:\n",
        "      preds = pred\n",
        "    else:\n",
        "      preds = tf.experimental.numpy.append(preds, pred, axis=0)\n",
        "  assert preds.shape == valid_ids.shape\n",
        "\n",
        "  # decoding ids to true labels\n",
        "  y_pred = decode_tags(preds, valid_ids, tag='previous')\n",
        "  return y_pred"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wq2DMVvhVZs"
      },
      "source": [
        "# НЕ ТРИЙ СЛЕДВАЩИЯ РЕД - СЛОЖИ ТЕКСТА ЗАПОЧВАЙКИ ОТ РЕДА СЛЕД КАВИЧКИТЕ\r\n",
        "INPUT = \"\"\"\r\n",
        "Р Е Ш Е Н И Е\r\n",
        "№ 778\r\n",
        "гр. София, 11.02.2021 г.\r\n",
        "В ИМЕТО НА НАРОДА\r\n",
        "АДМИНИСТРАТИВЕН СЪД - СОФИЯ-ГРАД, ХХІІ КАСАЦИОНЕН\r\n",
        "СЪСТАВ, в публично заседание на 15.01.2021 г. в следния състав:\r\n",
        "ПРЕДСЕДАТЕЛ: Светлана Димитрова\r\n",
        "ЧЛЕНОВЕ: Радина Карамфилова-Десподска\r\n",
        "Ванина Колева\r\n",
        "при участието на секретаря Александра Вълкова и при участието на\r\n",
        "прокурора Стоян Димитров, като разгледа дело номер 11068 по описа за\r\n",
        "2020 година докладвано от съдия Радина Карамфилова-Десподска, и за да се\r\n",
        "произнесе взе предвид следното:\r\n",
        "Производството е по реда на чл. 208 - чл. 228 от\r\n",
        "Административнопроцесуалния кодекс /АПК/, във вр. с чл. 63, ал. 1, изр. второ от\r\n",
        "Закона за административните нарушения и наказания /ЗАНН/.\r\n",
        "Образувано е по касационна жалба, подадена от И. Т. Д. чрез адв. А. Г. С. от\r\n",
        "АК С. З., със съдебен адрес [населено място], [улица], ет.1, офис 7 срещу Решение №\r\n",
        "20209414/28.09.2020 г., постановено по н.а.х.д. № 4613/2020 г. по описа на Софийски\r\n",
        "районен съд /СРС/, НО, 114 – ти състав, с което е потвърдено Наказателно\r\n",
        "постановление /НП/ № 19-4332-025675/09.12.2019г. на началник група към\r\n",
        "СДВР-ОПП-СДВР и с което на касатора са наложени административни наказания\r\n",
        "„глоба“ в размер на 750 лв. и „лишаване от право да управлява МПС“ за срок от 3\r\n",
        "месеца на основание чл. 182, ал. 1, т. 6 от Закона за движението по пътищата /ЗДвП/ за\r\n",
        "извършено нарушение на чл. 21, ал. 1 от ЗДвП, като на основание Наредба № Iз-2539\r\n",
        "на МВР са му отнети 12 контролни точки.\r\n",
        "С жалбата се навеждат доводи за незаконосъобразност и неправилност на\r\n",
        "решението на СРС. Счита, че по делото не е установено безспорно извършването на\r\n",
        "административно нарушение. Твърди се, че е налице допуснато нарушение на\r\n",
        "разпоредбата на чл. 10, ал. 3 от Наредба № 8121з-532 от 12.05.2015 г. за условията и\r\n",
        "реда за използване на автоматизирани технически средства и системи за контрол на\r\n",
        "правилата за движение по пътищата. Иска се отмяната на решението и на\r\n",
        "потвърденото с него НП.\r\n",
        "В проведено по делото открито съдебно заседание касаторът, редовно\r\n",
        "призован, не се явява и не изпраща представител.\r\n",
        "Ответникът - Столична дирекция на вътрешните работи, отдел „Пътна\r\n",
        "полиция“, не изпраща представител и не изразява становище по жалбата.\r\n",
        "Представителят на Софийска градска прокуратура дава заключение за\r\n",
        "неоснователност на жалбата.\r\n",
        "Административен съд София – град, ХХII касационен състав, като се запозна с\r\n",
        "обжалваното съдебно решение, съобрази доводите и възраженията на страните и\r\n",
        "обсъди наведените касационни основания и тези по чл. 218, ал. 2 от АПК, намира за\r\n",
        "установено от фактическа и правна страна следното:\r\n",
        "По допустимостта на касационната жалба:\r\n",
        "Касационната жалба е подадена в срока по чл. 211, ал. 1 от АПК, във вр.с чл.\r\n",
        "63, ал. 1, изр. второ от ЗАНН и от надлежна страна, която има право и интерес от\r\n",
        "обжалването, поради което е процесуално допустима и следва да бъде разгледана\r\n",
        "досежно нейната основателност.\r\n",
        "По основателността на касационната жалба:\r\n",
        "Разгледана по същество касационната жалба е неоснователна.\r\n",
        "Обжалваното съдебно решение е валидно и допустимо, като постановено по\r\n",
        "подадена в срок жалба срещу оспореното наказателно постановление, не са изтекли,\r\n",
        "както сроковете по чл. 34 ЗАНН, така и абсолютната погасителна давност за\r\n",
        "административнонаказателно преследване.\r\n",
        "От фактическа страна въз основа на събраните по реда на НПК доказателства и\r\n",
        "доказателствени средства районният съд е приел за безспорно установено следното:\r\n",
        "На 19.04.2019 г. в 10.07 часа И. Т. Д. управлявала лек автомобил „Тойота“ Рав 4 2.2 Д\r\n",
        "с рег. [рег.номер на МПС] в [населено място] по [улица]с посока на движение от\r\n",
        "[улица]към [улица]като при стопанство „Врана“, управляваният от Б. лек автомобил\r\n",
        "се движел със скорост 110 км/ч. (107 км/ч след корекция от 3% приспаднат толеранс в\r\n",
        "полза на водача) при ограничение на скоростта от 50 км/ч за населено място, въведено\r\n",
        "с пътен знак В-26. Нарушението е било установено посредством техническо средство\r\n",
        "система за видеоконтрол TFR 1-М № 506 в клип № 6206 по преписка ПК-2/0076/19 г.,\r\n",
        "записваща и заснемаща дата, час, скорост, място и рег. № на МПС. След установяване\r\n",
        "самоличността на собственика на автомобила, И. Т. Д. саморъчно попълнила\r\n",
        "декларация на 20.09.2019 г. в която посочила, че на 19.04.2019 г. тя е управлявала\r\n",
        "автомобила.\r\n",
        "За това нарушение на водача е съставен АУАН с бл. № 0862793/02.10.2019 г. за\r\n",
        "нарушение на чл. 21, ал. 1 от ЗДвП, предявен на нарушителя, който го е подписал без\r\n",
        "възражения. Такива не са постъпили в срока по чл. 44, ал. 1 от ЗАНН. Въз основа на\r\n",
        "така съставения АУАН е издадено процесното НП, с което е санкционирана\r\n",
        "жалбоподателката.\r\n",
        "За да постанови този резултат съдът след преценка в съвкупност на събраните\r\n",
        "по делото писмени и гласни доказателствени средства, от правна страна е приел, че\r\n",
        "АУАН и обжалваното НП са издадени от компетентни длъжностни лица, в сроковете\r\n",
        "по чл. 34 от ЗАНН, като били спазени и изискванията на чл. 42 и чл. 52 от ЗАНН и, че\r\n",
        "при издаването им не са допуснати съществени процесуални нарушения. По същество\r\n",
        "на спора въззивния съд е изложил съображения, че нарушението е безспорно доказано\r\n",
        "от обективна и субективна страна, квалифицирано е правилно от наказващия орган, \r\n",
        "като съответно по вид и размер е и наложената административна санкция. По\r\n",
        "изложените съображения съдът е достигнал до извода, че\r\n",
        "административнонаказателната отговорност на касатора е ангажирана\r\n",
        "законосъобразно, поради което е потвърдил НП.\r\n",
        "Пред настоящата инстанция не са ангажирани нови писмени доказателства по\r\n",
        "смисъла на чл. 219, ал. 1 от АПК.\r\n",
        "Решението е правилно и законосъобразно, съответно на материалния закон и\r\n",
        "постановено без допуснати съществени нарушения на съдопроизводствените правила\r\n",
        "по аргументите изложени в него.\r\n",
        "Не са налице наведените в жалбата касационни основания.\r\n",
        "Настоящият касационен състав намира за правилни и законосъобразни\r\n",
        "изложените мотиви в оспореното решение, че в хода на\r\n",
        "административнонаказателното производство не са допуснати съществени нарушения\r\n",
        "на процесуалните правила при провеждане на административно наказателното\r\n",
        "производство, представляващи самостоятелно основание за отмяна на атакуваното\r\n",
        "наказателно постановление.\r\n",
        "Актът за установяване на административно нарушение е редовно съставен в\r\n",
        "присъствието на нарушителя и свидетели - С. К. К. и В. Х. А., възприели\r\n",
        "извършването на нарушението. Съгласно чл. 189, ал. 2 от ЗДвП, в\r\n",
        "административнонаказателното производство редовно съставеният акт за\r\n",
        "установяване на административно нарушение има обвързваща доказателствена сила\r\n",
        "до доказване на противното, което в случая не е сторено по какъвто и да е начин от\r\n",
        "страна на касатора в настоящото производство, пред въззивната инстанция.\r\n",
        "Видно от представените по делото писмени доказателства – удостоверение за\r\n",
        "одобрен тип средство за измерване и посочени в него дата на издаване и срок на\r\n",
        "валидност, Протокол № 2-38-18/24.10.2018 г. от проверка на мобилна система за\r\n",
        "видеоконтрол TFR – 1M снимков материал се доказва противното на твърдяното от\r\n",
        "жалбоподателя.\r\n",
        "Настоящият касационен състав споделя и всички изложени мотиви от\r\n",
        "въззивния съд, поради което и на основание чл. 221, ал.1, изр.2 от АПК, същите не\r\n",
        "следва да се преповтарят. Съдът пълно и всестранно е изследвал фактическата\r\n",
        "обстановка и въз основа на събраните доказателства е извел правилни правни изводи.\r\n",
        "Твърденията на нарушителя, изложени пред районния съд са обсъдени в решението\r\n",
        "му, поради което възражението в касационната жалба, че съдът не е изяснил\r\n",
        "фактическата обстановка е неоснователно.\r\n",
        "От събраните по преписката доказателства в тяхната съвкупност категорично\r\n",
        "се установява от субективна и обективна страна нарушението по чл. 21, ал. 1 от ЗДвП.\r\n",
        "Квалификацията е правилна и не липсват елементи от фактическия състав на\r\n",
        "нарушението и деянието е описано със съставомерните си признаци.\r\n",
        "Сочената за нарушена разпоредба на чл. 21, ал. 1 от ЗДвП забранява на\r\n",
        "водачите на МПС, каквото качество касаторът безспорно е притежавал, при избиране\r\n",
        "скоростта на движение, да превишават посочените в разпоредбата стойности, като за\r\n",
        "населено място това е 50 км/ч., за леки автомобили. Безспорно се установява, че на\r\n",
        "посочената в АУАН и НП дата и час Д. е управлявала лекия си автомобил със скорост\r\n",
        "110 км/ч при ограничение 50 км/ч. Материалноправните предпоставки са налице за\r\n",
        "законосъобразното ангажиране на административнонаказателната отговорност на\r\n",
        "касатора. В случая на първо място касаторът се е движил в границите на населено\r\n",
        "място, където ограничението е по закон 50 км/ч. Както е видно от направената снимка,\r\n",
        "приложена като доказателство по делото, мястото на контрол е било сигнализирано с\r\n",
        "пътен знак В-26.\r\n",
        "В отлика от изложеното в касационната жалба, въззивният съд е направил\r\n",
        "пълен и последователен анализ на доказателствата по делото, като е формирал\r\n",
        "правилни и законосъобразни изводи по отношение на оплакванията, наведени и във\r\n",
        "възивната жалба за извършеното нарушение, в т.ч. и относно засечената скорост, дали\r\n",
        "е с работещо средство, мястото на извършване на нарушение, при отговор дали са\r\n",
        "спазени правилата по отчитане на заснемането на нарушението.\r\n",
        "СРС е изложил мотиви и относно размера на глобата. Съобразено е, че при\r\n",
        "определяне размера на наказанието се взема предвид характерът и тежестта на\r\n",
        "нарушението, както и важността на засегнатите с него обществени отношения. При\r\n",
        "действащата редакция на санкционната норма, глобата е в размер на 350 лв. и по 50\r\n",
        "лв., като за всеки следващи 5 км. превишение над 50 км. глобата се увеличава със 50\r\n",
        "лв.\r\n",
        "С оглед на изложеното, настоящият съдебен състав намира, че не са налице\r\n",
        "посочените в жалбата касационни основания, поради което касационната жалба е\r\n",
        "неоснователна, а обжалваното съдебно решение следва да бъде оставено в сила.\r\n",
        "По гореизложените съображения и на основание чл. 221, ал. 2, предл. първо и\r\n",
        "второ АПК, във вр. с чл. 63, ал. 1 ЗАНН, Административен съд – София град, XXII\r\n",
        "касационен състав,\r\n",
        " РЕШИ:\r\n",
        "ОСТАВЯ В СИЛА решение № 20209414/28.09.2020г., постановено по н.а.х.д.\r\n",
        "№ 4613/2020 г. по описа на Софийски районен съд, Наказателно отделение, 114 – ти\r\n",
        "състав.\r\n",
        "Решението е окончателно и не подлежи на обжалване и протест.\r\n",
        "ПРЕДСЕДАТЕЛ:\r\n",
        "ЧЛЕНОВЕ:\r\n",
        "\"\"\"  # ТЕЗИ КАВИЧКИ НЕ СЕ ТРИЯТ СЪЩО"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAcwGh-qqBm-",
        "outputId": "c62f749c-8dc9-41a9-e461-5df1717d81cb"
      },
      "source": [
        "#@title OUTPUT { run: \"auto\", display-mode: \"form\" }\r\n",
        "print(\"Preparing text for analysis...\", end=\"\")\r\n",
        "text = preparing_parameters(INPUT)\r\n",
        "\r\n",
        "sentences, returned = data_preprocess(text, label_map=label_map)\r\n",
        "\r\n",
        "dataset, valid_ids, dataset_size = returned\r\n",
        "print(\"End of text pre-process!\")\r\n",
        "\r\n",
        "print(\"Start of analysis itself...\")\r\n",
        "start = time()\r\n",
        "y_pred = predictions_postprocess(new_model, dataset, valid_ids)\r\n",
        "punctuations = '''!()[]{};:'\"\\,<>./?@#$%^&*_~'''\r\n",
        "\r\n",
        "law_expressions_list = []\r\n",
        "law_expr = ''\r\n",
        "for sen, pred_labels in zip(sentences, y_pred):\r\n",
        "  for i, lab in enumerate(pred_labels):\r\n",
        "    if 'LAW' in lab:\r\n",
        "      if sen[i] not in punctuations:\r\n",
        "        law_expr += ' '\r\n",
        "      law_expr += sen[i] if law_expr else sen[i]\r\n",
        "    elif law_expr:\r\n",
        "      law_expressions_list.append(law_expr)\r\n",
        "      law_expr = ''\r\n",
        "end = time()\r\n",
        "print(\"\\nRecognised legal expressions:\")\r\n",
        "pprint(law_expressions_list, width=120)\r\n",
        "\r\n",
        "print(f\"\\nThe process ended in {end - start}s.\")\r\n",
        "print(f\"Number of recognised legal expressions: {len(law_expressions_list)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing text for analysis...DONE! With the text re-organization!!!\n",
            "Number of sentences: 49\n",
            "End of text pre-process!\n",
            "Start of analysis itself...\n",
            "\n",
            "Recognised legal expressions:\n",
            "[' чл. 208 - чл. 228 от Административнопроцесуалния кодекс',\n",
            " ' АПК',\n",
            " ' чл. 63, ал. 1, изр. второ от Закона за административните нарушения и наказания',\n",
            " ' ЗАНН',\n",
            " ' чл. 182, ал. 1, т. 6 от Закона за движението по пътищата',\n",
            " ' чл. 10, ал. 3 от Наредба № 8121з-532 от 12. 05. 2015 г.',\n",
            " ' чл. 218, ал. 2 от АПК',\n",
            " ' чл. 211, ал. 1 от АПК',\n",
            " ' чл. 63, ал. 1, изр. второ от ЗАНН',\n",
            " ' чл. 34 ЗАНН',\n",
            " ' чл. 21, ал. 1 от ЗДвП',\n",
            " ' чл. 44, ал. 1 от ЗАНН',\n",
            " ' чл. 34 от ЗАНН',\n",
            " ' чл. 42 и чл. 52 от ЗАНН',\n",
            " ' чл. 219, ал. 1 от АПК',\n",
            " ' чл. 189, ал. 2 от ЗДвП',\n",
            " ' чл. 221, ал. 1, изр. 2 от АПК',\n",
            " ' чл. 21, ал. 1 от ЗДвП',\n",
            " ' чл. 21, ал. 1 от ЗДвП',\n",
            " ' чл. 221, ал. 2, предл. първо и второ АПК',\n",
            " ' чл. 63, ал. 1 ЗАНН']\n",
            "\n",
            "The process ended in 3.289665460586548s.\n",
            "Number of recognised legal expressions: 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePrferf3W0wA"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}