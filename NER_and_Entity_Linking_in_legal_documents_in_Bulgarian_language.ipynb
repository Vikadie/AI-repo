{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER and Entity Linking in legal documents in Bulgarian language.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "18QqUo4aVcjoZfepAgU-I6HNL1JRqm6Eu",
      "authorship_tag": "ABX9TyPAopoY63Rr2sMY3xcWUbxp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikadie/AI-repo/blob/master/NER_and_Entity_Linking_in_legal_documents_in_Bulgarian_language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlsqWrBmYiEz"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnhWNKNacJMU",
        "outputId": "611c112f-824a-472b-b3fd-8c3a4dc01de7"
      },
      "source": [
        "import os\r\n",
        "from random import randint, seed\r\n",
        "from time import time\r\n",
        "import json\r\n",
        "from pprint import pprint\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import pickle\r\n",
        "import logging\r\n",
        "\r\n",
        "!pip install nose\r\n",
        "\r\n",
        "from nose.tools import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 21.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40kB 20.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51kB 20.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61kB 18.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 12.0MB/s \n",
            "\u001b[?25hInstalling collected packages: nose\n",
            "Successfully installed nose-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLfOEgGz-N7w",
        "outputId": "ff3fc3d4-feb1-4ec9-d434-92b788da2737"
      },
      "source": [
        "!pip install -q -U tf-models-official\r\n",
        "!pip install bert-for-tf2\r\n",
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 30.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 54.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 36.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 1.3MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/75/2c/2256f28ef35946682ce703e69de914773c3f62048f4de6966d4e2dc1930a/py-params-0.10.1.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp36-none-any.whl size=30534 sha256=ffa3e7b44d929e1efa0479ee593e9393a3bfc40af5f87f8de7366ee80c56daf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.1-cp36-none-any.whl size=7849 sha256=d334e8b11a51a7d74c79abc674cd5840cc7322081f244dc223fcc225b6cd0921\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/13/cf/731530f5760266e69a40217ea27fa0d39a2d2a67230a73e2bc\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19474 sha256=3d3777eda1b2c4fc25c27e91000626318d22e8461dbc3d59d8624f1996870fca\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.1\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 12.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsaasJH4cY9r"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "import tensorflow_hub as hub\r\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\r\n",
        "\r\n",
        "import bert\r\n",
        "from bert import bert_tokenization\r\n",
        "\r\n",
        "#from official.nlp import bert # needed for tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-NtMro9h4Se"
      },
      "source": [
        "from official.nlp.optimization import AdamWeightDecay, WarmUp\r\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twPwH61fYk9j"
      },
      "source": [
        "# NER and Entity Linking in unstructured legal documents in Bulgarian language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otloh6UrcelW"
      },
      "source": [
        "##### Final exam report\r\n",
        "\r\n",
        "*Viktor Belchev - student*\r\n",
        "\r\n",
        "*Deep Learning - Software University*\r\n",
        "\r\n",
        "*February 2021*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ZAILvGhvbs"
      },
      "source": [
        "## Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KQ76Jofh7sb"
      },
      "source": [
        "Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rR9jyxPiPpb"
      },
      "source": [
        "In all legal documents there is usage of citation of different laws or other juridical terms often hidden behind some abbreviations. While this aims to make the text shorter and clearer it is mostly causing troubles in understanding and translation to simple language not only to regular persons, but sometimes even lawyers and people with juridical background feel lost. Therefore, it often requires an additional research in the legal litterature. At this stage another problem might occur - the correct decoding of abbreviated terms can become obstacle on top of the the overall understanding of the information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SDwDTdFmS0a"
      },
      "source": [
        "In this paper, I will try to use the modern approach of Deep Learning to create a helpful tool that overcomes these problems. Using the state-of-the-art available models in the field of Natual Language Processing like BERT and an abbreviation list available at this stage, I will try to achieve an acceptable accuracy in this task for Bulgarian language that is known as combination of two different tasks: Named Entity Recognition and Entity Linking. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC38oPP9iDuY"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HysH52mHqc3n"
      },
      "source": [
        "Generally, in Natural Language Processing (further, NLP) the process of disambiguation of terms is known as Entity Linknig (further, EL), which goes hand in hand with another operation called Named Entity Recognition (further, NER). As explained by Iva Marinova in her **\"Reconstructing NER Corpora: a Case Study on Bulgarian\"** while in the field of Deep Learning \r\n",
        "these two related tasks are considered to be well covered in\r\n",
        "NLP for Germanic, Romance and other language groups,\r\n",
        "they are still under-resourced for the Slavic languages, especially from a multilingual perspective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tppf-Tke1evN"
      },
      "source": [
        "Usually, the order of application of both tasks is by starting with NER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoh_BX7uu4CJ"
      },
      "source": [
        "The purpose of NER is to tag words in a sentences based on some predefined tags, in order to extract some important information of the sentence, like for instanse names, geographical locations, dates, currency etc.\r\n",
        "In NER, each token in the sentence will get tagged with a label, the label will tell the specific meaning of the token. In that way, through NER, we can analyze the sentence with more details and extract the important information.\r\n",
        "\r\n",
        "There are two popular approaches for NER:\r\n",
        "- multi-class classification based where NER is treated as a multi-class classification process, and we can use some text classification method to label the token.\r\n",
        "- Conditional Random Field(CRF) based method labels the token taking context into account, then predicts sequences of labels for sequences of sentence token then get the most reasonable one. It is a probabilistic graphical model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy19WENNgfHb"
      },
      "source": [
        "The identification of named entity mentions in texts is often implemented using a sequence tagger, where each token is labeled with an BIO tag, indicating whether the token begins a named entity — (B-), whether it is inside of a named entity (I-), or outside of a named entity (O-). This type of annotation has been proposed for the first time at CoNLL-2003 dataset created for NER (Tjong Kim Sang and De Meulder, 2003). There are other tag notation types. For instance, each token can be predicted with a tag indicated by B-(begin), I-(inside), E-(end), S-(singleton) of a named entity with its type, or O-(outside) of named entities. But, I will stick to BIO format of representation for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6VoY4TeHPNa"
      },
      "source": [
        "Entity linking can be applied rigth after the NER task is performed althought in some papers on this topic there is proposal to do it in parallel (jointly) for each token, so that each subtask benefits from the partial output of the other subtask, and thus alleviate error propagations that are unavoidable in pipeline settings. \r\n",
        "Generally, EL is the task of mapping words from text (e.g. names of persons, locations and organizations) to entities from the target knowledge base. For this pupose I use a document containing most of the existing abbreviations used in legal documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNl8XkxiQ5b"
      },
      "source": [
        "## NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Nzf5h0KOdV"
      },
      "source": [
        "Usually, no matter the specific task, Deep Learning models creation is based on big data for training, validation and test. For Bulgarian language generally such data could be available if we start scraping web pages, which is huge amount of work. But this is only one side of the hidden obstacles - the effectiveness of the model created for the task is a real challenge on its own. \r\n",
        "\r\n",
        "Luckily, after the publication of the famous paper called \"Attention is all you need\" by Vaswani and the \"appearance\" of *Transformer*, there is a huge advancement in the model creation compared to previous usage of recurrence (RNN), Bidirectional Lont-Short Term Memory units (BiLSTM), convolutions (CNN) and CRF. Transformer utilizes stacked self-attention and pointwise, fully connected layers to build basic blocks for encoder and decoder.  Experiments on various tasks show Transformers to be superior in quality while requiring significantly less time to train.\r\n",
        "\r\n",
        "Based mostly on transformer, it already exists pre-trained models that provide results pretty close to humans on some general tasks. Some of the most used methods are ELMo(Embeddings from Language Models), OpenAI GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers)... \r\n",
        "\r\n",
        "It is important to underline that these state-of-the-art models use specific representation of the text, called embeddings, usually so called *hybrid representation* of text in low dimensional real-valued dense vectors. It is called *hybrid* as it uses *Word-level* and *Character-level* representation along with some additional features, where each dimensions represents a latent feature. This way it also captures the semantic and syntactic properties of words, but also the context for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlIsgqLVaxN"
      },
      "source": [
        "In recent years, the advancements of NLP in general and NER in particular has been greatly influenced by deep transfer learning methods capable of creating contextual representations of words, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created. Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context – sentences around the sentence of interest – to improve sequence tagging performance.\r\n",
        "\r\n",
        "Precisely for the fact of using this cross-sentence context, but also with the advantage to be pre-trained on Bulgarian texts, in this notebook, I focus on the recent BERT deep transfer learning models based on self-attention and the transformer architecture. BERT uses a fixed-size window that limits the amount of text that can be input to the model at one time. The model maximum window size, or maximum sequence length, is fixed during pre-training, with 512 wordpieces a common choice. This window fits dozens of typical sentences of input at a time, allowing the inclusion of extensive sentence context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wejQVdKrgiZT"
      },
      "source": [
        "There are many advantages that pushed me towards usage of BERT. To enumerate some, I would say that it provides:\r\n",
        "1. quicker development\r\n",
        "2. overcome the problem of missing data for training, which is generally the case for Bulgarian\r\n",
        "3. state-of-the-art better results - BERT is built on top of a number of clever ideas considered top in NLP community in latest years – including but not limited to Semi-supervised Sequence Learning (by Andrew Dai and Quoc Le), ELMo (by Matthew Peters and researchers from AI2 and UW CSE), ULMFiT (by fast.ai founder Jeremy Howard and Sebastian Ruder), the OpenAI transformer (by OpenAI researchers Radford, Narasimhan, Salimans, and Sutskever), and the Transformer (Vaswani et al).\r\n",
        "\r\n",
        "BERT is also one of the preferred model giving the best results used by Ilias Chalkidis et al. when dealing with  Large-Scale Multi-Label Text Classification (LMTC) in the legal domain (EU legislation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaxZDnSrGtZ0"
      },
      "source": [
        "On the other hand, there are some disadvantages, like:\r\n",
        "1. it is very large. The LARGE version of BERT would provide better results, but unfortunately that would require bigger computational ability and time.\r\n",
        "2. Even when using the BASE version, it remains slow for fine-tuning.\r\n",
        "3. The multilingual version that I need to use cannot be disitilled - the vocabulary used for fine-tuning of BERT must remain the original one.\r\n",
        "4. It uses a specific and a bit complicated jargon (domain-specific language), meaning that the tokenization with BERT should be done with BERT Tokenizer.\r\n",
        "\r\n",
        "The last two disadvantages represent in fact the specifity and maybe the strength of BERT. Its vocabulary is indeed fixed, but it has the capability to break down the unknown word into subwords and makes a token out of each subword (if subword exists in the vocabulary). In case the subword do not exist in the vocabulary it can continue spliting it into subwords down to a character level. To recognize the subword it prepends it with \"##\" flag, exceot for the first subword.\r\n",
        "\r\n",
        "On its turn the subword split would create a problem with labeling. Generally, in the test and train part each word is tagged. If an unknown word is splitted to subwords, a specific tag should be used for it, that would indicate that the tag valid for this word (the initial whole word) would be the one given to the first subword (original word) and a specific tag would be assigned to subwords after the first one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Z6ezlroorO"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wzBDQNrAdGS"
      },
      "source": [
        "Before getting to the problem of tags given to subwords, we need a dataset, big enough, that can be used to fine-tune our BERT model. This dataset should implement the following requirements:\r\n",
        "   - it must be created for a NER task;\r\n",
        "   - it must be in Bulgarian;\r\n",
        "   - it must contain special annotations (tags) for recognition of legal phrases;\r\n",
        "   - it must be big enough to train deep network model;\r\n",
        "   - ideally it should have a train, validation and test datasets.\r\n",
        "\r\n",
        "Well, the first four requirements need to be mandatory fulfilled. After all if the dataset is big enough there are ways and methods to make a consise split for train, validation and test datasets. \r\n",
        "\r\n",
        "But it is hard task to implement all four requirements. In fact, I was not able to find such dataset on Internet. Luckily, there is one dataset recently created for NER task, which was in Bulgarian - the dataset done by Iva Marinova et al. pesented in May 2020. The dataset is available at https://github.com/usmiva/bg-ner. With it, I could cover half of the requirements for my task. Unfortunately, as it was not created for utilization on legal texts, there were not a specific tag for legal phrases inside. Still, it was the best one I could find. Therefore, I decided to use it as a base, a starting point, and add to it the required information covering the legal part gathered by me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqEl2ZSDprp"
      },
      "source": [
        "But before start adding information, let me reveal what and how is implemented inside, in order to decide at what level it will suit me and how to add the missing information.\r\n",
        "\r\n",
        "The original Bulgarian corpus consists of 916 text files extracted from various news websites. The training dataset contains information on two topics – Brexit and the trial of the Pakistani Christian Asia Bibi, accused of blasphemy, while the main subjects for the test data are the Nord Stream 2 project and the recent developments in RyanAir’s business history.\r\n",
        "\r\n",
        "The type of annotation used inside followed the format used for the first time at CoNLL-2003 but used only the first and the last column (ommitting the part-of-speech tag and synctatic chunk tag) - meaning that the input files were segmented into sentences and tokens per line (first column), and each token was combined with its corresponding Named Entity tag (the second column). The NE tags were of type person (PER), organization (ORG), location (LOC), product (PRO), and event (EVT) and each of them had a prefix using the BIO format. Like in most NER tasks, NEs are considered to be non-recursive, non- overlapping, and whenever one NE is embedded in another NE, only the top-most entity is annotated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLkW9vyiDgi8"
      },
      "source": [
        "The 2 files available for download were 2 text files (.txt) - one train file with 220 700 lines and one test file with ~65 000 lines.\r\n",
        "\r\n",
        "Well, armed with this information, it was obvious that the missing part was for legal phrases, thus missing tag for tham. I decided that I could simply add a NE tag LAW. After a quick review there were only few word that could match this new tag in the existing dataset.\r\n",
        "\r\n",
        "Therefore, I added to the training file 117 documents taken from the \"Decision Register\" of [The Administrative Court of Sofia City (ASCS)](http://www.admincourtsofia.bg/Default.aspx?alias=www.admincourtsofia.bg/en) representing  the first 5 working days of year 2021 (from 4.01 to 8.01.2021). Each of this document was transformed to text, the sentences containing legal mentions were extracted and transformed in a file following the format of the original dataset using a simple Python script. The tag were than manually reviewed and annotated as correctly as possible not forgeting the initially available tags for person, organization etc. with the BIO prefixes.\r\n",
        "\r\n",
        "In that way the train document grew up to 347 642 lines. \r\n",
        "\r\n",
        "The original test documant were split in two - one for validation and one for test datasets. 40 documents  from the same source (court decisions published from 11.01.2021) were annotated and splitted the same way it was done for the train part. With that operation the validation file consisted of 56 880 lines and the test file of 56 908 lines. \r\n",
        "\r\n",
        "In that way the ration train vs. validation was assured to be at the reasonable 84% / 14% level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWe3li-eK799"
      },
      "source": [
        "### Selecting the model and loading it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYUn2cXa7AN5"
      },
      "source": [
        "First, I need to upload the BERT model. The choosen model by is:\r\n",
        "- the BERT Multilanguage version (in order to have a pre-trained model that has already seen Bulgarian language and Bulgarian words are part of it vocabulary);\r\n",
        "- the Cased version, meaning that whether the words contains capital letter or not matter to the model;\r\n",
        "- BASE version, as using the LARGE model would take too many ressources for training without such significal improvement in the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIbmx1xC8ZtW"
      },
      "source": [
        "When using BERT one must be aware that the tokenization with BERT should be done with BERT Tokenizer. There are also other mandatory requirement for the fine-tunning of BERT - used vocabulary shoulb be exactly the original of the pre-trained model.\r\n",
        "\r\n",
        "There are two ways to upload the model.\r\n",
        "\r\n",
        "The first one is by loading the model from TensoFlow Hub:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JEjEGAu0ES",
        "outputId": "851304c1-7506-4593-db3e-4d0031742b04"
      },
      "source": [
        "bert_model_name = 'bert_multi_cased_L-12_H-768_A-12' \r\n",
        "map_name_to_handle = {'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'}\r\n",
        "\r\n",
        "map_model_to_preprocess = {'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2',}\r\n",
        "\r\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\r\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\r\n",
        "\r\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\r\n",
        "print(f'Preprocessing model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3YV-TZVv12B",
        "outputId": "6e30dd08-11b9-44d7-8825-323d9744151d"
      },
      "source": [
        "bert_preprocess = hub.load(tfhub_handle_preprocess)\r\n",
        "tok = bert_preprocess.tokenize(tf.constant(['Hello TensorFlow!']))\r\n",
        "print(tok)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[[31178], [16411, 28919, 11565, 27863], [106]]]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdgt-TIN9eDV"
      },
      "source": [
        "Second possibility is to download the required version to a directory from where it can be directly loaded. It can be done with 2 line of code: \r\n",
        "`model_name = \"multi_cased_L-12_H-768_A-12\"`\r\n",
        "\r\n",
        "`model_dir = bert.fetch_google_bert_model(model_name, gs_folder_bert)`\r\n",
        "\r\n",
        "I did that in my Google Disk drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EjrcRxhryqy",
        "outputId": "f152f322-d7ee-4272-c54d-4b0fac5f6ea7"
      },
      "source": [
        "gs_folder_bert = '/content/drive/MyDrive/Colab Notebooks/bert_model/'\r\n",
        "\r\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.ipynb_checkpoints',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_config.json',\n",
              " 'bert_model.ckpt.index',\n",
              " 'bert_model.ckpt.meta',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IGZmswUrlNG",
        "outputId": "b608c243-d590-4721-f606-4850c8991a54"
      },
      "source": [
        "# Set up tokenizer to generate Tensorflow dataset\r\n",
        "tokenizer = bert_tokenization.FullTokenizer(\r\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\r\n",
        "     do_lower_case=False)\r\n",
        "\r\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 119547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1m8OeaD-uCx"
      },
      "source": [
        "We can use the same example as above on a simple sentence that is not transformed to a list to verify that the upper case matters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOIr3_1n-tm_",
        "outputId": "60ad9f50-3136-41d1-9867-dc0afc4c3853"
      },
      "source": [
        "tokens = tokenizer.tokenize(\"Hello TensorFlow!\")\r\n",
        "print(tokens)\r\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "print(ids)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'Ten', '##sor', '##F', '##low', '!']\n",
            "[31178, 16411, 28919, 11565, 27863, 106]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQYKlNarASK5"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_iment_7NcZ"
      },
      "source": [
        "As \"prescribed\" on the official [GitHub page of BERT](https://github.com/google-research/bert) in order to avoid any out-of-memory issues when using BERT model `MAX_SEQ_LENGTH` should be up to 512. Having in mind the numerous splits of the specific `tokenizer` of BERT and the maximum tokens length found during the creation of the dataset, I fix the maximum sequence length to `256`. In this case the benchmark of the `BATCH_SIZE` found in teh same place is `16`.\r\n",
        "\r\n",
        "For the initial learning rate (`LEARNING_RATE`), in line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5). I'll use `5e-5`. During the BERT pre-training, the learning rate is a linear warm-up phase over the first `10% of training steps`, meaning the the `WARM-UP PROPOTION` should be set the same. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wez32z_oIU4-"
      },
      "source": [
        "EPOCHS = 3  # Total number of training epochs to perform\r\n",
        "MAX_SEQ_LENGTH = 256  # the length of the biggest sentence\r\n",
        "BATCH_SIZE = 16  # Total batch size for training.\r\n",
        "LEARNING_RATE = 5e-5  # The initial learning rate for Adam.\r\n",
        "WARM_UP_PROPORTION = 0.1  # Proportion of training to perform linear learning rate warmup for. e.g., 0.1 = 10% of training.\r\n",
        "WEIGHT_DECAY = 0.01  # Weight decay if we apply some\r\n",
        "ADAM_EPSILON = 0.01  # Epsilon for Adam optimizer\r\n",
        "SEED = seed(42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYRJWwDf_XSD"
      },
      "source": [
        "### Reading the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTlgRkwlLEog"
      },
      "source": [
        "After loading the model, it is time to read the documents in order to prepare the train, validation and test datasets. For that I need to create a function that will read the data sentence by sentence, that will be transformed afterwords to features with another function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIiGhmobLjeD"
      },
      "source": [
        "def read_data(filename):\r\n",
        "  \"\"\"\"reading the file and returning a list of attributes and a list of corresponding labels\"\"\"\r\n",
        "  data, sentence, label = [], [], []\r\n",
        "  num_sentense, max_sentence_length = 0, 0\r\n",
        "\r\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\r\n",
        "    for line in f:\r\n",
        "      if len(line) == 0 or line[0] == '\\n':\r\n",
        "        if len(sentence) > 0:\r\n",
        "          data.append((sentence, label))\r\n",
        "          if len(sentence) > max_sentence_length:\r\n",
        "            max_sentence_length = len(sentence)\r\n",
        "          sentence, label = [], []\r\n",
        "          num_sentense += 1\r\n",
        "        continue\r\n",
        "      word, lab = line.rstrip('\\n').split('\\t')\r\n",
        "      sentence.append(word)\r\n",
        "      label.append(lab)\r\n",
        "\r\n",
        "    if len(sentence) > 0:\r\n",
        "      data.append((sentence, label))\r\n",
        "      num_sentense += 1\r\n",
        "      if len(sentence) > max_sentence_length:\r\n",
        "            max_sentence_length = len(sentence)\r\n",
        "\r\n",
        "    print(\"Number of sentences:\", num_sentense)\r\n",
        "    print(\"Maximum token lenght of a sentence:\", max_sentence_length)\r\n",
        "  return data  # [tuple(attributes, labels)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QzJIUSeQL4o",
        "outputId": "35f7b2fd-d015-416c-9c9f-751c64a4f1f3"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/data/'\r\n",
        "\r\n",
        "val_file = os.path.join(path, 'val_NER_BG.txt')\r\n",
        "\r\n",
        "val_readed_data = read_data(val_file)\r\n",
        "\r\n",
        "print(val_readed_data[:5])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 1695\n",
            "Maximum token lenght of a sentence: 240\n",
            "[(['Газопроводът', 'Северен', 'поток', '2', ',', 'който', 'по', 'план', 'ще', 'пренася', 'ежегодно', '55', 'милиарда', 'кубични', 'метра', 'природен', 'газ', 'от', 'Русия', 'към', 'ЕС', 'през', 'Балтийско', 'море', ',', 'вече', 'бе', 'одобрен', 'от', 'Германия', 'и', 'Финландия', '.'], ['O', 'B-PRO', 'I-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O']), (['САЩ', ',', 'в', 'отговор', 'заявиха', ',', 'че', 'тръбопроводът', 'ще', 'повиши', 'зависимостта', 'на', 'Европа', 'от', 'руския', 'газ', '.'], ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']), (['Списание', '\"', 'Foreign', 'policy', '\"', 'цитира', 'три', 'източника', 'близки', 'до', 'въпроса', ',', 'които', 'твърдят', 'че', 'администрацията', 'на', 'САЩ', 'е', 'близо', 'до', 'налагането', 'на', 'санкции', 'върху', 'енергийни', 'компании', 'от', 'Германия', 'и', 'други', 'държави', 'от', 'ЕС', ',', 'които', 'са', 'замесени', 'в', 'изграждането', 'на', 'проекта', 'за', 'руски', 'газов', 'тръбопровод', 'Северен', 'поток', '2', '.'], ['O', 'O', 'B-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'I-PRO', 'O']), (['Ключови', 'фигури', 'в', 'администрацията', 'на', 'президента', 'Доналд', 'Тръмп', ',', 'които', 'виждат', 'санкциите', 'като', '\"', 'много', 'вероятна', 'опция', '\"', ',', 'няма', 'да', 'се', 'спрат', 'пред', 'нищо', ',', 'за_да', 'блокират', 'Северен', 'поток', ',', 'заявява', 'един', 'от', 'източниците', 'на', 'списанието', '.'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRO', 'I-PRO', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']), (['Източник', 'от', 'Държавния', 'департамент', 'е', 'заявил', ',', 'че', '\"', 'бяхме', 'ясни', ',', 'че', 'компании', 'работещи', 'в', 'руския', 'енергиен', 'сектор', 'и', 'сектора', 'за', 'износ', 'на', 'тръбопроводи', 'навлизат', 'в', 'бизнес', 'посока', ',', 'която', 'носи', 'опасност', 'от', 'санкции', '\"', '.'], ['O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TkDSJL-ry7k"
      },
      "source": [
        "#####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQYwbtNIpAZp"
      },
      "source": [
        "All these sentences need to be transformed in features that BERT understands using the BERT tokenization.\r\n",
        "\r\n",
        "By tokenizing a sentence we in fact encode the sentence. There is one special requirement for that tokenization - 2 additional labels should be added to the list of words representing each sentence: `['CLS']` and `['SEP']`. These 2 additional labels are required by BERT. `['CLS']` indicates that we will talk about \"classification problem\", so `['CLS']` token will be put at the beginning of each phrase, and each sentence and its corresponding label list should end with a `['SEP']` - \"separator\" token. Their ids a respectively `[101]` and `[102]`\r\n",
        ".\r\n",
        "\r\n",
        "The feature required by BERT, apart from the ids of the tokenized sentence and its respective labels using the same index mapping everywhere defined in advance), is the \"input masks\" which allows the model to cleanly differentiate between content and padding. Similar masking is needed for the labels.\r\n",
        "\r\n",
        "The most specific part that I will be using here is the creation and consequently usage of the variable ``valid_ids`. This variable will be a list which will be responsible for the replication of the logic of subwords to the `label_id` list, by marking only the first subword's label as valid, so that the label becomes \"responsible\" for the whole word.\r\n",
        "\r\n",
        "There are different ways to provide all features to the model. Most examples on this topic use dictionnaries, but the this a simple class will be more easy to deal with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ5Yflqymacj"
      },
      "source": [
        "class InputFeatures:\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, ntokens, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\r\n",
        "      self.ntokens = ntokens  # only for representatinal purpose\r\n",
        "      self.input_ids = input_ids  # encoded words\r\n",
        "      self.input_mask = input_mask  # mask indicating padding or not\r\n",
        "      self.segment_ids = segment_ids  # we talk about NER task, so the segment_ids will be just '0's\r\n",
        "      self.label_id = label_id  # encoded label\r\n",
        "      self.valid_ids = valid_ids  # when the word is split to subwords, it indicates only the first id as valid '1', next subwords as '0'\r\n",
        "      self.label_mask = label_mask  # mask indicating padding or not\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvpzX9nmRlD"
      },
      "source": [
        "def convert_examples_to_features(examples, label_map, max_seq_length, tokenizer):\r\n",
        "    \"\"\"Loads a data file into a list of `InputFeatures`s.\"\"\"\r\n",
        "\r\n",
        "    features = []  # the list of `InputFeatures` to be returned\r\n",
        "    for (ex_index, example) in enumerate(examples):\r\n",
        "        textlist = example[0]\r\n",
        "        labellist = example[1]\r\n",
        "        tokens = []\r\n",
        "        labels = []\r\n",
        "        valid_ids = []\r\n",
        "        label_mask = []\r\n",
        "        for i, word in enumerate(textlist):\r\n",
        "            token = tokenizer.tokenize(word)\r\n",
        "            tokens.extend(token)\r\n",
        "            label_1 = labellist[i]\r\n",
        "            for m in range(len(token)):\r\n",
        "                if m == 0:\r\n",
        "                    labels.append(label_1)\r\n",
        "                    valid_ids.append(1)\r\n",
        "                    label_mask.append(True)\r\n",
        "                else:\r\n",
        "                    valid_ids.append(0)\r\n",
        "        \r\n",
        "        # checking if a sentence is longer than max_seq_length, if yes -> cut it\r\n",
        "        if len(tokens) >= max_seq_length - 1:\r\n",
        "            tokens = tokens[0:(max_seq_length - 2)]\r\n",
        "            labels = labels[0:(max_seq_length - 2)]\r\n",
        "            valid_ids = valid_ids[0:(max_seq_length - 2)]\r\n",
        "            label_mask = label_mask[0:(max_seq_length - 2)]\r\n",
        "\r\n",
        "        # init\r\n",
        "        ntokens = []\r\n",
        "        segment_ids = []\r\n",
        "        label_ids = []\r\n",
        "\r\n",
        "        # adding the mandatory ['CLS'] at the beginning\r\n",
        "        ntokens.append(\"[CLS]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        valid_ids.insert(0, 1)\r\n",
        "        label_mask.insert(0, True)\r\n",
        "        label_ids.append(label_map[\"[CLS]\"])\r\n",
        "        for i, token in enumerate(tokens):\r\n",
        "            ntokens.append(token)\r\n",
        "            segment_ids.append(0)\r\n",
        "            if len(labels) > i:\r\n",
        "                label_ids.append(label_map[labels[i]])\r\n",
        "\r\n",
        "        # adding the mandatory ['SEP'] at the end of each sentence\r\n",
        "        ntokens.append(\"[SEP]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        valid_ids.append(1)\r\n",
        "        label_mask.append(True)\r\n",
        "        label_ids.append(label_map[\"[SEP]\"])\r\n",
        "\r\n",
        "        # transforming `ntokens` to BERT's tokenizer ids\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "        label_mask = [True] * len(label_ids)\r\n",
        "\r\n",
        "        # padding\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "            label_ids.append(0)\r\n",
        "            valid_ids.append(1)\r\n",
        "            label_mask.append(False)\r\n",
        "        while len(label_ids) < max_seq_length:\r\n",
        "            label_ids.append(0)\r\n",
        "            label_mask.append(False)\r\n",
        "\r\n",
        "        # last check - all lengths should correspond to max_seq_length\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "        assert len(label_ids) == max_seq_length\r\n",
        "        assert len(valid_ids) == max_seq_length\r\n",
        "        assert len(label_mask) == max_seq_length\r\n",
        "\r\n",
        "        # adding to the list\r\n",
        "        features.append(\r\n",
        "            InputFeatures(ntokens=ntokens,\r\n",
        "                          input_ids=input_ids,\r\n",
        "                          input_mask=input_mask,\r\n",
        "                          segment_ids=segment_ids,\r\n",
        "                          label_id=label_ids,\r\n",
        "                          valid_ids=valid_ids,\r\n",
        "                          label_mask=label_mask))\r\n",
        "    return features"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVqo7l9lrNAA"
      },
      "source": [
        "For using this function, I will need to create the list of the available labels including the required by BERT `['CLS']` and `['SEP']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1vPizjArq0k"
      },
      "source": [
        "label_list = [\"O\", \"B-LAW\", \"I-LAW\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-PRO\", \"I-PRO\", \"B-EVT\", \"I-EVT\", \"[CLS]\", \"[SEP]\"]\r\n",
        "\r\n",
        "label_map = {label: i for i, label in enumerate(label_list, 1)}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeX1MoH5sT2_"
      },
      "source": [
        "val_features = convert_examples_to_features(examples=val_readed_data, label_map=label_map, max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MkxTDgvGMqw"
      },
      "source": [
        "Here is an example of the outcome of this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVullaray4sI",
        "outputId": "954e3511-fed6-4e86-e1c8-f6410322e9cf"
      },
      "source": [
        "print(val_features[0])\r\n",
        "print(\"ntokens =\", val_features[0].ntokens)\r\n",
        "print(\"input_ids =\", val_features[0].input_ids)\r\n",
        "print(\"input_mask =\", val_features[0].input_mask)\r\n",
        "print(\"segment_ids =\", val_features[0].segment_ids)\r\n",
        "print(\"label_id =\", val_features[0].label_id)\r\n",
        "print(\"valid_ids =\", val_features[0].valid_ids)\r\n",
        "print(\"label_mask =\", val_features[0].label_mask)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.InputFeatures object at 0x7ffa560fac50>\n",
            "ntokens = ['[CLS]', 'Г', '##аз', '##оп', '##рово', '##д', '##ът', 'Се', '##вер', '##ен', 'по', '##ток', '2', ',', 'който', 'по', 'план', 'ще', 'пре', '##нася', 'ежегодно', '55', 'ми', '##ли', '##арда', 'к', '##уб', '##ични', 'метра', 'природе', '##н', 'газ', 'от', 'Русия', 'към', 'ЕС', 'през', 'Ба', '##лт', '##ий', '##ско', 'море', ',', 'вече', 'б', '##е', 'од', '##об', '##рен', 'от', 'Германия', 'и', 'Ф', '##ин', '##ландия', '.', '[SEP]']\n",
            "input_ids = [101, 512, 26313, 58056, 55048, 10746, 13368, 52203, 32418, 10928, 10297, 20422, 123, 117, 16362, 10297, 35718, 16892, 38494, 87280, 84167, 11358, 37140, 10783, 72123, 551, 40124, 53928, 41921, 93710, 10267, 44352, 10332, 13014, 15977, 109795, 12112, 101086, 33262, 11550, 13566, 27165, 117, 45721, 542, 10205, 10430, 33276, 27332, 10332, 20823, 549, 529, 12029, 68103, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "input_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "segment_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "label_id = [14, 1, 10, 11, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 6, 1, 8, 9, 1, 1, 1, 1, 1, 8, 1, 8, 1, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "valid_ids = [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_mask = [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wOtyK-8Oo6d"
      },
      "source": [
        "As we have both for each sentence and its respective lables the required ids and masks, in order ot be proceeded by BERT, it should be trnasformed to acceptable tensors. For that I will use the function `tf.data.Dataset.from_tensor_slices()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XvCYjc3RJ4x"
      },
      "source": [
        "def transform_to_dataset(features, examples, seed, batch_size, training_dataset=False):\r\n",
        "\r\n",
        "  all_input_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.input_ids for f in features]))\r\n",
        "  all_input_mask = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.input_mask for f in features]))\r\n",
        "  all_segment_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.segment_ids for f in features]))\r\n",
        "  all_valid_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.valid_ids for f in features]))\r\n",
        "  all_label_mask = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.label_mask for f in features]))\r\n",
        "  all_label_ids = tf.data.Dataset.from_tensor_slices(\r\n",
        "      np.asarray([f.label_id for f in features]))\r\n",
        "\r\n",
        "  # Dataset using tf.data\r\n",
        "  data = tf.data.Dataset.zip(\r\n",
        "      (all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids, all_label_mask))\r\n",
        "  \r\n",
        "  if training_dataset:\r\n",
        "      shuffled_data = data.shuffle(buffer_size=int(len(features) * 0.1),\r\n",
        "                                  seed=seed, reshuffle_each_iteration=True)\r\n",
        "      batched_data = shuffled_data.batch(batch_size)\r\n",
        "  else:\r\n",
        "      batched_data = data.batch(batch_size)\r\n",
        "\r\n",
        "  return data, batched_data\r\n",
        "\r\n",
        "  # inputs = {\r\n",
        "  #     'input_word_ids': all_input_ids.to_tensor(),\r\n",
        "  #     'input_mask': all_input_mask.to_tensor(),\r\n",
        "  #     'input_type_ids': all_segment_ids.to_tensor(),\r\n",
        "  #     # '':,\r\n",
        "  #     }\r\n",
        "\r\n",
        "  # return inputs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuziyKhTiRa"
      },
      "source": [
        "validation, batched_val_data = transform_to_dataset(features=val_features, examples=readed_data, seed=SEED, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMdQ3JB3XPWa",
        "outputId": "a06ea925-9396-40d2-871d-d9432a82d3ea"
      },
      "source": [
        "print(validation)\r\n",
        "print(batched_val_data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ZipDataset shapes: ((256,), (256,), (256,), (256,), (256,), (256,)), types: (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.bool)>\n",
            "<BatchDataset shapes: ((None, 256), (None, 256), (None, 256), (None, 256), (None, 256), (None, 256)), types: (tf.int64, tf.int64, tf.int64, tf.int64, tf.int64, tf.bool)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJdvTsQHSeM"
      },
      "source": [
        "I used the `validation` dataset preparation as example, but I'll need to do the same for the `train` and `test` datasets. But, we can do it again with a single function that includes all these required transformative functions in it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abpKccLKJIQs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYVEfkCqI6rL"
      },
      "source": [
        "def data_preprocess(dataset_type, path, label_map, max_seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer, batch_size=BATCH_SIZE, seed=SEED):\r\n",
        "  \"\"\"\r\n",
        "  function englobelling the customized bert pre-process of data for a NER task\r\n",
        "  params: dataset_type: string with possible dataset_types: 'train', 'val', 'test'\r\n",
        "          path: the directory where the datasets are situated\r\n",
        "          label_map: desired mapping of all the available labels + including '[SEP]' and '[CLS]'\r\n",
        "  \r\n",
        "  returns: tuple of zipped dataset and batched dataset\r\n",
        "  \"\"\"\r\n",
        "  filename = f\"{dataset_type}_NER_BG.txt\" \r\n",
        "  \r\n",
        "  file = os.path.join(path, filename)\r\n",
        "\r\n",
        "  readed_data = read_data(file)\r\n",
        "\r\n",
        "  features = convert_examples_to_features(examples=readed_data, label_map=label_map, max_seq_length=max_seq_length, tokenizer=tokenizer)\r\n",
        "\r\n",
        "  training_dataset = True if dataset_type == 'train' else False\r\n",
        "\r\n",
        "  return transform_to_dataset(features=features, examples=readed_data, seed=seed, batch_size=batch_size, training_dataset=training_dataset)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "8KTHtA2uHf9w",
        "outputId": "d71595f5-cede-429d-bc43-b7e0a03fb31b"
      },
      "source": [
        "# training dataset\r\n",
        "train_data, batched_train_data = data_preprocess(dataset_type='train', path=path, label_map=label_map)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-03f9d0513bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-5559ac8eb24c>\u001b[0m in \u001b[0;36mdata_preprocess\u001b[0;34m(dataset_type, path, label_map, max_seq_length, tokenizer, batch_size, seed)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mreaded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreaded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-689ee6be2514>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mnum_sentense\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJkRQQu9_npF"
      },
      "source": [
        "### Building the BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iGSOfS5_yZD"
      },
      "source": [
        "After the model is already selected, the configuration of the pretrained model should be downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtbZSUnIAECB",
        "outputId": "f7d7a459-467b-4927-a414-054d992d21ef"
      },
      "source": [
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\r\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\r\n",
        "\r\n",
        "# bert_config = bert.configs.BertConfig.from_dict(config_dict)\r\n",
        "\r\n",
        "bert_params = bert.params_from_pretrained_ckpt(gs_folder_bert)\r\n",
        "print(\"config_dict:\")\r\n",
        "pprint(config_dict)\r\n",
        "print(\"bert_params:\")\r\n",
        "pprint(bert_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config_dict:\n",
            "{'attention_probs_dropout_prob': 0.1,\n",
            " 'directionality': 'bidi',\n",
            " 'hidden_act': 'gelu',\n",
            " 'hidden_dropout_prob': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_size': 3072,\n",
            " 'max_position_embeddings': 512,\n",
            " 'num_attention_heads': 12,\n",
            " 'num_hidden_layers': 12,\n",
            " 'pooler_fc_size': 768,\n",
            " 'pooler_num_attention_heads': 12,\n",
            " 'pooler_num_fc_layers': 3,\n",
            " 'pooler_size_per_head': 128,\n",
            " 'pooler_type': 'first_token_transform',\n",
            " 'type_vocab_size': 2,\n",
            " 'vocab_size': 119547}\n",
            "bert_params:\n",
            "{'adapter_activation': 'gelu',\n",
            " 'adapter_init_scale': 0.001,\n",
            " 'adapter_size': None,\n",
            " 'attention_dropout': 0.1,\n",
            " 'embedding_size': None,\n",
            " 'extra_tokens_vocab_size': None,\n",
            " 'hidden_dropout': 0.1,\n",
            " 'hidden_size': 768,\n",
            " 'initializer_range': 0.02,\n",
            " 'intermediate_activation': 'gelu',\n",
            " 'intermediate_size': 3072,\n",
            " 'key_activation': None,\n",
            " 'mask_zero': False,\n",
            " 'max_position_embeddings': 512,\n",
            " 'negative_infinity': -10000.0,\n",
            " 'num_heads': 12,\n",
            " 'num_layers': 12,\n",
            " 'out_layer_ndxs': None,\n",
            " 'project_embeddings_with_bias': True,\n",
            " 'project_position_embeddings': True,\n",
            " 'query_activation': None,\n",
            " 'shared_layer': False,\n",
            " 'size_per_head': None,\n",
            " 'token_type_vocab_size': 2,\n",
            " 'use_position_embeddings': True,\n",
            " 'use_token_type': True,\n",
            " 'value_activation': None,\n",
            " 'vocab_size': 119547}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3bVmsK1KS33"
      },
      "source": [
        "l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0xgMw3JPozb"
      },
      "source": [
        "model_ckpt = os.path.join(gs_folder_bert, \"bert_model.ckpt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLIEi5PhVpSK"
      },
      "source": [
        "OUT_UNITS = len(label_list) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0KHLdauULQY"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.InputLayer(input_shape=(MAX_SEQ_LENGTH,)),\r\n",
        "  l_bert,\r\n",
        "  tf.keras.layers.Dense(units=OUT_UNITS, activation=\"softmax\")\r\n",
        "])\r\n",
        "model.build(input_shape=(None, MAX_SEQ_LENGTH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMggil-bCyi8"
      },
      "source": [
        "class BertNer(tf.keras.Model):\r\n",
        "\r\n",
        "    def __init__(self, bert_model,float_type, num_labels, max_seq_length, final_layer_initializer=None):\r\n",
        "        '''\r\n",
        "        bert_model : bert_model\r\n",
        "        float_type : tf.float32\r\n",
        "        num_labels : num of tags in NER task\r\n",
        "        max_seq_length : max_seq_length of tokens\r\n",
        "        final_layer_initializer : default:  tf.keras.initializers.TruncatedNormal\r\n",
        "        '''\r\n",
        "        super(BertNer, self).__init__()\r\n",
        "        \r\n",
        "        # defining the Input of the BERT layer\r\n",
        "        input_word_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int64, name='input_word_ids')\r\n",
        "        input_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int64, name='input_mask')\r\n",
        "        input_type_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int64, name='input_type_ids')\r\n",
        "\r\n",
        "        # defining the Outputs of the BERT model\r\n",
        "        sequence_output = bert_model(input_word_ids, input_mask,input_type_ids)\r\n",
        "\r\n",
        "        # BERT model\r\n",
        "        self.bert = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids],outputs=[sequence_output])\r\n",
        "        \r\n",
        "        if final_layer_initializer is not None:\r\n",
        "            initializer = final_layer_initializer\r\n",
        "        else:\r\n",
        "            initializer = tf.keras.initializers.TruncatedNormal(stddev=bert_model.params['initializer_range'])\r\n",
        "\r\n",
        "        # adding default DROPOUT Layer\r\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=bert_model.params['hidden_dropout'])\r\n",
        "        \r\n",
        "        # defining the output Layer\r\n",
        "        self.classifier = tf.keras.layers.Dense(\r\n",
        "            num_labels, kernel_initializer=initializer, activation='softmax',name='output', dtype=float_type)\r\n",
        "    \r\n",
        "\r\n",
        "    def call(self, input_word_ids,input_mask=None,input_type_ids=None,valid_mask=None, **kwargs):\r\n",
        "        sequence_output = self.bert([input_word_ids, input_mask, input_type_ids],**kwargs)\r\n",
        "        valid_output = []\r\n",
        "        for i in range(sequence_output.shape[0]):\r\n",
        "            r = 0\r\n",
        "            temp = []\r\n",
        "            for j in range(sequence_output.shape[1]):\r\n",
        "                if valid_mask[i][j] == 1:\r\n",
        "                    temp = temp + [sequence_output[i][j]]\r\n",
        "                else:\r\n",
        "                    r += 1\r\n",
        "            temp = temp + r * [tf.zeros_like(sequence_output[i][j])]\r\n",
        "            valid_output = valid_output + temp\r\n",
        "        valid_output = tf.reshape(tf.stack(valid_output),sequence_output.shape)\r\n",
        "        sequence_output = self.dropout(\r\n",
        "            valid_output, training=kwargs.get('training', False))\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZhzjzz8F_9T"
      },
      "source": [
        "ner = BertNer(l_bert, tf.float32, OUT_UNITS, MAX_SEQ_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8EkmS14YjyD",
        "outputId": "b9a4f600-b2e0-40f8-e271-6fa5b3bf9f63"
      },
      "source": [
        "ner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BertNer at 0x7fdf9eea94e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "qQETWafw815B",
        "outputId": "a17c687e-082a-4d17-d39d-5adab8bd2604"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-18d3168f2336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0mrankdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrankdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m       \u001b[0mexpand_nested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_nested\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m       dpi=dpi)\n\u001b[0m\u001b[1;32m    332\u001b[0m   \u001b[0mto_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0msub_w_last_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m   \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertModelLayer' object has no attribute 'layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipk897WZwBY_"
      },
      "source": [
        "def flatten_layers(root_layer):\r\n",
        "    if isinstance(root_layer, tf.keras.layers.Layer):\r\n",
        "        yield root_layer\r\n",
        "    for layer in root_layer._layers:\r\n",
        "        for sub_layer in flatten_layers(layer):\r\n",
        "            yield sub_layer\r\n",
        "\r\n",
        "def freeze_bert_layers(l_bert):\r\n",
        "    \"\"\"\r\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\r\n",
        "    \"\"\"\r\n",
        "    for layer in flatten_layers(l_bert):\r\n",
        "      print(layer)\r\n",
        "      print(layer.name)\r\n",
        "      print(layer._layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3uEBei7w0eR",
        "outputId": "1e73d76c-3edc-423f-f152-f7350bae562b"
      },
      "source": [
        "freeze_bert_layers(l_bert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bert.model.BertModelLayer object at 0x7fdc270e9eb8>\n",
            "bert\n",
            "[<bert.embeddings.BertEmbeddingsLayer object at 0x7fdc483c1588>, <bert.transformer.TransformerEncoderLayer object at 0x7fdc26ffc208>]\n",
            "<bert.embeddings.BertEmbeddingsLayer object at 0x7fdc483c1588>\n",
            "embeddings\n",
            "[]\n",
            "<bert.transformer.TransformerEncoderLayer object at 0x7fdc26ffc208>\n",
            "encoder\n",
            "[ListWrapper([])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6IxgfR7INZo"
      },
      "source": [
        "#####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTFie17CUvdG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3xorHTuUx5l"
      },
      "source": [
        "### Set up the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdXW6hI4lNP"
      },
      "source": [
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). As described in the prodecure of [Fine-Tuning Bert in Tensorflow Tutorials](https://www.tensorflow.org/official_models/fine_tuning_bert#set_up_the_optimizer): \"BERT adopts the Adam optimizer with weight decay (aka \"AdamW\"). It also employs a learning rate schedule that firstly warms up from 0 and then decays to 0.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhTFM4hEVXd9",
        "outputId": "f9521d27-8baa-40c6-a6d5-f6b7ba3a7bb7"
      },
      "source": [
        "# set_up steps\r\n",
        "\r\n",
        "train_data_size = len(val_features)\r\n",
        "\r\n",
        "steps_per_epoch = int(train_data_size / BATCH_SIZE)\r\n",
        "\r\n",
        "num_train_steps = steps_per_epoch * EPOCHS\r\n",
        "\r\n",
        "warmup_steps = int(WARM_UP_PROPORTION * num_train_steps)\r\n",
        "\r\n",
        "print(warmup_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1Cryrra5au"
      },
      "source": [
        "It is commonly observed that a monotonically decreasing learning rate, whose degree of change is carefully chosen, results in a better performing model (source [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PolynomialDecay)). The PolynomialDecay schedule applies a polynomial decay function to an optimizer step, given a provided initial_learning_rate, to reach an end_learning_rate in the given decay_steps. \r\n",
        "\r\n",
        "The base learning rate schedule used here is a linear decay to zero over the training run, visible also on the graph below :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "C2vbNw8PaFlM",
        "outputId": "a7f378c3-f71e-4475-a3b0-ff7a0a271cca"
      },
      "source": [
        "decay_schedule  = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=LEARNING_RATE,\r\n",
        "                                                                 decay_steps=num_train_steps,\r\n",
        "                                                                 end_learning_rate=0.0)\r\n",
        "\r\n",
        "plt.plot([decay_schedule(n) for n in range(num_train_steps)])\r\n",
        "plt.xlabel('number of training steps')\r\n",
        "plt.ylabel('learning rate decay schedule')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDAQNTfdpbj3"
      },
      "source": [
        "Using `nlp.optimization` `WarmUp` class, the definition of the warm-up schedule is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "bKLedlUkIN0X",
        "outputId": "5e3df3f7-a141-4df9-ce6a-0170945dd4e6"
      },
      "source": [
        "warmup_schedule = WarmUp(initial_learning_rate=decay_schedule(warmup_steps),\r\n",
        "                         decay_schedule_fn=decay_schedule,\r\n",
        "                         warmup_steps=warmup_steps)\r\n",
        "  \r\n",
        "plt.plot([warmup_schedule(n) for n in range(num_train_steps)])\r\n",
        "plt.xlabel('number of training steps')\r\n",
        "plt.ylabel('learning rate with warm-up and decay schedule')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGxrmrY_pw8a"
      },
      "source": [
        "It warms up to the `initial_learning_rate` following the learning rate level at the moment the decay schedule calculates it depending on the number of training steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaUPJZjLnClB"
      },
      "source": [
        "optimizer = AdamWeightDecay(\r\n",
        "            learning_rate=warmup_schedule,\r\n",
        "            weight_decay_rate=WEIGHT_DECAY,\r\n",
        "            epsilon=ADAM_EPSILON,\r\n",
        "            exclude_from_weight_decay=['LayerNorm', 'layer_norm', 'bias'])\r\n",
        "# default values: beta_1=0.9, beta_2=0.999,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J3vnfOBnGt3",
        "outputId": "5b0e624a-a053-48ca-acd0-083cafc63ebd"
      },
      "source": [
        "type(optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "official.nlp.optimization.AdamWeightDecay"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dbwhrdzl7bR"
      },
      "source": [
        "### Select loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwePPs-RmBwD"
      },
      "source": [
        "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Ww8ekymFzT"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\r\n",
        "\r\n",
        "loss_metric = F1Score(len(label_list))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUAqR5XpJo_u"
      },
      "source": [
        "def dice_loss(y_true, y_predicted):\r\n",
        "\r\n",
        "    y_true_f = tf.layers.flatten(y_true)\r\n",
        "    y_pred_f = tf.layers.flatten(y_predicted)\r\n",
        "\r\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\r\n",
        "\r\n",
        "    if loss_type == 'jaccard':\r\n",
        "        union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))\r\n",
        "\r\n",
        "    elif loss_type == 'sorensen':\r\n",
        "        union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\r\n",
        "    \r\n",
        "    return (2. * intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())\r\n",
        "\r\n",
        "    # num_sum = 2.0 * tf.reduce_sum(y_true * y_predicted) + tf.keras.backend.epsilon()\r\n",
        "    # den_sum = tf.reduce_sum(y_true) + tf.reduce_sum(y_predicted) + tf.keras.backend.epsilon()\r\n",
        "    # # den_sum = tf.reduce_sum(tf.square(y_predicted)) + tf.reduce_sum(tf.square(y_true)) + tf.keras.backend.epsilon()\r\n",
        "\r\n",
        "    # return np.ndarray([1 - num_sum/den_sum], dtype = 'float32')\r\n",
        "\r\n",
        "loss = dice_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaevRo3dXex9"
      },
      "source": [
        " def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids, label_mask, loss_fct=loss):\r\n",
        "\r\n",
        "            with tf.GradientTape() as tape:\r\n",
        "                logits = ner(input_ids, input_mask, segment_ids, valid_ids,\r\n",
        "                             training=True)  # batchsize, max_seq_length, num_labels\r\n",
        "                label_ids_masked = tf.boolean_mask(label_ids, label_mask)\r\n",
        "                logits_masked = tf.boolean_mask(logits, label_mask)\r\n",
        "                scce_loss = loss_fct(label_ids_masked, logits_masked)\r\n",
        "\r\n",
        "            gradients = tape.gradient(scce_loss, ner.trainable_variables)\r\n",
        "            optimizer.apply_gradients(list(zip(gradients, ner.trainable_variables)))\r\n",
        "            return scce_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90VJKgLKiXdq"
      },
      "source": [
        "## Entity Linking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m11CTxXgibRt"
      },
      "source": [
        "The Entity Linking (EL) process transforms amiguous textual mention to a unique identifier by looking at the context in which the mention occurs. Thus it can be looked as 2 step process after the NER:\r\n",
        "1. Creation of Entity Linker - list of candidates for each mention generation\r\n",
        "2. Reduce the list to the final ID that represents the correct name.\r\n",
        "\r\n",
        "This is generally the method used in `spacy` module.\r\n",
        "\r\n",
        "Another option used for this is used in `deeppavlov` module (http://docs.deeppavlov.ai/en/master/features/models/entity_linking.html)where:\r\n",
        "1. NER is fed to tf-idf Vectorizer and the resulting sparse vector is converted to dense vector.\r\n",
        "2. A library called Faiss (https://github.com/facebookresearch/faiss) is used to find the k-nearest neighbours for tf-idf vector in the matrix where each row is a tf-idf vectors of words in entity titles.\r\n",
        "3. entities are ranked by number of relations in Wikidata (number of outgoing edges of nodes in the knowledge graph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Rwzz77iHaP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6t76Ft8iN9F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev807Omviqes"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaJkcSbuir8H"
      },
      "source": [
        "## Conclusion and Future Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaQ_6W4aixJU"
      },
      "source": [
        "Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fIMcQMsiv_2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI_01lUDizGb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8g5xj1iz6G"
      },
      "source": [
        "## Ressources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJsVMxR_i5Bl"
      },
      "source": [
        "**Reconstructing NER Corpora: a Case Study on Bulgarian** - Iva Marinova, Laska Laskova, Petya Osenova, Kiril Simov, Alexander Popov - Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 4647–4652, Marseille, 11–16 May 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcmS0ntOi40-"
      },
      "source": [
        "**Tuning Multilingual Transformers for Named Entity Recognition on\r\n",
        "Slavic Languages** - Mikhail Arkhipov, Maria Trofimova, Yuri Kuratov, Alexey Sorokin - Neural Networks and Deep Learning Laboratory, Moscow Institute of Physics and Technology, Faculty of Mathematics and Mechanics, Moscow State University - Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 89–93, Florence, Italy, 2 August 2019. - https://www.aclweb.org/anthology/W19-3712.pdf\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFStE4fNi4dj"
      },
      "source": [
        "**BERT: Pre-training of Deep Bidirectional Transformers for\r\n",
        "Language Understanding** - Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova - Google AI Language, 24 May 2019 - https://arxiv.org/pdf/1810.04805.pdf - https://github.com/google-research/bert\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWT-xT0ex9P1"
      },
      "source": [
        "**Attention Is All You Need** - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin, 6 Dec 2017 - https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB8R_2r-CnYX"
      },
      "source": [
        "**A Survey on Deep Learning for Named Entity Recognition** - Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li - IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 18 Mar 2020 - https://arxiv.org/pdf/1812.09449v3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GxlANqfFcWY"
      },
      "source": [
        "**Zero-Resource Cross-Domain Named Entity Recognition** - Zihan Liu, Genta Indra Winata, Pascale Fung - Center for Artificial Intelligence Research (CAiRE), Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, 19 May 2020 - https://arxiv.org/pdf/2002.05923.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbLxoqMU-Eia"
      },
      "source": [
        "**Exploring Cross-sentence Contexts for Named Entity Recognition with BERT** - Jouni Luoma, Sampo Pyysalo - Turku NLP group, University of Turku, Finland, 2 Jun 2020 - https://arxiv.org/pdf/2006.01563v1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbaNql7k2j9y"
      },
      "source": [
        "**NER with BERT in Action** - Bill Huang - July 30, 2019- https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcNDPUIv4Fm3"
      },
      "source": [
        "**The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)** - Jay Alammar blog - http://jalammar.github.io/illustrated-bert/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlvqgC_rUXm5"
      },
      "source": [
        "**Deep contextualized word representations** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer - Allen Institute for Artificial Intelligence and Paul G. Allen School of Computer Science & Engineering, University of Washington, 22 Mar 2018 - https://arxiv.org/pdf/1802.05365.pdf\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPzo1mNoU0F-"
      },
      "source": [
        "**Improving Language Understanding by Generative Pre-Training** - Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever - Open AI - https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LYtPCX2usln"
      },
      "source": [
        "**Introduction to the conll-2003 shared task: Language independent named entity recognition.** - Tjong Kim Sang, E. F. and De Meulder, F. (2003) - https://arxiv.org/pdf/cs/0306050.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m28l7lsJxT-"
      },
      "source": [
        "**Large-Scale Multi-Label Text Classification on EU Legislation** - Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis,\r\n",
        "Ion Androutsopoulos - Department of Informatics, Athens University of Economics and Business, Greece (June 2019) - https://arxiv.org/pdf/1906.02192v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YotmtdHUvqxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}